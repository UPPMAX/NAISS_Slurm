{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the \u201cRunning jobs on HPC systems\u201d course materials \u00b6 Objectives In this course you will learn about: Cluster architecture sbatch options for CPU job scripts Writing submission scripts for: I/O intensive jobs OpenMP and MPI jobs Job arrays and simple task farms Jobs with more memory per task Running jobs on GPUs Monitoring jobs and their efficiency Prerequisites \u00b6 An account at an HPC centre If you do not have an account, you can still listen to the presentations. Knowledge of logging in to your chosen HPC centre The login and file transfer course has some information about logging in We have listed login info for several Swedish HPC centres Very basic Linux knowledge You can find the material and recordings from the most recent NAISS Linux introduction course here Basic knowledge about the software modules system used at the NAISS centres You can find the material and the recordings from the most recent NAISS selecting software modules course here","title":"Home"},{"location":"#welcome__to__the__running__jobs__on__hpc__systems__course__materials","text":"Objectives In this course you will learn about: Cluster architecture sbatch options for CPU job scripts Writing submission scripts for: I/O intensive jobs OpenMP and MPI jobs Job arrays and simple task farms Jobs with more memory per task Running jobs on GPUs Monitoring jobs and their efficiency","title":"Welcome to the &ldquo;Running jobs on HPC systems&rdquo; course materials"},{"location":"#prerequisites","text":"An account at an HPC centre If you do not have an account, you can still listen to the presentations. Knowledge of logging in to your chosen HPC centre The login and file transfer course has some information about logging in We have listed login info for several Swedish HPC centres Very basic Linux knowledge You can find the material and recordings from the most recent NAISS Linux introduction course here Basic knowledge about the software modules system used at the NAISS centres You can find the material and the recordings from the most recent NAISS selecting software modules course here","title":"Prerequisites"},{"location":"cluster/","text":"Introduction to clusters \u00b6 This section is a beginner\u2019s guide to clusters, and provides general information about computer clusters like Tetralith, Dardel, Alvis, Cosmos, Pelle, Kebnekaise and other HPC systems, but is not directly focused on any of them. What is a cluster \u00b6 A computer cluster consists of a number of computers (few or many), linked together and working closely together. In many ways, the computer cluster works as a single computer. Generally, the component-computers are connected to each other through fast local area networks (LANs). The advantage of computer clusters over single computers, are that they usually improves the performance (and availability) greatly, while still being cheaper than single computers of comparable speed and size. What are Nodes, Cores, and CPUs? \u00b6 A node is the name usually used for one unit (usually one computer) in a computer cluster. Generally, this computer will have one or two central processing units, or CPUs, each normally with (many) more than one core. Each core is a single processor able to handle a single programmed task. Memory is always shared between cores on the same CPU, but generally not between the CPUs. Computer nodes can also have GPUs (graphical processing units) in addition to the CPUs. Nodes in computer clusters are usually arranged in racks at a dedicated climate- controlled facility, and are connected via a communication network. With few exceptions, nearly all high-performance computer clusters use the Linux operating system. Normally, clusters have some sort of batch or queuing system to handle scheduling of jobs. On Linux systems, that batch or queuing system is most often Slurm (the Simple Linux Utility for Resource Management). What is a Supercomputer? Is it the same as a Cluster? \u00b6 A supercomputer is simply a computer with a processing capacity (generally calculation speed) several orders of magnitude better than a typical personal computer. For many years, supercomputers were single computers with many CPUs and usually large volumes of shared memory\u2014sometimes built specifically for a certain task. They have often been custom-built machines, like Cray, and still sometimes are. However, since desktop computers have become cheaper, most supercomputers today are made up of many \u201coff the shelf\u201d ordinary computers connected in parallel. A supercomputer is not the same as a computer cluster, though a computer cluster is often a supercomputer. How is a job run on a computer cluster? What is a batch system? \u00b6 In general, jobs are run with a batch- or queuing system. There are several variants of these, with the most common working by having the user log into a \u201clogin node\u201d and then assembling and submitting their jobs from there. A \u201cjob script\u201d (also called a \u201csubmission script\u201d) will typically be used to start a job. A job script is essentially a list of commands to the batch system telling it things like: - how many nodes to use, - how many CPU and/or GPU cores, - how much memory to allocate, - how long to run (maximum), - the program name, - any input data, etc. When the job has finished running, it should have produced some files, like output data, perhaps error messages, etc. The syntax of job scripts depends on the queuing system, but that system is so often Slurm that you may see the terms \u201cjob script\u201d and \u201cslurm script\u201d used interchangeably. Since jobs are queued internally and will run whenever the resources for them become available, programs requiring any kind of user interaction are usually not recommended (and often not possible) to be run via a job script. There are special programs like Open On-Demand and GfxLauncher that allow graphical programs to be run as scheduled jobs on some HPC clusters, but it is up to the administrators to decide which programs can be run with these tools, how they should be configured, and what options regular users will be allowed to set. Which programs can be run effectively on a computer cluster? \u00b6 Computer clusters are made up of many interconnected nodes, each with a limited number of cores and limited memory capacity. The main way an HPC cluster lets you speed up computations is by letting you execute several tasks in parallel. In other words, a problem must somehow be split into many tasks to gain any speed-up. Many serial jobs \u00b6 Running many independent tasks can be done faster on a computer cluster. No special programming is needed, but you can only run on one core for each task. It is good for long-running single-threaded jobs. This type of workflow is good for problems like parameter sweeps, where the same code is run repeatedly with different inputs. A job scheduler is used to control the flow of tasks. Using a small script, many instances of the same task (like a program run many times, each with slightly different parameters) can be set up. The tasks will be put in a job queue, and will run as free spaces open up in the queue. Normally, the tasks will run many at a time, since they are serial, which means each only uses one core. Example You launch 500 tasks (say, run a small program for 500 different temperatures). There are 50 cores on the machine in our example, that you can access. Fifty instances are started and then run, while the remaining 450 tasks wait. When the running programs finish, the next 50 will start, and so on. It will be is as if you ran on 50 computers instead of one, and you will finish in 1/50th of the time. Of course, this is an ideal example. In reality there may be overhead, waiting time between batches of jobs, etc. so the speed-up will not be as great, but it will certainly run faster. Parallelization \u00b6 Parallelization can be done in several ways: Inside a node: threaded/shared memory Across several nodes: distributed parallelism (generally done with MPI or similar.) Some combination of threaded and distributed parallelism (hard) Which is best depends on the size of each parallel process and whether or to what extent the processes have to communicate. What kinds of programs can be parallelized? \u00b6 For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication Shared memory/thread parallelism \u00b6 Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. Distributed parallelism \u00b6 While shared memory parallelism works well inside a node, you need distributed parallelism if you want to scale to more cores than are in a node. This is often done with MPI (Message Passing Interface) libraries. Processes/workers exchange information by sending and receiving messages/data. They can also share some memory. You need to write your code so it uses MPI or use software that is already prepared for it. Hybrid parallelism \u00b6 Sometimes code can be more efficient when using both OpenMP and MPI. This is called hybrid parallelism. GPUs \u00b6 Many computer clusters have GPUs in several of their nodes that jobs may take advantage of. Originally, GPUs were used for computer graphics, but now they are also used extensively for general-purpose computing (GPGPU computing). Image of CPU and GPU components from NVidia. ALU = Arithmetic Logic Unit, and DRAM = Dynamic Random-Access Memory. GPU-driven parallel computing is, among other things, used for: scientific modeling machine learning graphical rendering and other parallelizable jobs. Difference between CPUs and GPUs \u00b6 CPUs (Central Processing Units) are latency-optimized general-purpose processors designed to handle a wide range of distinct tasks sequentially. GPUs (Graphics Processing Units) are throughput-optimized specialized processors designed for high-end parallel computing. Whether you should use a CPU, a GPU, or both depends on the specifics of the problem you are solving. Using GPUs \u00b6 Programs must be written especially for GPUs in order to use them. Several programming frameworks handle the graphical primitives that GPUs understand, like CUDA (Compute Unified Device Architecture), OpenCL, OpenACC, HIP, etc. In addition to the above programming frameworks, you often have the option to use software that is already prepared for use on GPUs. This includes many types of MD software, Python packages, and others.","title":"Intro to clusters"},{"location":"cluster/#introduction__to__clusters","text":"This section is a beginner\u2019s guide to clusters, and provides general information about computer clusters like Tetralith, Dardel, Alvis, Cosmos, Pelle, Kebnekaise and other HPC systems, but is not directly focused on any of them.","title":"Introduction to clusters"},{"location":"cluster/#what__is__a__cluster","text":"A computer cluster consists of a number of computers (few or many), linked together and working closely together. In many ways, the computer cluster works as a single computer. Generally, the component-computers are connected to each other through fast local area networks (LANs). The advantage of computer clusters over single computers, are that they usually improves the performance (and availability) greatly, while still being cheaper than single computers of comparable speed and size.","title":"What is a cluster"},{"location":"cluster/#what__are__nodes__cores__and__cpus","text":"A node is the name usually used for one unit (usually one computer) in a computer cluster. Generally, this computer will have one or two central processing units, or CPUs, each normally with (many) more than one core. Each core is a single processor able to handle a single programmed task. Memory is always shared between cores on the same CPU, but generally not between the CPUs. Computer nodes can also have GPUs (graphical processing units) in addition to the CPUs. Nodes in computer clusters are usually arranged in racks at a dedicated climate- controlled facility, and are connected via a communication network. With few exceptions, nearly all high-performance computer clusters use the Linux operating system. Normally, clusters have some sort of batch or queuing system to handle scheduling of jobs. On Linux systems, that batch or queuing system is most often Slurm (the Simple Linux Utility for Resource Management).","title":"What are Nodes, Cores, and CPUs?"},{"location":"cluster/#what__is__a__supercomputer__is__it__the__same__as__a__cluster","text":"A supercomputer is simply a computer with a processing capacity (generally calculation speed) several orders of magnitude better than a typical personal computer. For many years, supercomputers were single computers with many CPUs and usually large volumes of shared memory\u2014sometimes built specifically for a certain task. They have often been custom-built machines, like Cray, and still sometimes are. However, since desktop computers have become cheaper, most supercomputers today are made up of many \u201coff the shelf\u201d ordinary computers connected in parallel. A supercomputer is not the same as a computer cluster, though a computer cluster is often a supercomputer.","title":"What is a Supercomputer? Is it the same as a Cluster?"},{"location":"cluster/#how__is__a__job__run__on__a__computer__cluster__what__is__a__batch__system","text":"In general, jobs are run with a batch- or queuing system. There are several variants of these, with the most common working by having the user log into a \u201clogin node\u201d and then assembling and submitting their jobs from there. A \u201cjob script\u201d (also called a \u201csubmission script\u201d) will typically be used to start a job. A job script is essentially a list of commands to the batch system telling it things like: - how many nodes to use, - how many CPU and/or GPU cores, - how much memory to allocate, - how long to run (maximum), - the program name, - any input data, etc. When the job has finished running, it should have produced some files, like output data, perhaps error messages, etc. The syntax of job scripts depends on the queuing system, but that system is so often Slurm that you may see the terms \u201cjob script\u201d and \u201cslurm script\u201d used interchangeably. Since jobs are queued internally and will run whenever the resources for them become available, programs requiring any kind of user interaction are usually not recommended (and often not possible) to be run via a job script. There are special programs like Open On-Demand and GfxLauncher that allow graphical programs to be run as scheduled jobs on some HPC clusters, but it is up to the administrators to decide which programs can be run with these tools, how they should be configured, and what options regular users will be allowed to set.","title":"How is a job run on a computer cluster? What is a batch system?"},{"location":"cluster/#which__programs__can__be__run__effectively__on__a__computer__cluster","text":"Computer clusters are made up of many interconnected nodes, each with a limited number of cores and limited memory capacity. The main way an HPC cluster lets you speed up computations is by letting you execute several tasks in parallel. In other words, a problem must somehow be split into many tasks to gain any speed-up.","title":"Which programs can be run effectively on a computer cluster?"},{"location":"cluster/#many__serial__jobs","text":"Running many independent tasks can be done faster on a computer cluster. No special programming is needed, but you can only run on one core for each task. It is good for long-running single-threaded jobs. This type of workflow is good for problems like parameter sweeps, where the same code is run repeatedly with different inputs. A job scheduler is used to control the flow of tasks. Using a small script, many instances of the same task (like a program run many times, each with slightly different parameters) can be set up. The tasks will be put in a job queue, and will run as free spaces open up in the queue. Normally, the tasks will run many at a time, since they are serial, which means each only uses one core. Example You launch 500 tasks (say, run a small program for 500 different temperatures). There are 50 cores on the machine in our example, that you can access. Fifty instances are started and then run, while the remaining 450 tasks wait. When the running programs finish, the next 50 will start, and so on. It will be is as if you ran on 50 computers instead of one, and you will finish in 1/50th of the time. Of course, this is an ideal example. In reality there may be overhead, waiting time between batches of jobs, etc. so the speed-up will not be as great, but it will certainly run faster.","title":"Many serial jobs"},{"location":"cluster/#parallelization","text":"Parallelization can be done in several ways: Inside a node: threaded/shared memory Across several nodes: distributed parallelism (generally done with MPI or similar.) Some combination of threaded and distributed parallelism (hard) Which is best depends on the size of each parallel process and whether or to what extent the processes have to communicate.","title":"Parallelization"},{"location":"cluster/#what__kinds__of__programs__can__be__parallelized","text":"For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"What kinds of programs can be parallelized?"},{"location":"cluster/#shared__memorythread__parallelism","text":"Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous.","title":"Shared memory/thread parallelism"},{"location":"cluster/#distributed__parallelism","text":"While shared memory parallelism works well inside a node, you need distributed parallelism if you want to scale to more cores than are in a node. This is often done with MPI (Message Passing Interface) libraries. Processes/workers exchange information by sending and receiving messages/data. They can also share some memory. You need to write your code so it uses MPI or use software that is already prepared for it.","title":"Distributed parallelism"},{"location":"cluster/#hybrid__parallelism","text":"Sometimes code can be more efficient when using both OpenMP and MPI. This is called hybrid parallelism.","title":"Hybrid parallelism"},{"location":"cluster/#gpus","text":"Many computer clusters have GPUs in several of their nodes that jobs may take advantage of. Originally, GPUs were used for computer graphics, but now they are also used extensively for general-purpose computing (GPGPU computing). Image of CPU and GPU components from NVidia. ALU = Arithmetic Logic Unit, and DRAM = Dynamic Random-Access Memory. GPU-driven parallel computing is, among other things, used for: scientific modeling machine learning graphical rendering and other parallelizable jobs.","title":"GPUs"},{"location":"cluster/#difference__between__cpus__and__gpus","text":"CPUs (Central Processing Units) are latency-optimized general-purpose processors designed to handle a wide range of distinct tasks sequentially. GPUs (Graphics Processing Units) are throughput-optimized specialized processors designed for high-end parallel computing. Whether you should use a CPU, a GPU, or both depends on the specifics of the problem you are solving.","title":"Difference between CPUs and GPUs"},{"location":"cluster/#using__gpus","text":"Programs must be written especially for GPUs in order to use them. Several programming frameworks handle the graphical primitives that GPUs understand, like CUDA (Compute Unified Device Architecture), OpenCL, OpenACC, HIP, etc. In addition to the above programming frameworks, you often have the option to use software that is already prepared for use on GPUs. This includes many types of MD software, Python packages, and others.","title":"Using GPUs"},{"location":"concepts/","text":"Batch system concepts \u00b6 What is a batch system? \u00b6 A batch system provides a mechanism register (submit) your computational jobs (program or group of programs) for execution by the system. You have to describe your job to the system, which includes the resources required, such as the number of nodes , the number of cores and the job time . The batch system may run you job directly or execute it at a later stage, depending on a variety of reasons. In the following animation we illustrate how a batch system might execute jobs. Imagine a system with 4 nodes, 8 cores each. Description of the animation \u00b6 We start with an empty cluster First job gets submitted, asking 1 core for 8 hours The job starts executing on core 0 of the first node Second job, asks for a full node with 8 cores for 4 hours System executes the job on node 3 Third job, asks for 2 nodes, 8 cores each for 8 hours System executes the job on nodes 2 & 4 Fourth job, asking 3 nodes, 8 cores each for 3 hours Insufficient idle nodes at this time System determines that 3 nodes are idle once job 3 finishes System schedules job four to be run on nodes 2, 3 & 4 once job 3 has finished Fifth job, asking for 1 node with 8 cores for 12 hours No idle node available a this time System determines that node 1 is idle once job 1 finishes System schedules job 5 to be run on nodes 1 once job 1 has finished Sixth job, asking for 2 nodes, 8 cores each for 4 hours The job has higher priority than jobs 4 & 5 If possible it should start before jobs 4 & 5 System re-schedules jobs 4 & 5 for a later time System schedules job 6 to be run on nodes 1 & 3 once job 1 has finished Job 5 gets re-scheduled to run on node 2 Job 4 gets re-scheduled to run on nodes 1, 3 & 4 The expected start time of jobs 4 & 5 is pushed back in time due to the submission of job six Seventh job, asking 1 core for 6 hours While there are currently enough idle cores, the job can not be started since there are less then 6 hours until the start of job 6 Job 7 gets scheduled to execute after jobs 6 & 4 are finished Eighth job, asking 4 cores for 2 hours Due to the short job time of 2 hours, the job starts directly It is finished before the scheduled start time of job 6 It starts executing before the jobs 4, 5, 6 & 7, which were submitted earlier This is called backfill This is not to be confused with priority - this job started because a slot was available to run it now. Key points \u00b6 The submitted job joins a waiting queue Jobs start executing once it is their turn The anticipated start time can move forward and backward depending on other users actions The description of the computational work is typically done by a script (this course) Important Computational work needs to be submitted using the job scheduler You need to describe the computational work at the time of the job submission, since you might not be there (out of hours) once your job starts executing Comment on priorities \u00b6 The priority of a jobs is affected by a number of factors. These may include: The time the job has already spent in queue How many resources (e.g. CPU hours) the project has used in relation to the allocated resources System administrators may adjust the batch system rules to ensure all user group can run jobs in line with the resources allocated to the project.","title":"Batch system concepts"},{"location":"concepts/#batch__system__concepts","text":"","title":"Batch system concepts"},{"location":"concepts/#what__is__a__batch__system","text":"A batch system provides a mechanism register (submit) your computational jobs (program or group of programs) for execution by the system. You have to describe your job to the system, which includes the resources required, such as the number of nodes , the number of cores and the job time . The batch system may run you job directly or execute it at a later stage, depending on a variety of reasons. In the following animation we illustrate how a batch system might execute jobs. Imagine a system with 4 nodes, 8 cores each.","title":"What is a batch system?"},{"location":"concepts/#description__of__the__animation","text":"We start with an empty cluster First job gets submitted, asking 1 core for 8 hours The job starts executing on core 0 of the first node Second job, asks for a full node with 8 cores for 4 hours System executes the job on node 3 Third job, asks for 2 nodes, 8 cores each for 8 hours System executes the job on nodes 2 & 4 Fourth job, asking 3 nodes, 8 cores each for 3 hours Insufficient idle nodes at this time System determines that 3 nodes are idle once job 3 finishes System schedules job four to be run on nodes 2, 3 & 4 once job 3 has finished Fifth job, asking for 1 node with 8 cores for 12 hours No idle node available a this time System determines that node 1 is idle once job 1 finishes System schedules job 5 to be run on nodes 1 once job 1 has finished Sixth job, asking for 2 nodes, 8 cores each for 4 hours The job has higher priority than jobs 4 & 5 If possible it should start before jobs 4 & 5 System re-schedules jobs 4 & 5 for a later time System schedules job 6 to be run on nodes 1 & 3 once job 1 has finished Job 5 gets re-scheduled to run on node 2 Job 4 gets re-scheduled to run on nodes 1, 3 & 4 The expected start time of jobs 4 & 5 is pushed back in time due to the submission of job six Seventh job, asking 1 core for 6 hours While there are currently enough idle cores, the job can not be started since there are less then 6 hours until the start of job 6 Job 7 gets scheduled to execute after jobs 6 & 4 are finished Eighth job, asking 4 cores for 2 hours Due to the short job time of 2 hours, the job starts directly It is finished before the scheduled start time of job 6 It starts executing before the jobs 4, 5, 6 & 7, which were submitted earlier This is called backfill This is not to be confused with priority - this job started because a slot was available to run it now.","title":"Description of the animation"},{"location":"concepts/#key__points","text":"The submitted job joins a waiting queue Jobs start executing once it is their turn The anticipated start time can move forward and backward depending on other users actions The description of the computational work is typically done by a script (this course) Important Computational work needs to be submitted using the job scheduler You need to describe the computational work at the time of the job submission, since you might not be there (out of hours) once your job starts executing","title":"Key points"},{"location":"concepts/#comment__on__priorities","text":"The priority of a jobs is affected by a number of factors. These may include: The time the job has already spent in queue How many resources (e.g. CPU hours) the project has used in relation to the allocated resources System administrators may adjust the batch system rules to ensure all user group can run jobs in line with the resources allocated to the project.","title":"Comment on priorities"},{"location":"interactive/","text":"Interactive jobs \u00b6 There are more than one way to start an interactive job. It can be done either from the command line or inside ThinLinc (GfxLauncher) or from a portal (OpenOnDemand portal). salloc and interactive \u00b6 The Slurm commands salloc and interactive are for requesting an interactive allocation. This is done differently depending on the centre. Some centres recommend using GfxLauncher or Open OnDemand for interactive jobs. Cluster interactive salloc srun GfxLauncher or OpenOnDemand Tetralith (NSC) Recommended N/A N/A N/A Dardel (PDC) N/A Recommended N/A Possible (GfxLauncher) Alvis (C3SE) N/A N/A Works Recommended (OOD) Kebnekaise (HPC2N) N/A Recommended N/A Recommended (OOD) Pelle (UPPMAX) Recommended Works N/A N/A Cosmos (LUNARC) Works N/A N/A Recommended (GfxLauncher) Examples \u00b6 Tetralith Dardel Alvis Kebnekaise Pelle Cosmos The command \u201cinteractive\u201d is recommended at NSC. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ x_birbr@tetralith3 ~ ] $ interactive -A naiss2026-4-66 salloc: Pending job allocation 44252533 salloc: job 44252533 queued and waiting for resources salloc: job 44252533 has been allocated resources salloc: Granted job allocation 44252533 salloc: Waiting for resource configuration salloc: Nodes n340 are ready for job [ x_birbr@n340 ~ ] $ The command salloc (or OpenOnDemand through Gfx launcher) is recommended at PDC. bbrydsoe@login1:~> salloc --time = 00 :10:00 -A naiss2026-4-66 -p main salloc: Pending job allocation 9722449 salloc: job 9722449 queued and waiting for resources salloc: job 9722449 has been allocated resources salloc: Granted job allocation 9722449 salloc: Waiting for resource configuration salloc: Nodes nid001134 are ready for job bbrydsoe@login1:~> Again, you are on the login node, and anything you want to run in the allocation must be preface with srun . However, you have another option; you can ssh to the allocated compute node and then it will be true interactivity: bbrydsoe@login1:~> ssh nid001134 bbrydsoe@nid001134:~ It is also possible to use OpenOnDemand through Gfx launcher. To do this, login with ThinLinc and start the Gfxlauncher application. There is some documentation here: Interactive HPC at PDC . Please be aware that the number of ThinLinc licenses are limited. The command \u201csrun\u201d from command line works at C3Se . It is not recommended as when the login node is restarted the interactive job is also terminated. [ brydso@alvis2 ~ ] $ srun --account = naiss2026-4-66 --gpus-per-node = T4:1 --time = 01 :00:00 --pty = /bin/bash [ brydso@alvis2-12 ~ ] $ The recommended way to do interactive jobs at Alvis is with OpenOnDemand. You access the Open OnDemand service through https://alvis.c3se.chalmers.se . NOTE that you need to connect from a network on SUNET. More information about C3SE\u2019s Open OnDemand service can be found here: https://www.c3se.chalmers.se/documentation/connecting/ondemand/ . The command salloc (or OpenOnDemand) is recommended at HPC2N. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. b-an01 [ ~ ] $ salloc -A hpc2n2025-151 -t 00 :10:00 salloc: Pending job allocation 34624444 salloc: job 34624444 queued and waiting for resources salloc: job 34624444 has been allocated resources salloc: Granted job allocation 34624444 salloc: Nodes b-cn1403 are ready for job b-an01 [ ~ ] $ WARNING! This is not true interactivity! Note that we are still on the login node! In order to run anything in the allocation, you need to preface with srun like this: b-an01 [ ~ ] $ srun /bin/hostname b-cn1403.hpc2n.umu.se b-an01 [ ~ ] $ Otherwise anything will run on the login node! Also, interactive sessions (for instance a program that asks for input) will not work correctly as that dialogoue happens on the compute node which you do not have real access to! OpenOnDemand This is the recommended way to do interactive jobs at HPC2N. Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand At UPPMAX, \u201cinteractive\u201d is recommended. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ bbrydsoe@pelle1 ~ ] $ interactive -A uppmax2025-2-393 -t 00 :15:00 This is a temporary version of interactive-script for Pelle Most interactive-script functionality is removed salloc: Pending job allocation 205612 salloc: job 205612 queued and waiting for resources salloc: job 205612 has been allocated resources salloc: Granted job allocation 205612 salloc: Waiting for resource configuration salloc: Nodes p115 are ready for job [ bbrydsoe@p115 ~ ] $ salloc also works. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. [ bbrydsoe@pelle1 ~ ] $ salloc -A uppmax2025-2-393 -t 00 :15:00 salloc: Pending job allocation 205613 salloc: job 205613 queued and waiting for resources salloc: job 205613 has been allocated resources salloc: Granted job allocation 205613 salloc: Nodes p115 are ready for job [ bbrydsoe@p115 ~ ] $ The command interactive works at LUNARC . It is not the recommended way to do interactive work. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ bbrydsoe@cosmos2 ~ ] $ interactive -A lu2025-7-76 -t 00 :15:00 Cluster name: COSMOS Waiting for JOBID 1724396 to start After a short wait, you get something like this: [ bbrydsoe@cn094 ~ ] $ GfxLauncher This is the recommended wait to work interactively at LUNARC. Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/ GfxLauncher and OpenOnDemand \u00b6 While these are mentioned above, the information is gathered here for ease. GfxLauncher and OpenOnDemand This is the recommended way to do interactive jobs at HPC2N, LUNARC, and C3SE, and is possible at PDC. Kebnekaise Cosmos Alvis Dardel Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/ Go to https://alvis.c3se.chalmers.se/ There is some documentation here: https://www.c3se.chalmers.se/documentation/connecting/ondemand/ Login with ThinLinc and follow the documentation for starting the Gfxlauncher for OpenOnDemand: Interactive HPC at PDC . Please be aware that the number of ThinLinc licenses are limited. Note At centres that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At centres that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB.","title":"Interactive jobs"},{"location":"interactive/#interactive__jobs","text":"There are more than one way to start an interactive job. It can be done either from the command line or inside ThinLinc (GfxLauncher) or from a portal (OpenOnDemand portal).","title":"Interactive jobs"},{"location":"interactive/#salloc__and__interactive","text":"The Slurm commands salloc and interactive are for requesting an interactive allocation. This is done differently depending on the centre. Some centres recommend using GfxLauncher or Open OnDemand for interactive jobs. Cluster interactive salloc srun GfxLauncher or OpenOnDemand Tetralith (NSC) Recommended N/A N/A N/A Dardel (PDC) N/A Recommended N/A Possible (GfxLauncher) Alvis (C3SE) N/A N/A Works Recommended (OOD) Kebnekaise (HPC2N) N/A Recommended N/A Recommended (OOD) Pelle (UPPMAX) Recommended Works N/A N/A Cosmos (LUNARC) Works N/A N/A Recommended (GfxLauncher)","title":"salloc and interactive"},{"location":"interactive/#examples","text":"Tetralith Dardel Alvis Kebnekaise Pelle Cosmos The command \u201cinteractive\u201d is recommended at NSC. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ x_birbr@tetralith3 ~ ] $ interactive -A naiss2026-4-66 salloc: Pending job allocation 44252533 salloc: job 44252533 queued and waiting for resources salloc: job 44252533 has been allocated resources salloc: Granted job allocation 44252533 salloc: Waiting for resource configuration salloc: Nodes n340 are ready for job [ x_birbr@n340 ~ ] $ The command salloc (or OpenOnDemand through Gfx launcher) is recommended at PDC. bbrydsoe@login1:~> salloc --time = 00 :10:00 -A naiss2026-4-66 -p main salloc: Pending job allocation 9722449 salloc: job 9722449 queued and waiting for resources salloc: job 9722449 has been allocated resources salloc: Granted job allocation 9722449 salloc: Waiting for resource configuration salloc: Nodes nid001134 are ready for job bbrydsoe@login1:~> Again, you are on the login node, and anything you want to run in the allocation must be preface with srun . However, you have another option; you can ssh to the allocated compute node and then it will be true interactivity: bbrydsoe@login1:~> ssh nid001134 bbrydsoe@nid001134:~ It is also possible to use OpenOnDemand through Gfx launcher. To do this, login with ThinLinc and start the Gfxlauncher application. There is some documentation here: Interactive HPC at PDC . Please be aware that the number of ThinLinc licenses are limited. The command \u201csrun\u201d from command line works at C3Se . It is not recommended as when the login node is restarted the interactive job is also terminated. [ brydso@alvis2 ~ ] $ srun --account = naiss2026-4-66 --gpus-per-node = T4:1 --time = 01 :00:00 --pty = /bin/bash [ brydso@alvis2-12 ~ ] $ The recommended way to do interactive jobs at Alvis is with OpenOnDemand. You access the Open OnDemand service through https://alvis.c3se.chalmers.se . NOTE that you need to connect from a network on SUNET. More information about C3SE\u2019s Open OnDemand service can be found here: https://www.c3se.chalmers.se/documentation/connecting/ondemand/ . The command salloc (or OpenOnDemand) is recommended at HPC2N. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. b-an01 [ ~ ] $ salloc -A hpc2n2025-151 -t 00 :10:00 salloc: Pending job allocation 34624444 salloc: job 34624444 queued and waiting for resources salloc: job 34624444 has been allocated resources salloc: Granted job allocation 34624444 salloc: Nodes b-cn1403 are ready for job b-an01 [ ~ ] $ WARNING! This is not true interactivity! Note that we are still on the login node! In order to run anything in the allocation, you need to preface with srun like this: b-an01 [ ~ ] $ srun /bin/hostname b-cn1403.hpc2n.umu.se b-an01 [ ~ ] $ Otherwise anything will run on the login node! Also, interactive sessions (for instance a program that asks for input) will not work correctly as that dialogoue happens on the compute node which you do not have real access to! OpenOnDemand This is the recommended way to do interactive jobs at HPC2N. Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand At UPPMAX, \u201cinteractive\u201d is recommended. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ bbrydsoe@pelle1 ~ ] $ interactive -A uppmax2025-2-393 -t 00 :15:00 This is a temporary version of interactive-script for Pelle Most interactive-script functionality is removed salloc: Pending job allocation 205612 salloc: job 205612 queued and waiting for resources salloc: job 205612 has been allocated resources salloc: Granted job allocation 205612 salloc: Waiting for resource configuration salloc: Nodes p115 are ready for job [ bbrydsoe@p115 ~ ] $ salloc also works. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. [ bbrydsoe@pelle1 ~ ] $ salloc -A uppmax2025-2-393 -t 00 :15:00 salloc: Pending job allocation 205613 salloc: job 205613 queued and waiting for resources salloc: job 205613 has been allocated resources salloc: Granted job allocation 205613 salloc: Nodes p115 are ready for job [ bbrydsoe@p115 ~ ] $ The command interactive works at LUNARC . It is not the recommended way to do interactive work. Usage: interactive -A [project_name] -t HHH:MM:SS If you need more CPUs/GPUs, etc. you need to ask for that as well. The default which gives 1 CPU. [ bbrydsoe@cosmos2 ~ ] $ interactive -A lu2025-7-76 -t 00 :15:00 Cluster name: COSMOS Waiting for JOBID 1724396 to start After a short wait, you get something like this: [ bbrydsoe@cn094 ~ ] $ GfxLauncher This is the recommended wait to work interactively at LUNARC. Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/","title":"Examples"},{"location":"interactive/#gfxlauncher__and__openondemand","text":"While these are mentioned above, the information is gathered here for ease. GfxLauncher and OpenOnDemand This is the recommended way to do interactive jobs at HPC2N, LUNARC, and C3SE, and is possible at PDC. Kebnekaise Cosmos Alvis Dardel Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/ Go to https://alvis.c3se.chalmers.se/ There is some documentation here: https://www.c3se.chalmers.se/documentation/connecting/ondemand/ Login with ThinLinc and follow the documentation for starting the Gfxlauncher for OpenOnDemand: Interactive HPC at PDC . Please be aware that the number of ThinLinc licenses are limited. Note At centres that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At centres that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB.","title":"GfxLauncher and OpenOnDemand"},{"location":"intro/","text":"Introduction to \u201cRunning jobs on HPC systems\u201d \u00b6 Welcome page and syllabus: https://uppmax.github.io/NAISS_Slurm/index.html Link also in the House symbol at the top of the page. Learning outcomes Cluster architecture Login/compute nodes Cores, nodes, GPUs Memory Node local storage Global storage system Concepts of a job scheduler why it is needed basic priniples how it works sbatch with options for CPU job scripts sample job scripts Basic jobs I/O intensive jobs OpenMP and MPI jobs Job arrays Simple example for task farming increasing the memory per task / memory hungry jobs running on GPUs job monitoring, job efficiency how to find optimal sbatch options Login info, project number, project directory \u00b6 Project number and project directory \u00b6 Warning This part is only relevant for people attending the course, and indeed only for those who needed access to Tetralith or Dardel through the course project. It should be ignored if you are doing it as self-study later. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Tetralith at NSC Project ID: naiss2026-4-66 Project storage: /proj/spring-courses-naiss/users Dardel at PDC Project ID: naiss2026-4-66 Project storage: /cfs/klemming/projects/supr/spring-courses-naiss Alvis at C3SE Project ID: naiss2026-4-66 Project storage: /mimer/NOBACKUP/groups/spring-courses-naiss Kebnekaise at HPC2N Project ID: Project storage: Cosmos at LUNARC Project ID: Pelle at UPPMAX Project ID: Project storage: Hint Most of you are not part of the course project and will use your own access and project. If you do not know what your project id is, you can use the command projinfo which works at most clusters. If no projinfo From the terminal, you can manually ask Slurm for project IDs currently associated with your user with this command: sacctmgr list assoc where user=\"$USER\" format=Account -P --noheader You can also find the project id in SUPR, if you are a member of a project. See the page for Active Projects You Belong To . Login info \u00b6 You will not need a graphical user interface for this course. Even so, if you do not have a preferred SSH client, we recommend using ThinLinc . Connection info Login to the system you are using (Tetralith/Dardel, other Swedish HPC system) Connection info for some Swedish HPC systems - use the one you have access to: Tetralith Dardel Alvis Kebnekaise Pelle Cosmos SSH: ssh <user>@tetralith.nsc.liu.se ThinLinc: Server: tetralith.nsc.liu.se Username: <your-nsc-username> Password: <your-nsc-password> Note that you need to setup TFA to use NSC! SSH: ssh <user>@dardel.pdc.kth.se ThinLinc: Server: dardel-vnc.pdc.kth.se Username: <your-pdc-username> Password: <your-pdc-password> Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se Remote Desktop Protocol (RDP): Server: alvis1.c3se.chalmers.se or alvis2.c3se.chalmers.se Username: <your-c3se-username> Password: <your-c3se-username> OpenOndemand portal: Put https://alvis.c3se.chalmers.se in browser address bar Put <your-c3se-username> and <your-c3se-password> in the login box Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. SSH: ssh <user>@kebnekaise.hpc2n.umu.se ThinLinc: Server: kebnekaise-tl.hpc2n.umu.se Username: <your-hpc2n-username> Password: <yout-hpc2n-password> ThinLinc Webaccess: Put https://kebnekaise-tl.hpc2n.umu.se:300/ in browser address bar Put <your-hpc2n-username> and <your-hpc2n-password> in th e login box that opens and click Login Open OnDemand: https://portal.hpc2n.umu.se SSH: ssh <user>@pelle.uppmax.uu.se ThinLinc: Server: pelle-gui.uppmax.uu.se Username: <your-uppmax-username> Password: <your-uppmax-password> Note that you have to setup 2FA for Uppmax . SSH: ssh <user>@cosmos.lunarc.lu.se ThinLinc: Server: cosmos-dt.lunarc.lu.se Username: <your-lunarc-username> Password: <your-lunarc-password> Note that you need to setup TFA (PocketPass) to use LUNARC\u2019s systems! Schedule \u00b6 Time Topic Activity Teacher 13:00 - 13:05 Intro to course Lecture Sahar 13:05 - 13:25 Intro to clusters Lecture Sahar 13:25 - 13:40 Batch system concepts / job scheduling Lecture Joachim 13:40 - 14:20 Intro to Slurm (sbatch, squeue, scontrol, \u2026) Lecture+type along Birgitte 14:20 - 14:22 Interactive jobs - mainly meant as self-study Lecture 14:22 - 14:35 BREAK 14:35 - 15:45 Additional sample scripts, including job arrays, task farms??? Joachim, Diana 15:45 - 15:47 Job monitoring and efficiency - mainly meant as Self-reading material Diana 15:47 - 16:00 Summary Diana Prepare the exercise environment \u00b6 It is now time to login and download the exercises. Login to your cluster. You find login info for several Swedish HPC clusters here . Create a directory to work in: mkdir cluster-intro Fetch the exercises tarball: wget https://github.com/UPPMAX/NAISS_Slurm/raw/refs/heads/main/exercises.tar.gz Unpack the tarball: tar zxvf exercises.tar.gz You will get a directory exercises . Go into it: cd exercises You will find some sub directories for most of the Swedish HPC centres. Change to the directory of your cluster. If it is not listed, pick \u201cother\u201d. There should be various batch script examples (and some .py, .f90 and .c files for the test scripts).","title":"Introduction"},{"location":"intro/#introduction__to__running__jobs__on__hpc__systems","text":"Welcome page and syllabus: https://uppmax.github.io/NAISS_Slurm/index.html Link also in the House symbol at the top of the page. Learning outcomes Cluster architecture Login/compute nodes Cores, nodes, GPUs Memory Node local storage Global storage system Concepts of a job scheduler why it is needed basic priniples how it works sbatch with options for CPU job scripts sample job scripts Basic jobs I/O intensive jobs OpenMP and MPI jobs Job arrays Simple example for task farming increasing the memory per task / memory hungry jobs running on GPUs job monitoring, job efficiency how to find optimal sbatch options","title":"Introduction to &ldquo;Running jobs on HPC systems&rdquo;"},{"location":"intro/#login__info__project__number__project__directory","text":"","title":"Login info, project number, project directory"},{"location":"intro/#project__number__and__project__directory","text":"Warning This part is only relevant for people attending the course, and indeed only for those who needed access to Tetralith or Dardel through the course project. It should be ignored if you are doing it as self-study later. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Tetralith at NSC Project ID: naiss2026-4-66 Project storage: /proj/spring-courses-naiss/users Dardel at PDC Project ID: naiss2026-4-66 Project storage: /cfs/klemming/projects/supr/spring-courses-naiss Alvis at C3SE Project ID: naiss2026-4-66 Project storage: /mimer/NOBACKUP/groups/spring-courses-naiss Kebnekaise at HPC2N Project ID: Project storage: Cosmos at LUNARC Project ID: Pelle at UPPMAX Project ID: Project storage: Hint Most of you are not part of the course project and will use your own access and project. If you do not know what your project id is, you can use the command projinfo which works at most clusters. If no projinfo From the terminal, you can manually ask Slurm for project IDs currently associated with your user with this command: sacctmgr list assoc where user=\"$USER\" format=Account -P --noheader You can also find the project id in SUPR, if you are a member of a project. See the page for Active Projects You Belong To .","title":"Project number and project directory"},{"location":"intro/#login__info","text":"You will not need a graphical user interface for this course. Even so, if you do not have a preferred SSH client, we recommend using ThinLinc . Connection info Login to the system you are using (Tetralith/Dardel, other Swedish HPC system) Connection info for some Swedish HPC systems - use the one you have access to: Tetralith Dardel Alvis Kebnekaise Pelle Cosmos SSH: ssh <user>@tetralith.nsc.liu.se ThinLinc: Server: tetralith.nsc.liu.se Username: <your-nsc-username> Password: <your-nsc-password> Note that you need to setup TFA to use NSC! SSH: ssh <user>@dardel.pdc.kth.se ThinLinc: Server: dardel-vnc.pdc.kth.se Username: <your-pdc-username> Password: <your-pdc-password> Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se Remote Desktop Protocol (RDP): Server: alvis1.c3se.chalmers.se or alvis2.c3se.chalmers.se Username: <your-c3se-username> Password: <your-c3se-username> OpenOndemand portal: Put https://alvis.c3se.chalmers.se in browser address bar Put <your-c3se-username> and <your-c3se-password> in the login box Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. SSH: ssh <user>@kebnekaise.hpc2n.umu.se ThinLinc: Server: kebnekaise-tl.hpc2n.umu.se Username: <your-hpc2n-username> Password: <yout-hpc2n-password> ThinLinc Webaccess: Put https://kebnekaise-tl.hpc2n.umu.se:300/ in browser address bar Put <your-hpc2n-username> and <your-hpc2n-password> in th e login box that opens and click Login Open OnDemand: https://portal.hpc2n.umu.se SSH: ssh <user>@pelle.uppmax.uu.se ThinLinc: Server: pelle-gui.uppmax.uu.se Username: <your-uppmax-username> Password: <your-uppmax-password> Note that you have to setup 2FA for Uppmax . SSH: ssh <user>@cosmos.lunarc.lu.se ThinLinc: Server: cosmos-dt.lunarc.lu.se Username: <your-lunarc-username> Password: <your-lunarc-password> Note that you need to setup TFA (PocketPass) to use LUNARC\u2019s systems!","title":"Login info"},{"location":"intro/#schedule","text":"Time Topic Activity Teacher 13:00 - 13:05 Intro to course Lecture Sahar 13:05 - 13:25 Intro to clusters Lecture Sahar 13:25 - 13:40 Batch system concepts / job scheduling Lecture Joachim 13:40 - 14:20 Intro to Slurm (sbatch, squeue, scontrol, \u2026) Lecture+type along Birgitte 14:20 - 14:22 Interactive jobs - mainly meant as self-study Lecture 14:22 - 14:35 BREAK 14:35 - 15:45 Additional sample scripts, including job arrays, task farms??? Joachim, Diana 15:45 - 15:47 Job monitoring and efficiency - mainly meant as Self-reading material Diana 15:47 - 16:00 Summary Diana","title":"Schedule"},{"location":"intro/#prepare__the__exercise__environment","text":"It is now time to login and download the exercises. Login to your cluster. You find login info for several Swedish HPC clusters here . Create a directory to work in: mkdir cluster-intro Fetch the exercises tarball: wget https://github.com/UPPMAX/NAISS_Slurm/raw/refs/heads/main/exercises.tar.gz Unpack the tarball: tar zxvf exercises.tar.gz You will get a directory exercises . Go into it: cd exercises You will find some sub directories for most of the Swedish HPC centres. Change to the directory of your cluster. If it is not listed, pick \u201cother\u201d. There should be various batch script examples (and some .py, .f90 and .c files for the test scripts).","title":"Prepare the exercise environment"},{"location":"jobscripts/","text":"Sample job scripts \u00b6 Basic Serial Job \u00b6 In this section we discuss the running of a serial Python script using a couple of services as an example. But first let\u2019s spend a few lines on partitions. Partitions \u00b6 As discussed, not all compute nodes offered by a service are equal. Nodes may offer different hardware (e.g. CPU type, amount of memory, number of GPUs or no GPU). There might also be differences on how the nodes are configured. To control that a job is placed on the correct kind of compute nodes, the nodes may be placed in partitions. Many but not all services have a default partition. Information about partitions can usually be found with sinfo . Tetralith Dardel Cosmos Kebnekaise All nodes are in a single partion. There is no need to specify a partition on Tetralith. There is no default partition on Dardel. One always has to specify a partion on Dardel. Partition name Node type Node sharing Max job time shared Thin part of a node up to 7 days main Thin, large, huge exclusive up to 24 h long Thin exclusive up to 7 days memory Large, huge, giant exclusive up to 7 days gpu AMD GPU exclusive up to 24 h gpugh Nvidia Grace Hopper exclusive up to 24 h Explanations exclusive nodes: One gets all the cores, all the memory and all the GPUs of the requested nodes. The allocation gets charged for all these resources consumed, including the 128 cores of the node. Don\u2019t use for serial jobs or small parallel jobs part of a node : One can request any number of cores up to 128 cores using the -n and -c options of sbatch. Your allocation gets charged for the number of requested cores. Use for serial jobs and small parallel jobs On Cosmos at LUNARC you will be placed in the CPU partition by default. If you need access to a GPU node, you need to select a partition. Please visit the LUNARC documentation on readthedocs.io for a detailed discussion. There is only a single partition, batch , that users can submit jobs to. The system then figures out, based on requested features which actual partition(s) the job should be sent to. Since there is only one partition available for users to submit jobs to, you should remove any use of #SBATCH -p you may have in your scripts. Previously, the most common use of -p was for targeting the LargeMemory nodes, this is now done using a feature request like this: #SBATCH -C largemem Examples by service \u00b6 Let\u2019s say you have a simple Python script called mmmult.py that creates 2 random-valued matrices, multiplies them together, and prints the shape of the result and the computation time. Let\u2019s also say that you want to run this code in your current working directory. Here is how you can run that program utilising 1 core on 1 node on a number of services: Tetralith Dardel Kebnekaise Cosmos Pelle mmmult.py #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # ask for 1 core, serial running #SBATCH -n 1 # Asking for 1 core # name output and error file #SBATCH -o process_%j.out #SBATCH -e process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # load a modern Python distribution and make NumPy available module load buildenv-gcc/2023b-eb module load Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py On Dardel you always have to specify a partition. #!/bin/bash #SBATCH -A <project ID> # Change to your own project #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p shared # ask to be placed in the shared partition #SBATCH -o process_%j.out # name the output file #SBATCH -e process_%j.err # name the error file cat $0 # Load any modules you need, here for cray-python/3.11.7. module load cray-python/3.11.7 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.3 and compatible SciPy-bundle module load GCC/12.3.0 Python/3.11.3 SciPy-bundle/2023.07 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A luXXXX-Y-ZZ # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.5 and compatible SciPy-bundle module load GCC/13.2.0 Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZ # Change to your own after the course #SBATCH --time=00:20:00 # Asking for 20 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here Python 3.12.3 # and a compatible SciPy-bundle module load Python/3.12.3-GCCcore-13.3.0 module load SciPy-bundle/2024.05-gfbf-2024a # Run your Python script python mmmult.py import timeit import numpy as np starttime = timeit . default_timer () np . random . seed ( 1701 ) A = np . random . randint ( - 1000 , 1000 , size = ( 8 , 4 )) B = np . random . randint ( - 1000 , 1000 , size = ( 4 , 4 )) print ( \"This is matrix A: \\n \" , A ) print ( \"The shape of matrix A is \" , A . shape ) print () print ( \"This is matrix B: \\n \" , B ) print ( \"The shape of matrix B is \" , B . shape ) print () print ( \"Doing matrix-matrix multiplication...\" ) print () C = np . matmul ( A , B ) print ( \"The product of matrices A and B is: \\n \" , C ) print ( \"The shape of the resulting matrix is \" , C . shape ) print () print ( \"Time elapsed for generating matrices and multiplying them is \" , timeit . default_timer () - starttime ) There is no example for Alvis since you should only use that for running GPU jobs. OpenMP and shared memory programming \u00b6 Shared memory programming is a parallel programming model associated with threads. You start a LINUX/UNIX process, which spawns threads. The memory of the process can be accessed by all the threads. The threads are typically placed on and often bound to different logical or physical cores of a single hardware node. The number of cores available on a node limits the number of threads one can reasonably start on a node. In shared memory programming it is typically not possible to utilise cores from different nodes. All cores need to be in the same node. The aim of spawning threads is to speed up the calculation to achieve a fast time to solution. OpenMP is an API widely used in scientific computing to facilitate shared memory programming. The behaviour of an application utilising OpenMP can be controlled by a number of environment variables. Even the behaviour of many applications utilising a different API to facilitate shared memory programming, can be controlled by OpenMP environment variables. When executing shared memory applications, unless there is a suitable default, one may need to ensure that only one task is used. This can be done by using the -n option of SLURM, e.g. having a line: #SBATCH -n 1 in the script. The number of cores to host the threads can be requested by using either the -c or the --cpus-per-task option. Both of which do exactly the same thing, so use only one of those. The following would request eight (logical) cores #SBATCH -c 8 Important Depending on how the service you are using is configured, you might be requesting logical cores, with multiple logical cores being placed on a single physical core. This is called hyperthreading. It is important to experiment whether placing threads on multiple logical cores of a physical core benefits or hinders the performance of your application. On most services it is not required to set the environment variable OMP_NUM_THREADS in your SLURM scripts. If you are happy with the default of the service this will be picked up from your request with the -c option. It typically uses all the cores you requested. Tetralith Dardel Hyperthreading is not active on Tetralith. By default a single thread is placed on each physical core. In the following we give an example using thread binding, which typically benefits the performance. When using binding one can easily modify how the theads are mapped onto the hardware. This can be done by changing the value of the environment variable OMP_PROC_BIND . It is advisable to experiment with the values close and spread for the binding. This can be accomplished in the below script by commenting the unwanted option and uncommenting the wanted option. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time, #SBATCH -t 00:10:00 # ask for 8 core here, modify for your needs. # When running OpenMP code on Tetralith one can ask up to 32 cores #SBATCH -c 8 # name output and error file #SBATCH -o omp_process_%j.out #SBATCH -e omp_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # process binding is typically recommended. Try what works best spread or close #export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores export OMP_PLACES = cores # Run your OpenMP executable ./openmp_application On the shared partition of Dardel hyperthreading is engaged. The shared partions is typically recommended to run application spawning threads, such as those parallelised using OpenMP. Different compilers react differently to hyperthreading, in particular in combination with thread binding. Using the -c option of sbatch you request a number of logical cores for your run. There are two logical cores per physical core, which is called hyperthreading. With this line commented, the script will place two threads on each physical core. One thread for each logical core. We start with a submission script for the CRAY clang compiler . It is advisable to experiment with close and spread binding, as well as binding to cores or threads . Binding to cores will not utilise hyperthreading, while binding to threads does. For each of the two options we have provided the relevant lines in the script. Comment of uncomment to explore what give best performance for your application. #!/bin/bash # Project id - change to your own! #SBATCH -A <proj-id> # Number of cores per tasks # The number of physical cores is half that number #SBATCH -c 8 # Asking for a walltime of 5 min on the shared partition #SBATCH --time=00:05:00 #SBATCH -p shared #SBATCH -o process_omp_%j.out #SBATCH -e process_omp_%j.err cat $0 # Load a compiler toolchain so we can run an OpenMP program module load cpe/24.11 # process binding is typically recommended. Try what works best spread or close # export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores - this disengages hyper-threading export OMP_PLACES = cores # we bind to threads - this engages hyper-threading # export OMP_PLACES=threads ./openmp_application If your application has been compiled using GCC 13.2, the following script should work. Again one should explore the effect of close or spread binding. If you want to disengage Hyperthreading, uncomment the line setting the OMP_NUM_THREADS environment variable. #!/bin/bash # Project id - change to your own! #SBATCH -A <proj-id> # Number of cores per tasks #SBATCH -c 8 # Asking for a walltime of 5 min #SBATCH --time=00:05:00 #SBATCH -p shared #SBATCH -o process_omp_%j.out #SBATCH -e process_omp_%j.err cat $0 # Load a compiler toolchain so we can run an OpenMP program module load gcc-native/13.2 # process binding is typically recommended. Try what works best spread or close #export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores export OMP_PLACES = cores # if we want to have a single thread per core and ignor hyperthreading, un-comment the below # export OMP_NUM_THREADS=$(($SLURM_CPUS_PER_TASK/2)) ./openmp_application Remember, Alvis is only for GPU jobs Applications using MPI \u00b6 Some form of message passing is required when utilising multiple nodes for a simulation. One has multiple programs, called tasks, running. Typically these are multiple copies of the same executable with each getting its own dedicated core. Each task has its own memory, which is called distributed memory. Data exchange is facilitated by coping data between the tasks. This can accomplished inside the node if both task are running on the same node or has to utilise the network if the tasks in question are located on different nodes. The Message Passing Interface (MPI) is the most commonly used API in scientific computing, when programming message passing applications. The illustration shows 5 tasks being executed, with the time running from the top to the bottom. At the beginning, data (e.g. read from an input file) is distributed from task 0 to the other tasks, indicated by the blue arrows. Following this, the tasks exchange data at regular intervals. In a real application the communication patterns are typically more complex than this. Important When runing an executable that utilises MPI you need to start multiple executables. Typically you start one executable on each requested core. Most of the time multiple copies of the same excutable are used. To start multiple copies of the same executable a special program, a so called job launcher is required. Depending on the system and libraries used the name of the jobs launcher differs. In the following we have sample scripts for a number of services, including NAISS\u2019 Tetralith and Dardel services. The sample script assumes an mpi executable name integration2D_f90 in the submission directory. The executable takes the problem size as a number as a command line argument. In the example the problem size is 10000. Tetralith Dardel On Tetralith, when using the job launcher mpprun , the user does not need to specify the compiler version and the version of the MPI library used to compile the application. Tetralith nodes have 32 cores. One should aim to use multiples of 32 cores when running MPI workloads. In this example we ask for 16 cores, which is 1/2 node. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # ask for 16 core, experiment for what works best #SBATCH -n 16 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Run your mpi_executable mpprun ./integration2D_f90 10000 The following is a script utilising 2 full nodes in the main partition, to run a code compiled by the user utilising the CRAY clang compiler. This script utilises a total of 256 cores and even modest run times will be expensive by means of CPU hours for your allocation. Scripts requesting multiple nodes are required by projects which have been allocated significant resourse and need to run large calculations to achieve their project goals. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time, #SBATCH -t 00:10:00 # Using the Dardel's main partition #SBATCH -p main # ask for 256 cores located on 2 nodes, modify for your needs. #SBATCH -N 2 #SBATCH --ntasks-per-node=128 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Loading a suitable module. Here for Cray programming environment etc. module load PDC/24.11 # Run your mpi_executable srun ./mpi_hello The following is a script utilising part of a shared node, to run a code compiled by the user utilising the CRAY clang compiler. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # Using the Dardel shared partition #SBATCH -p shared # ask for 16 core on one node, modify for your needs. #SBATCH -N 1 #SBATCH --ntasks-per-node=16 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Loading a suitable module. Here for Cray programming environment etc. module load PDC/24.11 # Run your mpi_executable srun ./integration2D_f90 100000 Asking for whole nodes ( - N ) and possibly --tasks-per-node srun and mpirun should be interchangeable at many centres. Tetralith uses mpprun and Dardel uses srun Remember, you need to load modules with MPI At some centres mpirun --bind-to-core or srun --cpu-bind=cores is recommended for MPI jobs NOTE: Alvis is only used for GPU jobs Memory-intensive jobs \u00b6 Running out of memory (\u201cOOM\u201d): usually the job stops (\u201ccrashes\u201d) check the Slurm error/log files check with sacct/seff/jobstats/job-usage depending on cluster Fixes: use \u201cfat\u201d nodes allocate more cores just for memory tweak memory usage in app, if possible Increasing memory per task \u00b6 A way to increase memory per task that works generally is to simply ask for more cores per task, where some are just giving memory. Example In this case, we are asking for 16 tasks, with 2 cores per task. This means we are asking for 32 cores in total. We do this by adding this to our batch script: #SBATCH --ntasks=16 --cpus-per-task=2 NOTE You can also write --cpus-per-task=#num in short form as -c #num --ntasks=#numtasks in short form as -n #numtasks Example script template Here asking for 8 tasks, 2 cores per task. #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n 8 #SBATCH -c 2 module load <modules> srun ./myprogram Example script template Here we have a non-threaded code which needs more memory (up to twice the amount we have on two cores). #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -c 2 module load <modules> ./myprogram Remember : if you are on Dardel, you also need to add a partition. Note At some centres, you can also use #SBATCH --mem-per-cpu=<MEMORY> . If you ask for more memory than is on one core, some cores will have to remain idle while only providing memory. You will also be charged for these cores, of course. To see the amount of available memory per core, see the next section. Memory availability \u00b6 Another way of getting extra memory is to use nodes that have more memory. A useful command to identify how much memory is available on different nodes is sinfo -o \"%10P %20l %30N %10z %10c %20m %20f %20G\" . Here is an overview of some of the available nodes at the Swedish HPC centres: Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Type RAM/node RAM/core cores/node Requesting flag Intel Xeon Gold 6130 thin 96 GB 3 GB 32 -C thin --exclusive Intel Xeon Gold 6130 fat 384 GB 12 GB 32 -C fat --exclusive Type RAM/node RAM/core cores/node Partition Available Requesting flag AMD EPYC\u2122 Zen2 Thin 256 GB 2 GB 128 main, shared, long 227328 MB AMD EPYC\u2122 Zen2 Large 512 GB 4 GB 128 main, memory 456704 MB --mem=440GB AMD EPYC\u2122 Zen2 Huge 1 TB 7.8 GB 128 main, memory 915456 MB --mem=880GB AMD EPYC\u2122 Zen2 Giant 2 TB 15.6 GB 128 memory 1832960 MB --mem=1760GB 4 x AMD Instinct\u2122 MI250X dual GPUs 512 GB 8 GB 64 gpu 456704 MB --mem=440GB On shared partitions you need to give number of cores and will get RAM equivalent for that RAM GPUs Requesting flag 768 V100 (2) V100 (4) and a no GPU skylake #SBATCH -C MEM768 #SBATCH --gpus-per-node=V100:[1-4] 576 T4 (8) #SBATCH -C MEM576 #SBATCH --gpus-per-node=T4:[1-8] 1536 T4 (8) #SBATCH -C MEM1536 #SBATCH --gpus-per-node=A40:[1-4] 512 A100 (4) and a no GPU icelake #SBATCH -C mem512 #SBATCH --gpus-per-node=A100:[1-4] 256 A40 (4, no IB) A100 (4) #SBATCH -C mem256 and either #SBATCH --gpus-per-node=A40[1-4] or #SBATCH --gpus-per-node=A100[1-4] 1024 A100fat (4) #SBATCH -C mem1024 #SBATCH --gpus-per-node=A100fat:[1-4] Note be aware, though that you also need to ask for a GPU, as usual, unless you need the pre/post processing CPU nodes ( -C NOGPU ). You only really need to give the memory constraint for those bolded as the others follow from the GPU choice sinfo -o \"%20N %9P %4c %24f %50G\" will give you a full list of all nodes and features Type RAM/core cores/node requesting flag Intel Skylake 6785 MB 28 -C skylake AMD Zen3 8020 MB 128 -C zen3 AMD Zen4 2516 MB 256 -C zen4 V100 6785 MB 28 --gpus=<#num> -C v100 A100 10600 MB 48 --gpus=<#num> -C a100 MI100 10600 MB 48 --gpus=<#num> -C mi100 A6000 6630 MB 48 --gpus=<#num> -C a6000 H100 6630 MB 96 --gpus=<#num> -C h100 L40s 11968 MB 64 --gpus=<#num> -C l40s A40 11968 MB 64 --gpus=<#num> -C a40 Largemem 41666 MB 72 -C largemem Type RAM/core cores/node requesting flag AMD 7413 5.3 GB 48 Intel / A100 12 GB 32 -p gpua100i AMD / A100 10.7 GB 48 -p gpua100 Type RAM/node RAM/core cores/node requesting flag AMD EPYC 9454P (Zen4) 768 GB 16 GB 48 -p pelle AMD EPYC 9454P (Zen4) 2 or 3 TB 41.67 or 62.5 GB 48 -p fat 2xAMD EPYC 9124 (Zen4), 10xL40s 384 GB 12 GB 32 -p gpu --gpus=l40s:[1-10] 2xAMD EPYC 9124 (Zen4), 2xH100 384 GB 12 GB 32 -p gpu --gpus=h100:[1-2] In addition you can use all the Slurm options for memory: --mem --mem-per-cpu --mem-per-gpu to specify memory requirements. Pelle at UPPMAX The compute node CPUs have Simultaneous multithreading (SMT) enabled. Each CPU core runs two Threads. In Slurm the Threads are referred to as CPUs. If you suspect SMT degrades the performance of your jobs, you can you can specify --threads-per-core=1 in your job. More information here: Simultaneous multi-threading . I/O intensive jobs \u00b6 Note This section comes with many caveats; it depends a lot on the type of job and the system. Often, if you are in the situation where you have an I/O intensive job, you need to talk to support as it will be very individualized. Not all systems offer node local discs The Dardel system does not offer node local discs. The use of $SNIC_TMP , $NAISS_TMP and $TMPDIR is discouraged. $SNIC_TMP and $NAISS_TMP do not offer a performance advantage over the project storage. In addition they are not protected against name space conflicts by jobs submitted by the same user, which are running on different nodes. $TMPDIR will utilise the node\u2019s RAM, which in most cases defeats the purpose of using $TMPDIR . In most cases, you should use the project storage Centre-dependent. If needed you can use node-local disc for single-node jobs Remember you need to copy data to/from the node-local scratch ( $SNIC_TMP )! On some systems $TMPDIR also points to the node local disc The environment variable $SLURM_SUBMIT_DIR is the directory you submitted from On Tetralith, the data access between /home or /proj and GPU/CPU compute nodes are not suitable for I/O intensive jobs => use /scratch/local ( $SNIC_TMP ) Example \u00b6 #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n <cores> module load <modules> # Copy your data etc. to node local scratch disc cp -p mydata.dat $SNIC_TMP cp -p myprogram $SNIC_TMP # Change to that directory cd $SNIC_TMP # Run your program ./myprogram # Copy the results back to the submission directory cp -p mynewdata.dat $SLURM_SUBMIT_DIR NOTE When using node local disk it is important to remember to copy the output data back, since it will go away when the job ends! Job arrays \u00b6 Job arrays: a mechanism for submitting and managing collections of similar jobs. All jobs must have the same initial options (e.g. size, time limit, etc.) the execution times can vary depending on input data You create multiple jobs from one script, using the -- array directive. This requires very little BASH scripting abilities max number of jobs is restricted by max number of jobs/user - centre specific More information here on the official Slurm documentation pages. Example This shows how to run a small Python script hello-world-array.py as an array. # import sys library (we need this for the command line args) import sys # print task number print ( 'Hello world! from task number: ' , sys . argv [ 1 ]) You could then make a batch script like this, hello-world-array.sh : #!/bin/bash # A very simple example of how to run a Python script with a job array #SBATCH -A <account> #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --array=1-10 # how many tasks in the array #SBATCH -c 1 # Asking for 1 core # one core per task # Create specific output files for each task with the environment variable %j # which contains the job id and %a for each step #SBATCH -o hello-world-%j-%a.out # Load any modules you need module load <module> <python-module> # Run your Python script srun python hello-world-array.py $SLURM_ARRAY_TASK_ID Hint Try it! You can find the above script under any of the cluster resources folders in the exercise tarball. Some array comments \u00b6 Default step of 1 Example: #SBATCH --array=4-80 Give an index (here steps of 4) Example: #SBATCH --array=1-100:4 Give a list instead of a range Example: #SBATCH --array=5,8,33,38 Throttle jobs, so only a smaller number of jobs run at a time Example: #SBATCH --array1-400%4 Name output/error files so each job ( %j or %A ) and step ( %a ) gets own file #SBATCH -o process_%j_%a.out #SBATCH -e process_%j_%a.err There is an environment variable $SLURM_ARRAY_TASK_ID which can be used to check/query with GPU jobs \u00b6 There are some differences between the centres in Sweden what type of GPUs they have. The command sinfo -o \"%10P %20l %30N %10z %10c %20m %20f %20G\" | grep gpu is very useful as well to identify the GPUs available on a cluster. Resource cores/node RAM/node GPUs, type (per node) Tetralith 32 96-384 GB Nvidia T4 GPUs (1) Dardel 128 256-2048 GB 4 AMD Instinct\u2122 MI250X (2) Alvis 16 (skylake 2xV100), 32 (skylake 4xV100, 8xT4), 64 (icelake 4xA40, 4xA100) 256-1024 GB Nvidia v100 (2), v100 (4), T4 (8), A40 (4), A100 (4) Kebnekaise 28 (skylake), 72 (largemem), 128/256 (Zen3/Zen4) 128-3072 GB NVidia v100 (2), NVidia a100 (2), NVidia a6000 (2), NVidia l40s (2 or 6), NVidia H100 (4), NVidia A40 (8), AMD MI100 (2) Cosmos 32 (Intel) or 48 (AMD) 256-512 GB A100 Pelle 32 384 GB L40s (10), H100 (2) Alvis also has a small number of nodes without GPUs, for heavy-duty pre- and post-processing that does not require a GPU. To use, specify the constraint -C NOGPU in your Slurm script. Allocating a GPU \u00b6 This is the most different of the Slurm settings, between centers. Resource batch settings Comments Tetralith #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu Alvis #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x - no node-sharing on multi-node jobs ( --exclusive is automatic) - Requesting -N 1 does not mean 1 full node - type is V100, A40, A100, or A100fat - x is number of GPUs, 1-4 Cosmos #SBATCH -p gpua100 #SBATCH --gres=gpu:1 Kebnekaise #SBATCH --gpus=x #SBATCH -C <type> - type is the type of GPU in lower case - x is the number of that type of GPU. See above table for both Pelle -p gpu --gpus=<type>:x - type is the type of GPU in lower case - x is the number of that type of GPU. See above table for both Example GPU scripts \u00b6 This shows a simple GPU script, asking for 1 or 2 cards on a single node. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code> #!/bin/bash -l # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code - REMEMBER NOT NVIDIA/CUDA!> #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ #SBATCH -t HHH:MM:SS #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:4 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code> #!/bin/bash #SBATCH -A hpc2nXXXX-YYY # Change to your own project ID #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS # Ask for GPU resources. You pick type as one of the ones shown above # and how many cards you want, at most as many as shown above. Here 2 L40s #SBATCH --gpus:2 #SBATCH -C l40s # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Purge unneeded modules. Load any needed GPU modules and any prerequisites ml purge > /dev/null 2 > & 1 module load <MODULES> <run-my-GPU-code> #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A luXXXX-Y-ZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Remove any loaded modules and load the GPU modules and prerequisites ones we need module purge > /dev/null 2 > & 1 module load <MODULES> <run-my-GPU-code> #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZZ # Change to your own! #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH -p gpu #SBATCH --gpus:l40s:1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load the GPU modules we need module load <MODULES> <run-my-GPU-code> Hint You can find a few example GPU batch scripts and corresponding programs in the cluster subfolders in the exercises tarball. Some of them requires installing some Python packages in a virtual environment. It is described in the .sh file for each alvis, cosmos, kebnekaise, pelle, tetralith add-list.py, add-list.sh pytorch_fitting_gpu.py, pytorch_fitting_gpu.sh integration2d_gpu.py, integration2d_gpu_shared.py, job-gpu.sh dardel hello_world_gpu.cpp, hello_world_gpu.sh Miscellaneous \u00b6 There are many other types of jobs in Slurm. Here are a few more examples. Note If you are on Dardel, you also need to add a partition. Multiple serial jobs, simultaneously \u00b6 #!/bin/bash #SBATCH -A <job ID> # Add enough cores that all jobs can run at the same time #SBATCH -n <cores> # Make sure the time is long enough that the longest job have time to finish #SBATCH --time=HHH:MM:SS module load <modules> srun -n 1 --exclusive ./program data1 & srun -n 1 --exclusive ./program2 data2 & srun -n 1 --exclusive ./program3 data3 & ... srun -n 1 --exclusive ./program4 data4 & wait Multiple simultaneous jobs (serial or parallel) \u00b6 In this example, 3 jobs each with 14 cores #!/bin/bash #SBATCH -A <job ID> # Since the files run simultaneously I need enough cores for all of them to run #SBATCH -n 56 # Remember to ask for enough time for all jobs to complete #SBATCH --time=00:10:00 module load <modules> srun -n 14 --exclusive ./mympiprogram data & srun -n 14 --exclusive ./my2mpi data2 & srun -n 14 --exclusive ./my3mpi data3 & wait Multiple sequential jobs (serial or parallel) \u00b6 This example is for jobs where some are with 14 tasks with 2 cores per task and some are 4 tasks with 4 cores per task #!/bin/bash #SBATCH -A <job ID> # Since the programs run sequentially I only need enough cores for the largest of them to run #SBATCH -c 28 # Remember to ask for enough time for all jobs to complete #SBATCH --time=HHH:MM:SS module load <modules> srun -n 14 -c 2 ./myprogram data srun -n 4 -c 4 ./myotherprogram mydata ... srun -n 14 -c 2 ./my2program data2","title":"Sample job scripts"},{"location":"jobscripts/#sample__job__scripts","text":"","title":"Sample job scripts"},{"location":"jobscripts/#basic__serial__job","text":"In this section we discuss the running of a serial Python script using a couple of services as an example. But first let\u2019s spend a few lines on partitions.","title":"Basic Serial Job"},{"location":"jobscripts/#partitions","text":"As discussed, not all compute nodes offered by a service are equal. Nodes may offer different hardware (e.g. CPU type, amount of memory, number of GPUs or no GPU). There might also be differences on how the nodes are configured. To control that a job is placed on the correct kind of compute nodes, the nodes may be placed in partitions. Many but not all services have a default partition. Information about partitions can usually be found with sinfo . Tetralith Dardel Cosmos Kebnekaise All nodes are in a single partion. There is no need to specify a partition on Tetralith. There is no default partition on Dardel. One always has to specify a partion on Dardel. Partition name Node type Node sharing Max job time shared Thin part of a node up to 7 days main Thin, large, huge exclusive up to 24 h long Thin exclusive up to 7 days memory Large, huge, giant exclusive up to 7 days gpu AMD GPU exclusive up to 24 h gpugh Nvidia Grace Hopper exclusive up to 24 h Explanations exclusive nodes: One gets all the cores, all the memory and all the GPUs of the requested nodes. The allocation gets charged for all these resources consumed, including the 128 cores of the node. Don\u2019t use for serial jobs or small parallel jobs part of a node : One can request any number of cores up to 128 cores using the -n and -c options of sbatch. Your allocation gets charged for the number of requested cores. Use for serial jobs and small parallel jobs On Cosmos at LUNARC you will be placed in the CPU partition by default. If you need access to a GPU node, you need to select a partition. Please visit the LUNARC documentation on readthedocs.io for a detailed discussion. There is only a single partition, batch , that users can submit jobs to. The system then figures out, based on requested features which actual partition(s) the job should be sent to. Since there is only one partition available for users to submit jobs to, you should remove any use of #SBATCH -p you may have in your scripts. Previously, the most common use of -p was for targeting the LargeMemory nodes, this is now done using a feature request like this: #SBATCH -C largemem","title":"Partitions"},{"location":"jobscripts/#examples__by__service","text":"Let\u2019s say you have a simple Python script called mmmult.py that creates 2 random-valued matrices, multiplies them together, and prints the shape of the result and the computation time. Let\u2019s also say that you want to run this code in your current working directory. Here is how you can run that program utilising 1 core on 1 node on a number of services: Tetralith Dardel Kebnekaise Cosmos Pelle mmmult.py #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # ask for 1 core, serial running #SBATCH -n 1 # Asking for 1 core # name output and error file #SBATCH -o process_%j.out #SBATCH -e process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # load a modern Python distribution and make NumPy available module load buildenv-gcc/2023b-eb module load Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py On Dardel you always have to specify a partition. #!/bin/bash #SBATCH -A <project ID> # Change to your own project #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core #SBATCH -p shared # ask to be placed in the shared partition #SBATCH -o process_%j.out # name the output file #SBATCH -e process_%j.err # name the error file cat $0 # Load any modules you need, here for cray-python/3.11.7. module load cray-python/3.11.7 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.3 and compatible SciPy-bundle module load GCC/12.3.0 Python/3.11.3 SciPy-bundle/2023.07 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A luXXXX-Y-ZZ # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.5 and compatible SciPy-bundle module load GCC/13.2.0 Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZ # Change to your own after the course #SBATCH --time=00:20:00 # Asking for 20 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here Python 3.12.3 # and a compatible SciPy-bundle module load Python/3.12.3-GCCcore-13.3.0 module load SciPy-bundle/2024.05-gfbf-2024a # Run your Python script python mmmult.py import timeit import numpy as np starttime = timeit . default_timer () np . random . seed ( 1701 ) A = np . random . randint ( - 1000 , 1000 , size = ( 8 , 4 )) B = np . random . randint ( - 1000 , 1000 , size = ( 4 , 4 )) print ( \"This is matrix A: \\n \" , A ) print ( \"The shape of matrix A is \" , A . shape ) print () print ( \"This is matrix B: \\n \" , B ) print ( \"The shape of matrix B is \" , B . shape ) print () print ( \"Doing matrix-matrix multiplication...\" ) print () C = np . matmul ( A , B ) print ( \"The product of matrices A and B is: \\n \" , C ) print ( \"The shape of the resulting matrix is \" , C . shape ) print () print ( \"Time elapsed for generating matrices and multiplying them is \" , timeit . default_timer () - starttime ) There is no example for Alvis since you should only use that for running GPU jobs.","title":"Examples by service"},{"location":"jobscripts/#openmp__and__shared__memory__programming","text":"Shared memory programming is a parallel programming model associated with threads. You start a LINUX/UNIX process, which spawns threads. The memory of the process can be accessed by all the threads. The threads are typically placed on and often bound to different logical or physical cores of a single hardware node. The number of cores available on a node limits the number of threads one can reasonably start on a node. In shared memory programming it is typically not possible to utilise cores from different nodes. All cores need to be in the same node. The aim of spawning threads is to speed up the calculation to achieve a fast time to solution. OpenMP is an API widely used in scientific computing to facilitate shared memory programming. The behaviour of an application utilising OpenMP can be controlled by a number of environment variables. Even the behaviour of many applications utilising a different API to facilitate shared memory programming, can be controlled by OpenMP environment variables. When executing shared memory applications, unless there is a suitable default, one may need to ensure that only one task is used. This can be done by using the -n option of SLURM, e.g. having a line: #SBATCH -n 1 in the script. The number of cores to host the threads can be requested by using either the -c or the --cpus-per-task option. Both of which do exactly the same thing, so use only one of those. The following would request eight (logical) cores #SBATCH -c 8 Important Depending on how the service you are using is configured, you might be requesting logical cores, with multiple logical cores being placed on a single physical core. This is called hyperthreading. It is important to experiment whether placing threads on multiple logical cores of a physical core benefits or hinders the performance of your application. On most services it is not required to set the environment variable OMP_NUM_THREADS in your SLURM scripts. If you are happy with the default of the service this will be picked up from your request with the -c option. It typically uses all the cores you requested. Tetralith Dardel Hyperthreading is not active on Tetralith. By default a single thread is placed on each physical core. In the following we give an example using thread binding, which typically benefits the performance. When using binding one can easily modify how the theads are mapped onto the hardware. This can be done by changing the value of the environment variable OMP_PROC_BIND . It is advisable to experiment with the values close and spread for the binding. This can be accomplished in the below script by commenting the unwanted option and uncommenting the wanted option. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time, #SBATCH -t 00:10:00 # ask for 8 core here, modify for your needs. # When running OpenMP code on Tetralith one can ask up to 32 cores #SBATCH -c 8 # name output and error file #SBATCH -o omp_process_%j.out #SBATCH -e omp_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # process binding is typically recommended. Try what works best spread or close #export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores export OMP_PLACES = cores # Run your OpenMP executable ./openmp_application On the shared partition of Dardel hyperthreading is engaged. The shared partions is typically recommended to run application spawning threads, such as those parallelised using OpenMP. Different compilers react differently to hyperthreading, in particular in combination with thread binding. Using the -c option of sbatch you request a number of logical cores for your run. There are two logical cores per physical core, which is called hyperthreading. With this line commented, the script will place two threads on each physical core. One thread for each logical core. We start with a submission script for the CRAY clang compiler . It is advisable to experiment with close and spread binding, as well as binding to cores or threads . Binding to cores will not utilise hyperthreading, while binding to threads does. For each of the two options we have provided the relevant lines in the script. Comment of uncomment to explore what give best performance for your application. #!/bin/bash # Project id - change to your own! #SBATCH -A <proj-id> # Number of cores per tasks # The number of physical cores is half that number #SBATCH -c 8 # Asking for a walltime of 5 min on the shared partition #SBATCH --time=00:05:00 #SBATCH -p shared #SBATCH -o process_omp_%j.out #SBATCH -e process_omp_%j.err cat $0 # Load a compiler toolchain so we can run an OpenMP program module load cpe/24.11 # process binding is typically recommended. Try what works best spread or close # export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores - this disengages hyper-threading export OMP_PLACES = cores # we bind to threads - this engages hyper-threading # export OMP_PLACES=threads ./openmp_application If your application has been compiled using GCC 13.2, the following script should work. Again one should explore the effect of close or spread binding. If you want to disengage Hyperthreading, uncomment the line setting the OMP_NUM_THREADS environment variable. #!/bin/bash # Project id - change to your own! #SBATCH -A <proj-id> # Number of cores per tasks #SBATCH -c 8 # Asking for a walltime of 5 min #SBATCH --time=00:05:00 #SBATCH -p shared #SBATCH -o process_omp_%j.out #SBATCH -e process_omp_%j.err cat $0 # Load a compiler toolchain so we can run an OpenMP program module load gcc-native/13.2 # process binding is typically recommended. Try what works best spread or close #export OMP_PROC_BIND=spread export OMP_PROC_BIND = close # we bind to cores export OMP_PLACES = cores # if we want to have a single thread per core and ignor hyperthreading, un-comment the below # export OMP_NUM_THREADS=$(($SLURM_CPUS_PER_TASK/2)) ./openmp_application Remember, Alvis is only for GPU jobs","title":"OpenMP and shared memory programming"},{"location":"jobscripts/#applications__using__mpi","text":"Some form of message passing is required when utilising multiple nodes for a simulation. One has multiple programs, called tasks, running. Typically these are multiple copies of the same executable with each getting its own dedicated core. Each task has its own memory, which is called distributed memory. Data exchange is facilitated by coping data between the tasks. This can accomplished inside the node if both task are running on the same node or has to utilise the network if the tasks in question are located on different nodes. The Message Passing Interface (MPI) is the most commonly used API in scientific computing, when programming message passing applications. The illustration shows 5 tasks being executed, with the time running from the top to the bottom. At the beginning, data (e.g. read from an input file) is distributed from task 0 to the other tasks, indicated by the blue arrows. Following this, the tasks exchange data at regular intervals. In a real application the communication patterns are typically more complex than this. Important When runing an executable that utilises MPI you need to start multiple executables. Typically you start one executable on each requested core. Most of the time multiple copies of the same excutable are used. To start multiple copies of the same executable a special program, a so called job launcher is required. Depending on the system and libraries used the name of the jobs launcher differs. In the following we have sample scripts for a number of services, including NAISS\u2019 Tetralith and Dardel services. The sample script assumes an mpi executable name integration2D_f90 in the submission directory. The executable takes the problem size as a number as a command line argument. In the example the problem size is 10000. Tetralith Dardel On Tetralith, when using the job launcher mpprun , the user does not need to specify the compiler version and the version of the MPI library used to compile the application. Tetralith nodes have 32 cores. One should aim to use multiples of 32 cores when running MPI workloads. In this example we ask for 16 cores, which is 1/2 node. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # ask for 16 core, experiment for what works best #SBATCH -n 16 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Run your mpi_executable mpprun ./integration2D_f90 10000 The following is a script utilising 2 full nodes in the main partition, to run a code compiled by the user utilising the CRAY clang compiler. This script utilises a total of 256 cores and even modest run times will be expensive by means of CPU hours for your allocation. Scripts requesting multiple nodes are required by projects which have been allocated significant resourse and need to run large calculations to achieve their project goals. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time, #SBATCH -t 00:10:00 # Using the Dardel's main partition #SBATCH -p main # ask for 256 cores located on 2 nodes, modify for your needs. #SBATCH -N 2 #SBATCH --ntasks-per-node=128 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Loading a suitable module. Here for Cray programming environment etc. module load PDC/24.11 # Run your mpi_executable srun ./mpi_hello The following is a script utilising part of a shared node, to run a code compiled by the user utilising the CRAY clang compiler. #!/bin/bash # Set account #SBATCH -A <project ID> # Set the time #SBATCH -t 00:10:00 # Using the Dardel shared partition #SBATCH -p shared # ask for 16 core on one node, modify for your needs. #SBATCH -N 1 #SBATCH --ntasks-per-node=16 # name output and error file #SBATCH -o mpi_process_%j.out #SBATCH -e mpi_process_%j.err # write this script to stdout-file - useful for scripting errors cat $0 # Loading a suitable module. Here for Cray programming environment etc. module load PDC/24.11 # Run your mpi_executable srun ./integration2D_f90 100000 Asking for whole nodes ( - N ) and possibly --tasks-per-node srun and mpirun should be interchangeable at many centres. Tetralith uses mpprun and Dardel uses srun Remember, you need to load modules with MPI At some centres mpirun --bind-to-core or srun --cpu-bind=cores is recommended for MPI jobs NOTE: Alvis is only used for GPU jobs","title":"Applications using MPI"},{"location":"jobscripts/#memory-intensive__jobs","text":"Running out of memory (\u201cOOM\u201d): usually the job stops (\u201ccrashes\u201d) check the Slurm error/log files check with sacct/seff/jobstats/job-usage depending on cluster Fixes: use \u201cfat\u201d nodes allocate more cores just for memory tweak memory usage in app, if possible","title":"Memory-intensive jobs"},{"location":"jobscripts/#increasing__memory__per__task","text":"A way to increase memory per task that works generally is to simply ask for more cores per task, where some are just giving memory. Example In this case, we are asking for 16 tasks, with 2 cores per task. This means we are asking for 32 cores in total. We do this by adding this to our batch script: #SBATCH --ntasks=16 --cpus-per-task=2 NOTE You can also write --cpus-per-task=#num in short form as -c #num --ntasks=#numtasks in short form as -n #numtasks Example script template Here asking for 8 tasks, 2 cores per task. #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n 8 #SBATCH -c 2 module load <modules> srun ./myprogram Example script template Here we have a non-threaded code which needs more memory (up to twice the amount we have on two cores). #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -c 2 module load <modules> ./myprogram Remember : if you are on Dardel, you also need to add a partition. Note At some centres, you can also use #SBATCH --mem-per-cpu=<MEMORY> . If you ask for more memory than is on one core, some cores will have to remain idle while only providing memory. You will also be charged for these cores, of course. To see the amount of available memory per core, see the next section.","title":"Increasing memory per task"},{"location":"jobscripts/#memory__availability","text":"Another way of getting extra memory is to use nodes that have more memory. A useful command to identify how much memory is available on different nodes is sinfo -o \"%10P %20l %30N %10z %10c %20m %20f %20G\" . Here is an overview of some of the available nodes at the Swedish HPC centres: Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Type RAM/node RAM/core cores/node Requesting flag Intel Xeon Gold 6130 thin 96 GB 3 GB 32 -C thin --exclusive Intel Xeon Gold 6130 fat 384 GB 12 GB 32 -C fat --exclusive Type RAM/node RAM/core cores/node Partition Available Requesting flag AMD EPYC\u2122 Zen2 Thin 256 GB 2 GB 128 main, shared, long 227328 MB AMD EPYC\u2122 Zen2 Large 512 GB 4 GB 128 main, memory 456704 MB --mem=440GB AMD EPYC\u2122 Zen2 Huge 1 TB 7.8 GB 128 main, memory 915456 MB --mem=880GB AMD EPYC\u2122 Zen2 Giant 2 TB 15.6 GB 128 memory 1832960 MB --mem=1760GB 4 x AMD Instinct\u2122 MI250X dual GPUs 512 GB 8 GB 64 gpu 456704 MB --mem=440GB On shared partitions you need to give number of cores and will get RAM equivalent for that RAM GPUs Requesting flag 768 V100 (2) V100 (4) and a no GPU skylake #SBATCH -C MEM768 #SBATCH --gpus-per-node=V100:[1-4] 576 T4 (8) #SBATCH -C MEM576 #SBATCH --gpus-per-node=T4:[1-8] 1536 T4 (8) #SBATCH -C MEM1536 #SBATCH --gpus-per-node=A40:[1-4] 512 A100 (4) and a no GPU icelake #SBATCH -C mem512 #SBATCH --gpus-per-node=A100:[1-4] 256 A40 (4, no IB) A100 (4) #SBATCH -C mem256 and either #SBATCH --gpus-per-node=A40[1-4] or #SBATCH --gpus-per-node=A100[1-4] 1024 A100fat (4) #SBATCH -C mem1024 #SBATCH --gpus-per-node=A100fat:[1-4] Note be aware, though that you also need to ask for a GPU, as usual, unless you need the pre/post processing CPU nodes ( -C NOGPU ). You only really need to give the memory constraint for those bolded as the others follow from the GPU choice sinfo -o \"%20N %9P %4c %24f %50G\" will give you a full list of all nodes and features Type RAM/core cores/node requesting flag Intel Skylake 6785 MB 28 -C skylake AMD Zen3 8020 MB 128 -C zen3 AMD Zen4 2516 MB 256 -C zen4 V100 6785 MB 28 --gpus=<#num> -C v100 A100 10600 MB 48 --gpus=<#num> -C a100 MI100 10600 MB 48 --gpus=<#num> -C mi100 A6000 6630 MB 48 --gpus=<#num> -C a6000 H100 6630 MB 96 --gpus=<#num> -C h100 L40s 11968 MB 64 --gpus=<#num> -C l40s A40 11968 MB 64 --gpus=<#num> -C a40 Largemem 41666 MB 72 -C largemem Type RAM/core cores/node requesting flag AMD 7413 5.3 GB 48 Intel / A100 12 GB 32 -p gpua100i AMD / A100 10.7 GB 48 -p gpua100 Type RAM/node RAM/core cores/node requesting flag AMD EPYC 9454P (Zen4) 768 GB 16 GB 48 -p pelle AMD EPYC 9454P (Zen4) 2 or 3 TB 41.67 or 62.5 GB 48 -p fat 2xAMD EPYC 9124 (Zen4), 10xL40s 384 GB 12 GB 32 -p gpu --gpus=l40s:[1-10] 2xAMD EPYC 9124 (Zen4), 2xH100 384 GB 12 GB 32 -p gpu --gpus=h100:[1-2] In addition you can use all the Slurm options for memory: --mem --mem-per-cpu --mem-per-gpu to specify memory requirements. Pelle at UPPMAX The compute node CPUs have Simultaneous multithreading (SMT) enabled. Each CPU core runs two Threads. In Slurm the Threads are referred to as CPUs. If you suspect SMT degrades the performance of your jobs, you can you can specify --threads-per-core=1 in your job. More information here: Simultaneous multi-threading .","title":"Memory availability"},{"location":"jobscripts/#io__intensive__jobs","text":"Note This section comes with many caveats; it depends a lot on the type of job and the system. Often, if you are in the situation where you have an I/O intensive job, you need to talk to support as it will be very individualized. Not all systems offer node local discs The Dardel system does not offer node local discs. The use of $SNIC_TMP , $NAISS_TMP and $TMPDIR is discouraged. $SNIC_TMP and $NAISS_TMP do not offer a performance advantage over the project storage. In addition they are not protected against name space conflicts by jobs submitted by the same user, which are running on different nodes. $TMPDIR will utilise the node\u2019s RAM, which in most cases defeats the purpose of using $TMPDIR . In most cases, you should use the project storage Centre-dependent. If needed you can use node-local disc for single-node jobs Remember you need to copy data to/from the node-local scratch ( $SNIC_TMP )! On some systems $TMPDIR also points to the node local disc The environment variable $SLURM_SUBMIT_DIR is the directory you submitted from On Tetralith, the data access between /home or /proj and GPU/CPU compute nodes are not suitable for I/O intensive jobs => use /scratch/local ( $SNIC_TMP )","title":"I/O intensive jobs"},{"location":"jobscripts/#example","text":"#!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n <cores> module load <modules> # Copy your data etc. to node local scratch disc cp -p mydata.dat $SNIC_TMP cp -p myprogram $SNIC_TMP # Change to that directory cd $SNIC_TMP # Run your program ./myprogram # Copy the results back to the submission directory cp -p mynewdata.dat $SLURM_SUBMIT_DIR NOTE When using node local disk it is important to remember to copy the output data back, since it will go away when the job ends!","title":"Example"},{"location":"jobscripts/#job__arrays","text":"Job arrays: a mechanism for submitting and managing collections of similar jobs. All jobs must have the same initial options (e.g. size, time limit, etc.) the execution times can vary depending on input data You create multiple jobs from one script, using the -- array directive. This requires very little BASH scripting abilities max number of jobs is restricted by max number of jobs/user - centre specific More information here on the official Slurm documentation pages. Example This shows how to run a small Python script hello-world-array.py as an array. # import sys library (we need this for the command line args) import sys # print task number print ( 'Hello world! from task number: ' , sys . argv [ 1 ]) You could then make a batch script like this, hello-world-array.sh : #!/bin/bash # A very simple example of how to run a Python script with a job array #SBATCH -A <account> #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --array=1-10 # how many tasks in the array #SBATCH -c 1 # Asking for 1 core # one core per task # Create specific output files for each task with the environment variable %j # which contains the job id and %a for each step #SBATCH -o hello-world-%j-%a.out # Load any modules you need module load <module> <python-module> # Run your Python script srun python hello-world-array.py $SLURM_ARRAY_TASK_ID Hint Try it! You can find the above script under any of the cluster resources folders in the exercise tarball.","title":"Job arrays"},{"location":"jobscripts/#some__array__comments","text":"Default step of 1 Example: #SBATCH --array=4-80 Give an index (here steps of 4) Example: #SBATCH --array=1-100:4 Give a list instead of a range Example: #SBATCH --array=5,8,33,38 Throttle jobs, so only a smaller number of jobs run at a time Example: #SBATCH --array1-400%4 Name output/error files so each job ( %j or %A ) and step ( %a ) gets own file #SBATCH -o process_%j_%a.out #SBATCH -e process_%j_%a.err There is an environment variable $SLURM_ARRAY_TASK_ID which can be used to check/query with","title":"Some array comments"},{"location":"jobscripts/#gpu__jobs","text":"There are some differences between the centres in Sweden what type of GPUs they have. The command sinfo -o \"%10P %20l %30N %10z %10c %20m %20f %20G\" | grep gpu is very useful as well to identify the GPUs available on a cluster. Resource cores/node RAM/node GPUs, type (per node) Tetralith 32 96-384 GB Nvidia T4 GPUs (1) Dardel 128 256-2048 GB 4 AMD Instinct\u2122 MI250X (2) Alvis 16 (skylake 2xV100), 32 (skylake 4xV100, 8xT4), 64 (icelake 4xA40, 4xA100) 256-1024 GB Nvidia v100 (2), v100 (4), T4 (8), A40 (4), A100 (4) Kebnekaise 28 (skylake), 72 (largemem), 128/256 (Zen3/Zen4) 128-3072 GB NVidia v100 (2), NVidia a100 (2), NVidia a6000 (2), NVidia l40s (2 or 6), NVidia H100 (4), NVidia A40 (8), AMD MI100 (2) Cosmos 32 (Intel) or 48 (AMD) 256-512 GB A100 Pelle 32 384 GB L40s (10), H100 (2) Alvis also has a small number of nodes without GPUs, for heavy-duty pre- and post-processing that does not require a GPU. To use, specify the constraint -C NOGPU in your Slurm script.","title":"GPU jobs"},{"location":"jobscripts/#allocating__a__gpu","text":"This is the most different of the Slurm settings, between centers. Resource batch settings Comments Tetralith #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu Alvis #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x - no node-sharing on multi-node jobs ( --exclusive is automatic) - Requesting -N 1 does not mean 1 full node - type is V100, A40, A100, or A100fat - x is number of GPUs, 1-4 Cosmos #SBATCH -p gpua100 #SBATCH --gres=gpu:1 Kebnekaise #SBATCH --gpus=x #SBATCH -C <type> - type is the type of GPU in lower case - x is the number of that type of GPU. See above table for both Pelle -p gpu --gpus=<type>:x - type is the type of GPU in lower case - x is the number of that type of GPU. See above table for both","title":"Allocating a GPU"},{"location":"jobscripts/#example__gpu__scripts","text":"This shows a simple GPU script, asking for 1 or 2 cards on a single node. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code> #!/bin/bash -l # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for resources, including GPU resources #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code - REMEMBER NOT NVIDIA/CUDA!> #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A naissXXXX-YY-ZZZ #SBATCH -t HHH:MM:SS #SBATCH -p alvis #SBATCH -N 1 --gpus-per-node=T4:4 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load any needed GPU modules and any prerequisites module load <MODULES> <run-my-GPU-code> #!/bin/bash #SBATCH -A hpc2nXXXX-YYY # Change to your own project ID #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS # Ask for GPU resources. You pick type as one of the ones shown above # and how many cards you want, at most as many as shown above. Here 2 L40s #SBATCH --gpus:2 #SBATCH -C l40s # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Purge unneeded modules. Load any needed GPU modules and any prerequisites ml purge > /dev/null 2 > & 1 module load <MODULES> <run-my-GPU-code> #!/bin/bash # Remember to change this to your own project ID! #SBATCH -A luXXXX-Y-ZZ # Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH --time=HHH:MM:SS # Ask for GPU resources - x is how many cards, 1 or 2 #SBATCH -p gpua100 #SBATCH --gres=gpu:1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Remove any loaded modules and load the GPU modules and prerequisites ones we need module purge > /dev/null 2 > & 1 module load <MODULES> <run-my-GPU-code> #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZZ # Change to your own! #Asking for runtime: hours, minutes, seconds. At most 1 week #SBATCH -t HHH:MM:SS #SBATCH -p gpu #SBATCH --gpus:l40s:1 # Writing output and error files #SBATCH --output=output%J.out #SBATCH --error=error%J.error # Load the GPU modules we need module load <MODULES> <run-my-GPU-code> Hint You can find a few example GPU batch scripts and corresponding programs in the cluster subfolders in the exercises tarball. Some of them requires installing some Python packages in a virtual environment. It is described in the .sh file for each alvis, cosmos, kebnekaise, pelle, tetralith add-list.py, add-list.sh pytorch_fitting_gpu.py, pytorch_fitting_gpu.sh integration2d_gpu.py, integration2d_gpu_shared.py, job-gpu.sh dardel hello_world_gpu.cpp, hello_world_gpu.sh","title":"Example GPU scripts"},{"location":"jobscripts/#miscellaneous","text":"There are many other types of jobs in Slurm. Here are a few more examples. Note If you are on Dardel, you also need to add a partition.","title":"Miscellaneous"},{"location":"jobscripts/#multiple__serial__jobs__simultaneously","text":"#!/bin/bash #SBATCH -A <job ID> # Add enough cores that all jobs can run at the same time #SBATCH -n <cores> # Make sure the time is long enough that the longest job have time to finish #SBATCH --time=HHH:MM:SS module load <modules> srun -n 1 --exclusive ./program data1 & srun -n 1 --exclusive ./program2 data2 & srun -n 1 --exclusive ./program3 data3 & ... srun -n 1 --exclusive ./program4 data4 & wait","title":"Multiple serial jobs, simultaneously"},{"location":"jobscripts/#multiple__simultaneous__jobs__serial__or__parallel","text":"In this example, 3 jobs each with 14 cores #!/bin/bash #SBATCH -A <job ID> # Since the files run simultaneously I need enough cores for all of them to run #SBATCH -n 56 # Remember to ask for enough time for all jobs to complete #SBATCH --time=00:10:00 module load <modules> srun -n 14 --exclusive ./mympiprogram data & srun -n 14 --exclusive ./my2mpi data2 & srun -n 14 --exclusive ./my3mpi data3 & wait","title":"Multiple simultaneous jobs (serial or parallel)"},{"location":"jobscripts/#multiple__sequential__jobs__serial__or__parallel","text":"This example is for jobs where some are with 14 tasks with 2 cores per task and some are 4 tasks with 4 cores per task #!/bin/bash #SBATCH -A <job ID> # Since the programs run sequentially I only need enough cores for the largest of them to run #SBATCH -c 28 # Remember to ask for enough time for all jobs to complete #SBATCH --time=HHH:MM:SS module load <modules> srun -n 14 -c 2 ./myprogram data srun -n 4 -c 4 ./myotherprogram mydata ... srun -n 14 -c 2 ./my2program data2","title":"Multiple sequential jobs (serial or parallel)"},{"location":"monitoring/","text":"Job monitoring and efficiency \u00b6 This section looks at how to monitor your job(s), including to see if they are efficient. Many of the relevant commands have already been discussed in previous parts: squeue : for viewing the state of the batch queue. More here: https://uppmax.github.io/NAISS_Slurm/slurm/#squeue scancel : to cancel a job. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#scancel sinfo : information about the partitions/queues. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#sinfo scontrol show job : lots of information about a job. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#scontrol__show__job But there are several others that have either not been mentioned or only done so briefly, including sacct , projinfo , `sshare`` and a number of center specific commands. We will look more into all of them here. Why is a job ineffective? \u00b6 There are several reasons that a job might be ineffective. Some of those could be: using more threads than the allocated number of cores not using all the cores you have allocated (unless on purpose/for memory) inefficient use of the file system (many small files, open/close many files) running a job that could run on GPUs on CPUs instead Job monitoring is (also) about detecting signs the job is not running efficiently. This can be done with many different commands. Job monitoring \u00b6 Now let us look at some of the commands that are generally available, as well as those that are specific to one or more centres. Commands valid at all centres \u00b6 Command What scontrol show job JOBID info about a job, including estimated start time squeue --me --start your running and queued jobs with estimated start time sacct -l -j JOBID info about j ob, pipe to less -S for scrolling side-ways (it is a wide output) projinfo usage of your project, adding -vd lists member usage sshare -l -A <proj-account> gives priority/fairshare (LevelIFS) Most up-to-date project usage on a project\u2019s SUPR page, linked from here: https://supr.naiss.se/project/ Site-specific commands \u00b6 Command What Cluster jobinfo wrapper around squeue Bianca, Cosmos, Alvis jobstats -p JOBID CPU and memory use of finished job (> 5 min) in a plot Bianca job_stats.py link to Grafana dashboard with overview of your running jobs. Add JOBID for real-time usage of a job Alvis job-usage JOBID grafana graphics of resource use for job (> few minutes) Kebnekaise jobload JOBID show cpu and memory usage in a job Tetralith jobsh NODE login to node, run \u201ctop\u201d Tetralith seff JOBID displays memory and CPU usage from job run Tetralith, Dardel lastjobs lists 10 most recent job in recent 30 days Tetralith https://pdc-web.eecs.kth.se/cluster_usage/ Information about project usage Dardel https://grafana.c3se.chalmers.se/d/user-jobs/user-jobs Grafana dashboard for user jobs Alvis https://www.nsc.liu.se/support/batch-jobs/tetralith/monitoring/ Job monitoring Tetralith https://docs.uppmax.uu.se/software/jobstats/ Job efficiency Bianca","title":"Job monitoring and efficiency"},{"location":"monitoring/#job__monitoring__and__efficiency","text":"This section looks at how to monitor your job(s), including to see if they are efficient. Many of the relevant commands have already been discussed in previous parts: squeue : for viewing the state of the batch queue. More here: https://uppmax.github.io/NAISS_Slurm/slurm/#squeue scancel : to cancel a job. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#scancel sinfo : information about the partitions/queues. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#sinfo scontrol show job : lots of information about a job. More info here: https://uppmax.github.io/NAISS_Slurm/slurm/#scontrol__show__job But there are several others that have either not been mentioned or only done so briefly, including sacct , projinfo , `sshare`` and a number of center specific commands. We will look more into all of them here.","title":"Job monitoring and efficiency"},{"location":"monitoring/#why__is__a__job__ineffective","text":"There are several reasons that a job might be ineffective. Some of those could be: using more threads than the allocated number of cores not using all the cores you have allocated (unless on purpose/for memory) inefficient use of the file system (many small files, open/close many files) running a job that could run on GPUs on CPUs instead Job monitoring is (also) about detecting signs the job is not running efficiently. This can be done with many different commands.","title":"Why is a job ineffective?"},{"location":"monitoring/#job__monitoring","text":"Now let us look at some of the commands that are generally available, as well as those that are specific to one or more centres.","title":"Job monitoring"},{"location":"monitoring/#commands__valid__at__all__centres","text":"Command What scontrol show job JOBID info about a job, including estimated start time squeue --me --start your running and queued jobs with estimated start time sacct -l -j JOBID info about j ob, pipe to less -S for scrolling side-ways (it is a wide output) projinfo usage of your project, adding -vd lists member usage sshare -l -A <proj-account> gives priority/fairshare (LevelIFS) Most up-to-date project usage on a project\u2019s SUPR page, linked from here: https://supr.naiss.se/project/","title":"Commands valid at all centres"},{"location":"monitoring/#site-specific__commands","text":"Command What Cluster jobinfo wrapper around squeue Bianca, Cosmos, Alvis jobstats -p JOBID CPU and memory use of finished job (> 5 min) in a plot Bianca job_stats.py link to Grafana dashboard with overview of your running jobs. Add JOBID for real-time usage of a job Alvis job-usage JOBID grafana graphics of resource use for job (> few minutes) Kebnekaise jobload JOBID show cpu and memory usage in a job Tetralith jobsh NODE login to node, run \u201ctop\u201d Tetralith seff JOBID displays memory and CPU usage from job run Tetralith, Dardel lastjobs lists 10 most recent job in recent 30 days Tetralith https://pdc-web.eecs.kth.se/cluster_usage/ Information about project usage Dardel https://grafana.c3se.chalmers.se/d/user-jobs/user-jobs Grafana dashboard for user jobs Alvis https://www.nsc.liu.se/support/batch-jobs/tetralith/monitoring/ Job monitoring Tetralith https://docs.uppmax.uu.se/software/jobstats/ Job efficiency Bianca","title":"Site-specific commands"},{"location":"mpi/","text":"MPI (Message Passing Interface \u00b6 In order to take advantage of more than one core per job and get speed-up this way, you need to do some extra programming. In return, the job will be able to run across several processors that communicate over the local network - distributed parallelism. MPI is a way of doing distributed parallelism. MPI is a language-independent communications protocol used to program parallel computers. There are several MPI implementations, but most of them consists of a set of routines that can be called from Fortran, C, C++, and Python, as well as any language that can interface with their libraries. MPI is very portable and generally optimized for the hardware it runs on, so it will be reasonably fast. Programs that are parallelizable should be reasonably easy to convert to MPI programs by adding MPI routines to it. What kinds of programs can be parallelized? \u00b6 For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"A bit about MPI"},{"location":"mpi/#mpi__message__passing__interface","text":"In order to take advantage of more than one core per job and get speed-up this way, you need to do some extra programming. In return, the job will be able to run across several processors that communicate over the local network - distributed parallelism. MPI is a way of doing distributed parallelism. MPI is a language-independent communications protocol used to program parallel computers. There are several MPI implementations, but most of them consists of a set of routines that can be called from Fortran, C, C++, and Python, as well as any language that can interface with their libraries. MPI is very portable and generally optimized for the hardware it runs on, so it will be reasonably fast. Programs that are parallelizable should be reasonably easy to convert to MPI programs by adding MPI routines to it.","title":"MPI (Message Passing Interface"},{"location":"mpi/#what__kinds__of__programs__can__be__parallelized","text":"For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"What kinds of programs can be parallelized?"},{"location":"openmp/","text":"Shared memory \u00b6 Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. \u00a7","title":"Short about OpenMP"},{"location":"openmp/#shared__memory","text":"Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. \u00a7","title":"Shared memory"},{"location":"slurm/","text":"Introduction to Slurm \u00b6 The batch system used at UPPMAX, HPC2N, LUNARC, NSC, PDC, C3SE (and most other HPC centres in Sweden) is called Slurm. Guides and documentation HPC2N: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ UPPMAX: https://docs.uppmax.uu.se/cluster_guides/slurm/ LUNARC: https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ NSC: https://www.nsc.liu.se/support/batch-jobs/introduction/ PDC: https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ C3SE: https://www.c3se.chalmers.se/documentation/submitting_jobs/ Slurm is an Open Source job scheduler, which provides three key functions: Keeps track of available system resources - it allocates to users, exclusive or non-exclusive access to resources for some period of time Enforces local system resource usage and job scheduling policies - provides a framework for starting, executing, and monitoring work Manages a job queue, distributing work across resources according to policies Slurm is designed to handle thousands of nodes in a single cluster, and can sustain throughput of 120,000 jobs per hour. You can run programs either by giving all the commands on the command line or by submitting a job script. Using a job script is often recommended: If you ask for the resources on the command line, you will wait for the program to run before you can use the window again (unless you can send it to the background with & ). If you use a job script you have an easy record of the commands you used, to reuse or edit for later use. In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Slurm commands \u00b6 There are many more commands than the ones we have chosen to look at here, but these are the most commonly used ones. You can find more information on the Slurm homepage: Slurm documentation . salloc : requesting an interactive allocation interactive : another way of requesting an interactive allocation sbatch : submitting jobs to the batch system squeue : viewing the state of the batch queue scancel : cancel a job scontrol show : getting more info on jobs, nodes sinfo : information about the partitions/queues Let us look at these one at a time. salloc and interactive \u00b6 This will be covered in the next section about Interactive. sbatch \u00b6 The command sbatch is used to submit jobs to the batch system. This is done from the command line in the same way at all the HPC centres in Sweden: sbatch <batchscript.sh> For any batch submit script <batchscript.sh> . You can name the submit script whatever you want to. It is a convention to use the suffix .sbatch or .sh , but it is not a requirement. You can use any or no suffix. It is merely to make it easier to find the script among the other files. Note At clusters that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At clusters that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB. We will talk much more about batch scripts in a short while, but for now we can use this small batch script for testing the Slurm commands: #!/bin/bash # Project id - change to your own! #SBATCH -A PROJ-ID # Asking for 1 core #SBATCH -n 1 # Asking for a walltime of 1 min #SBATCH --time=00:01:00 echo \"What is the hostname? It is this: \" /bin/hostname Hint You find the above batch script named \u201csimple.sh\u201d in the exercises tarball, under the folder \u201cintroslurm\u201d. REMEMBER TO CHANGE THE PROJECT ID TO YOUR OWN! If you are on Dardel, you also need to add a partition in order to make it run. Use the \u201cdardel-simple.sh\u201d from the tarball in that case. Checking for Slurm project IDs valid for your user From the terminal, you can ask Slurm for project IDs currently associated with your user with the command projinfo , on most clusters. If not available, use this command: sacctmgr list assoc where user=\"$USER\" format=Account -P --noheader Example : Submitting the above batch script on Tetralith (NSC) [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194426 As you can see, you get the job id when submitting the batch script. When it has run, you can see with ls that you got a file called slurm-JOBID.out in your directory. Hint Try it out! squeue \u00b6 The command squeue is for viewing the state of the batch queue. If you just give the command, you will get a long list of all jobs in the queue, so it is usually best to constrain it to your own jobs. This can be done in two ways: squeue -u <username> squeue --me Example : b-an01 [ ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 34815904 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1404 34815905 cpu_sky mpi_hell bbrydsoe R 0 :00 2 b-cn [ 1404 ,1511 ] 34815906 cpu_sky mpi_hi.s bbrydsoe R 0 :00 2 b-cn [ 1511 -1512 ] 34815907 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1512 34815908 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1415 ,1512 ] 34815909 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1415 34815910 cpu_sky mpi_hell bbrydsoe R 0 :00 3 b-cn [ 1415 ,1421-1422 ] 34815911 cpu_sky mpi_hi.s bbrydsoe R 0 :00 1 b-cn1422 34815912 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1422 34815913 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1422 ,1427 ] 34815902 cpu_zen4 simple.s bbrydsoe CG 0 :03 1 b-cn1707 34815903 cpu_zen4 compiler bbrydsoe R 0 :00 1 b-cn1708 34815898 cpu_zen4 compiler bbrydsoe R 0 :03 2 b-cn [ 1703 ,1705 ] 34815899 cpu_zen4 mpi_gree bbrydsoe R 0 :03 2 b-cn [ 1705 ,1707 ] 34815900 cpu_zen4 mpi_hell bbrydsoe R 0 :03 1 b-cn1707 34815901 cpu_zen4 mpi_hi.s bbrydsoe R 0 :03 1 b-cn1707 34815922 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815921 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815920 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815919 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Priority ) 34815918 cpu_zen4, compiler bbrydsoe PD 0 :00 1 ( Priority ) 34815917 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815916 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815915 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815914 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Resources ) Here you also see some of the \u201cstates\u201d a job can be in. Some of the more common ones are: CA : CANCELLED. Job was explicitly cancelled by the user or system administrator. CF : CONFIGURING. Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG : COMPLETING. Job is in the process of completing. Some processes on some nodes may still be active. PD : PENDING. Job is awaiting resource allocation. R : RUNNING. Job currently has an allocation. S : SUSPENDED. Job has an allocation, but execution has been suspended and resources have been released for other jobs. List above from Slurm workload manager page about squeue . Example : Submit the \u201csimple.sh\u201d script several times, then do squeue --me to see that it is running, pending, or completing. [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194596 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194597 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194598 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194599 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194600 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194601 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194602 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194603 [ x_birbr@tetralith3 ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 45194603 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194602 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194601 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194600 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194599 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194598 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194597 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194596 tetralith simple.s x_birbr PD 0 :00 1 ( None ) Hint Try it! Remember, \u201carrow up\u201d lets you quickly access a previous command. scancel \u00b6 The command to cancel a job is scancel . You can either cancel a specific job: scancel <job id> or cancel all your jobs: scancel -u <username> Note As before, you get the <job id> either from when you submitted the job or from squeue --me . Note Only administrators can cancel other people\u2019s jobs! scontrol show \u00b6 The command scontrol show is used for getting more info on jobs and nodes. scontrol show job \u00b6 As usual, you get the <job id> from either when you submit the job or from squeue --me . The command is: scontrol show job <job id> Example : b-an01 [ ~ ] $ scontrol show job 34815931 JobId = 34815931 JobName = compiler-run UserId = bbrydsoe ( 2897 ) GroupId = folk ( 3001 ) MCS_label = N/A Priority = 2748684 Nice = 0 Account = staff QOS = normal JobState = COMPLETED Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:07 TimeLimit = 00 :10:00 TimeMin = N/A SubmitTime = 2025 -06-24T11:36:32 EligibleTime = 2025 -06-24T11:36:32 AccrueTime = 2025 -06-24T11:36:32 StartTime = 2025 -06-24T11:36:32 EndTime = 2025 -06-24T11:36:39 Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2025 -06-24T11:36:32 Scheduler = Main Partition = cpu_zen4 AllocNode:Sid = b-an01:626814 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = b-cn [ 1703 ,1705 ] BatchHost = b-cn1703 NumNodes = 2 NumCPUs = 12 NumTasks = 12 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* ReqTRES = cpu = 12 ,mem = 30192M,node = 1 ,billing = 12 AllocTRES = cpu = 12 ,mem = 30192M,node = 2 ,billing = 12 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryCPU = 2516M MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :02:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/compile-run.sh WorkDir = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage StdErr = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out StdIn = /dev/null StdOut = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out Power = Here you get much interesting information: JobState=COMPLETED : the job was completed and was not FAILED. It could also have been PENDING or COMPLETING RunTime=00:00:07 : the job ran for 7 seconds TimeLimit=00:10:00 : It could have run for up to 10 min (what you asked for) SubmitTime=2025-06-24T11:36:32 : when your job was submitted StartTime=2025-06-24T11:36:32 : when the job started Partition=cpu_zen4 : what partition/type of node it ran on NodeList=b-cn[1703,1705] : which specific nodes it ran on BatchHost=b-cn1703 : which of the nodes (if several) that was the master NumNodes=2 NumCPUs=12 NumTasks=12 CPUs/Task=1 : number of nodes, cpus, tasks WorkDir=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage : which directory your job was submitted from/was running in StdOut=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out : which directory the output files will be placed in The command scontrol show job <job id> can be run also while the job is pending, and can be used to get an estimate of when the job will start. Actual start time depends on the jobs priority, any other (people\u2019s) jobs starting and completing and being submitted, etc. It is often useful to know which nodes a job ran on if something did not work - perhaps the node was faulty. Exercise Try it! Submit the \u201csimple.sh\u201d batch script and then do scontrol show job JOBID using the job ID you got when you submitted the batch script. Try to find the \u201cSubmitTime\u201d, \u201cStartTime\u201d, and \u201cNodeList\u201d in the output. scontrol show node \u00b6 This command is used to get information about a specific node. You can for instance see its features, how many cores per socket, uptime, etc. Specifics will vary and depend on the cluster you are running jobs at. Example : This is for one of the AMD Zen4 nodes at Kebnekaise, HPC2N. b-an01 [ ~ ] $ scontrol show node b-cn1703 NodeName = b-cn1703 Arch = x86_64 CoresPerSocket = 128 CPUAlloc = 253 CPUEfctv = 256 CPUTot = 256 CPULoad = 253 .38 AvailableFeatures = rack17,amd_cpu,zen4 ActiveFeatures = rack17,amd_cpu,zen4 Gres =( null ) NodeAddr = b-cn1703 NodeHostName = b-cn1703 Version = 23 .02.7 OS = Linux 5 .15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 RealMemory = 644096 AllocMem = 636548 FreeMem = 749623 Sockets = 2 Boards = 1 State = MIXED ThreadsPerCore = 1 TmpDisk = 0 Weight = 100 Owner = N/A MCS_label = N/A Partitions = cpu_zen4 BootTime = 2025 -06-24T06:32:25 SlurmdStartTime = 2025 -06-24T06:37:02 LastBusyTime = 2025 -06-24T08:29:45 ResumeAfterTime = None CfgTRES = cpu = 256 ,mem = 629G,billing = 256 AllocTRES = cpu = 253 ,mem = 636548M CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s sinfo \u00b6 The command sinfo gives you information about the partitions/queues. Example : This is for Tetralith, NSC [ x_birbr@tetralith3 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST tetralith* up 7 -00:00:00 1 plnd n1541 tetralith* up 7 -00:00:00 16 drain* n [ 237 ,245,439,532,625,646,712,759-760,809,1290,1364,1455,1638,1847,1864 ] tetralith* up 7 -00:00:00 3 drain n [ 13 ,66,454 ] tetralith* up 7 -00:00:00 20 resv n [ 1 -4,108,774,777,779-780,784-785,788,1109,1268,1281-1285,1288 ] tetralith* up 7 -00:00:00 327 mix n [ 7 ,39-40,46,50,69-70,75,78,85,91-93,97,112,119,121,124,126,128,130-134,137,139,141,149-150,156,159,164,167-168,170,174,184,187-188,190,193,196,203,206,208,212,231,241,244,257,259,262,267,280,287,293-294,310,315,323,327,329,333,340,350,352,371,377,379-381,385,405,420-422,434,441,446,465,467,501,504-505,514,524,529,549,553,558,561,564,573,575,602,610,612,615,617,622-623,626-627,631,637,651,662,671,678,691-692,699,703,709,718,720,723,726,741,745,752,754-755,768,776,781,790,792-793,803-804,808,818,853,855,859,863,867,881,883,915,925,959,966,974,981,984,999,1001-1003,1007-1011,1015,1018,1033,1044-1046,1050-1052,1056-1058,1071,1077,1102,1105-1106,1111-1115,1117,1119,1130,1132,1134-1136,1138-1140,1142-1143,1252,1254,1257,1267,1278-1279,1292,1296,1298,1309,1328,1339,1343,1345,1347,1349,1352,1354-1355,1357,1367,1375-1376,1379,1381,1386-1388,1398,1403,1410,1412,1420-1422,1428,1440,1446,1450,1459,1466,1468-1470,1474,1490-1491,1493,1498,1506,1510,1513,1520,1524,1529,1548-1549,1553,1562,1574-1575,1579,1586,1592,1595,1601,1606,1608,1612,1615,1620-1621,1631,1634,1639,1642,1647,1651-1653,1665,1688,1690,1697,1702,1706,1715-1716,1725,1728,1749,1754,1756,1767,1772,1774-1775,1778,1795-1796,1798-1799,1811,1816,1822,1826,1834,1842,1849,1857-1858,1871,1874,1879,1881,1896,1900,1902,1909,1911-1914,1945,1951,1953,1955-1956,1960,1969,1978,1983,2001,2005-2006,2008 ] tetralith* up 7 -00:00:00 1529 alloc n [ 5 -6,8-12,14-38,41-45,47-49,51-60,65,67-68,71-74,76-77,79-84,86-90,94-96,98-107,109-111,113-118,120,122-123,125,127,129,135-136,138,140,142-148,151-155,157-158,160-163,165-166,169,171-173,175-183,185-186,189,191-192,194-195,197-202,204-205,207,209-211,213-230,232-236,238-240,242-243,246-256,258,260-261,263-266,268-279,281-286,288-292,295-309,311-314,316-322,324-326,328,330-332,334-339,341-349,351,353-370,372-376,378,382-384,386-404,406-419,423-433,435-438,440,442-445,447-453,455-464,466,468-500,502-503,506-513,515-523,525-528,530-531,533-548,550-552,554-557,559-560,562-563,565-572,574,576-601,603-609,611,613-614,616,618-621,624,628-630,632-636,638-645,647-650,652-661,663-670,672-677,679-690,693-698,700-702,704-708,710-711,713-717,719,721-722,724-725,727-740,742-744,746-751,753,756-758,761-767,769-773,775,778,782-783,786-787,789,791,794-802,805-807,810-817,819-852,854,856-858,860-862,864-866,868-880,882,884-889,893,896-914,916-924,926-937,939-940,943,945,948-958,960-965,967-973,975-980,982-983,985-998,1000,1004-1006,1012-1014,1016-1017,1019-1032,1034-1043,1047-1049,1053-1055,1059-1070,1072-1076,1078-1101,1103-1104,1107-1108,1110,1116,1118,1120-1129,1131,1133,1137,1141,1144,1249-1251,1253,1255-1256,1258-1266,1269-1277,1280,1286-1287,1289,1291,1293-1295,1297,1299-1308,1310-1327,1329-1338,1340-1342,1344,1346,1348,1350-1351,1353,1356,1358-1363,1365-1366,1368-1374,1377-1378,1380,1382-1385,1389-1397,1399-1402,1404-1409,1411,1413-1419,1423-1427,1429-1439,1441-1445,1447-1449,1451-1454,1456-1458,1460-1465,1467,1471-1473,1475-1489,1492,1494-1497,1499-1505,1507-1509,1511-1512,1514-1519,1521-1523,1525-1528,1530-1540,1542-1547,1550-1552,1554-1561,1563-1573,1576-1578,1580-1585,1587-1591,1593-1594,1596-1600,1602-1605,1607,1609-1611,1613-1614,1616-1619,1622-1630,1632-1633,1635-1637,1640-1641,1643-1646,1648-1650,1654-1664,1666-1687,1689,1691-1696,1698-1701,1703-1705,1707-1714,1717-1724,1726-1727,1729-1748,1750-1753,1755,1757-1766,1768-1771,1773,1776-1777,1779-1794,1797,1800-1810,1812-1815,1817-1821,1824-1825,1827-1833,1835-1841,1843-1846,1848,1850-1856,1859-1863,1865-1870,1872-1873,1875-1878,1880,1882-1895,1897-1899,1901,1903-1908,1910,1915-1944,1946-1950,1952,1954,1957-1959,1961-1968,1970-1977,1979-1982,1984-2000,2002-2004,2007,2009-2016 ] As you can see, it shows partitions, nodes, and states. State can be drain, idle, resv, alloc, mix, plnd (and a few others), where the exact naming varies between centers. drain : node is draining after running a job resv : node is reserved/has a reservation for something alloc : node is allocated for a job mix : node is in several states, could for instance be that it is allocated, but starting to drain idle : node is free and can be allocated plnd : job planned for a higher priority job You can see the full list of states and their meaning with man sinfo . Hint Try it! Give the command sinfo and look at the output from your chosen HPC cluster. Slurm job scripts \u00b6 Now we have looked at the commands to control the job, but what about the job scripts? We had a small example further up on the page, which we used to test the commands, but now we will look more at the job scripts themselves. Simplest job \u00b6 The simplest possible batch script would look something like this: Other Dardel This works for most of the clusters, except Dardel which also requires you to give the partition. #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 #SBATCH -n 1 echo $HOSTNAME Dardel requires you to give the partition. There are several: \u201cmain\u201d, \u201cshared\u201d, \u201clong\u201d, \u201cmemory\u201d, \u201cgpu\u201d. In most cases when you are running a regular CPU job on less than all cores on a node, you should use \u201cshared\u201d. #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 #SBATCH -n 1 #SBATCH -p shared echo $HOSTNAME Note A job submission file can either be very simple, with most of the job attributes specified on the command line, or it may consist of several Slurm directives, comments and executable statements. A Slurm directive provides a way of specifying job attributes in addition to the command line options. All Slurm directives are prefaced with #SBATCH which must be written with capital letters. Comments are added with a # in front - an extra # in front of the Slurm directive comment that out. Common Slurm arguments Some of the most commonly used arguments are: -A PROJ-ID : The project that should be accounted. It is a simple conversion from the SUPR project id. You can also find your project account with the command projinfo. The PROJ-ID argument is of the form naissXXXX-YY-ZZZ -N : number of nodes -n , --ntasks= : number of tasks. Since cores-per-task is 1 as default, this then translates to number of cores. NOTE that you cannot be sure the cores all end up on the same node. If you have a threaded job or otherwise need to have all the cores on the same node, you should instead use -c or a combination of -N and -c . -c , --cores-per-task= : This changes the number of cores each task may use. Can also be used for getting more memory, with some cores only providing memory. (example: -c 2 -n 4 allocates 4 tasks and 2 cores per task, totally 8 cores). More about this argument later. -t , --time= : walltime. How long your job is allowed to run. Given as HHH:MM:SS (example: 4 hours and 20 min is given as 4:20:00). Different clusters have different maximum walltime, but it is usually at least a week. -p : partition. Only used at some clusters. On Dardel it is required. In addition, these can be quite useful: -o , --output= : Used for naming the output differently than slurm-<job-id>.out and splitting it from errors and such. A %j in the specified filename will be replaced with the job id, which is highly recommended to prevent output from being overwritten the next time you run the job. Additional specifiers for the filename pattern , for more advanced cases, can be found in the Slurm documentation. -e , --error= : Used for naming and splitting the error from the other output. Using %j in the name to get separate files for separate runs is again highly recommended. Example: #SBATCH -o process_%j.out #SBATCH -e process_%j.err Now for the example script, \u201cSimplest job\u201d, above : The first line is called the \u201cshebang\u201d and it indicates that the script is written in the bash shell language. The second, third, and fourth lines are resource statements to the Slurm batch scheduler. Also called Slurm directives. The second line above is where you put your project ID . Depending on cluster, this is either always required or not technically required if you only have one project to your name. Regardless, we recommend that you make a habit of including it. The third line in the example above provides the walltime , the maximum amount of time that the program would be allowed to run (5 minutes in this example). If a job does not finish within the specified walltime, the resource management system terminates it and any data that were not already written to a file before time ran out are lost. The fourth line asks for compute resources, here one task (resulting in one core as cores-per-task is defaulting to one). You could ask for more cores if it was a parallel job, or for GPUs and so on. More about that later. The last line in the above sample is the code to be executed by the batch script. In this case, it just prints the name of the server on which the code ran. All of the parameters that Slurm needs to determine which resources to allocate, under whose account, and for how long, are given as a series of resource statements of the form #SBATCH -<option> <value> or #SBATCH --<key-words>=<value> (note: < and > are not typically used in real arguments; they are just used here to indicate placeholder text). Alternatively they can be given as command-line options to sbatch but it is generally useful to save them in the script. Depending on cluster, for most compute nodes, unless otherwise specified, a batch script will run on 1 core of 1 node by default. However, at several clusters it is required to always give the number of cores or nodes, so you should make it a habit to include it. Note You find the above job script in the exercises tarball. It is named \u201cfirst.sh\u201d. Remember to change the project ID to your own! Some comments on time/walltime the job will terminate when the time runs out, whether it has finished or not you will only be \u201ccharged\u201d for the consumed time asking for a lot more time than needed will generally make the job take longer to start short jobs can start quicker (backfill) if you have no idea how long your job takes, ask for \u201clong\u201d time Conclusion : It is typically a good idea to overbook your job with perhaps 30% (or more). Note There are many more resource statements and other commands that can go into the Slurm batch script. We will look at some of them in the next section, where we show some sample job scripts. You submit the job script with sbatch <jobscript.sh> as was mentioned earlier. Dependencies \u00b6 Sometimes your workflow has jobs that depend on a previous job (a pipeline). This can be handled through Slurm (if many, make a script): Submit your first job: sbatch my-job.sh Wait for that job to finish before starting next job: sbatch -d afterok:<prev-JOBID> my-next-job.sh Generally: after:jobid[:jobid...] begin after specified jobs have started afterany:jobid[:jobid...] begin after specified jobs have terminated afternotok:jobid[:jobid...] begin after specified jobs have failed afterok:jobid[:jobid...] begin after specified jobs have run to completion with exit code zero singleton begin execution after all previously launched jobs with the same name and user have ended Hint Try it! You can use matrix-gen.sh as the first and mmmult-v2.sh as the dependent job. You find these batch scripts in the exercises tarball, under your cluster. Remember to change the project ID of the scripts to be your project ID. Remember, you can use squeue --me to see if your jobs are running - and probably also that one of them is now marked as being dependent on another. Script example \u00b6 This simple script can be run from the command line. It starts one job; gets the job ID, then tell Slurm to wait until job one has finished before starting a second job which is dependent on the first. #!/bin/bash # first job - no dependencies jid1 = $( sbatch --parsable matrix-gen.sh ) # Next job depend on first job sbatch --dependency = afterany: ${ jid1 } mmmult-v2.sh If you want to test it, the scripts for this can be found in the tarball with the exercises under the cluster name and then \u201cdependency\u201d. The first job it runs generates two matrices and then the second job does matrix-matrix multiplication, but not until the first has finished. How to monitor jobs \u00b6 Unless mentioned, these are valid at all clusters. Use the following: sacct : sacct -l -j JOBID . Lots of (wide format) info about a job with job-id JOBID jobinfo (LUNARC, C3SE): wrapper around squeue scontrol show job JOBID : info about a job, including estimated start time, assigned nodes, working directory, etc. squeue --me --start : your running and queued jobs with estimated start time job_usage (HPC2N): grafana graphics of resource use for job (> few minutes) and several more See Job monitoring and efficiency for more about job info, including several commands that are site-specific and very useful.","title":"Introduction to Slurm"},{"location":"slurm/#introduction__to__slurm","text":"The batch system used at UPPMAX, HPC2N, LUNARC, NSC, PDC, C3SE (and most other HPC centres in Sweden) is called Slurm. Guides and documentation HPC2N: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ UPPMAX: https://docs.uppmax.uu.se/cluster_guides/slurm/ LUNARC: https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ NSC: https://www.nsc.liu.se/support/batch-jobs/introduction/ PDC: https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ C3SE: https://www.c3se.chalmers.se/documentation/submitting_jobs/ Slurm is an Open Source job scheduler, which provides three key functions: Keeps track of available system resources - it allocates to users, exclusive or non-exclusive access to resources for some period of time Enforces local system resource usage and job scheduling policies - provides a framework for starting, executing, and monitoring work Manages a job queue, distributing work across resources according to policies Slurm is designed to handle thousands of nodes in a single cluster, and can sustain throughput of 120,000 jobs per hour. You can run programs either by giving all the commands on the command line or by submitting a job script. Using a job script is often recommended: If you ask for the resources on the command line, you will wait for the program to run before you can use the window again (unless you can send it to the background with & ). If you use a job script you have an easy record of the commands you used, to reuse or edit for later use. In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script).","title":"Introduction to Slurm"},{"location":"slurm/#slurm__commands","text":"There are many more commands than the ones we have chosen to look at here, but these are the most commonly used ones. You can find more information on the Slurm homepage: Slurm documentation . salloc : requesting an interactive allocation interactive : another way of requesting an interactive allocation sbatch : submitting jobs to the batch system squeue : viewing the state of the batch queue scancel : cancel a job scontrol show : getting more info on jobs, nodes sinfo : information about the partitions/queues Let us look at these one at a time.","title":"Slurm commands"},{"location":"slurm/#salloc__and__interactive","text":"This will be covered in the next section about Interactive.","title":"salloc and interactive"},{"location":"slurm/#sbatch","text":"The command sbatch is used to submit jobs to the batch system. This is done from the command line in the same way at all the HPC centres in Sweden: sbatch <batchscript.sh> For any batch submit script <batchscript.sh> . You can name the submit script whatever you want to. It is a convention to use the suffix .sbatch or .sh , but it is not a requirement. You can use any or no suffix. It is merely to make it easier to find the script among the other files. Note At clusters that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At clusters that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB. We will talk much more about batch scripts in a short while, but for now we can use this small batch script for testing the Slurm commands: #!/bin/bash # Project id - change to your own! #SBATCH -A PROJ-ID # Asking for 1 core #SBATCH -n 1 # Asking for a walltime of 1 min #SBATCH --time=00:01:00 echo \"What is the hostname? It is this: \" /bin/hostname Hint You find the above batch script named \u201csimple.sh\u201d in the exercises tarball, under the folder \u201cintroslurm\u201d. REMEMBER TO CHANGE THE PROJECT ID TO YOUR OWN! If you are on Dardel, you also need to add a partition in order to make it run. Use the \u201cdardel-simple.sh\u201d from the tarball in that case. Checking for Slurm project IDs valid for your user From the terminal, you can ask Slurm for project IDs currently associated with your user with the command projinfo , on most clusters. If not available, use this command: sacctmgr list assoc where user=\"$USER\" format=Account -P --noheader Example : Submitting the above batch script on Tetralith (NSC) [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194426 As you can see, you get the job id when submitting the batch script. When it has run, you can see with ls that you got a file called slurm-JOBID.out in your directory. Hint Try it out!","title":"sbatch"},{"location":"slurm/#squeue","text":"The command squeue is for viewing the state of the batch queue. If you just give the command, you will get a long list of all jobs in the queue, so it is usually best to constrain it to your own jobs. This can be done in two ways: squeue -u <username> squeue --me Example : b-an01 [ ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 34815904 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1404 34815905 cpu_sky mpi_hell bbrydsoe R 0 :00 2 b-cn [ 1404 ,1511 ] 34815906 cpu_sky mpi_hi.s bbrydsoe R 0 :00 2 b-cn [ 1511 -1512 ] 34815907 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1512 34815908 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1415 ,1512 ] 34815909 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1415 34815910 cpu_sky mpi_hell bbrydsoe R 0 :00 3 b-cn [ 1415 ,1421-1422 ] 34815911 cpu_sky mpi_hi.s bbrydsoe R 0 :00 1 b-cn1422 34815912 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1422 34815913 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1422 ,1427 ] 34815902 cpu_zen4 simple.s bbrydsoe CG 0 :03 1 b-cn1707 34815903 cpu_zen4 compiler bbrydsoe R 0 :00 1 b-cn1708 34815898 cpu_zen4 compiler bbrydsoe R 0 :03 2 b-cn [ 1703 ,1705 ] 34815899 cpu_zen4 mpi_gree bbrydsoe R 0 :03 2 b-cn [ 1705 ,1707 ] 34815900 cpu_zen4 mpi_hell bbrydsoe R 0 :03 1 b-cn1707 34815901 cpu_zen4 mpi_hi.s bbrydsoe R 0 :03 1 b-cn1707 34815922 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815921 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815920 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815919 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Priority ) 34815918 cpu_zen4, compiler bbrydsoe PD 0 :00 1 ( Priority ) 34815917 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815916 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815915 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815914 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Resources ) Here you also see some of the \u201cstates\u201d a job can be in. Some of the more common ones are: CA : CANCELLED. Job was explicitly cancelled by the user or system administrator. CF : CONFIGURING. Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG : COMPLETING. Job is in the process of completing. Some processes on some nodes may still be active. PD : PENDING. Job is awaiting resource allocation. R : RUNNING. Job currently has an allocation. S : SUSPENDED. Job has an allocation, but execution has been suspended and resources have been released for other jobs. List above from Slurm workload manager page about squeue . Example : Submit the \u201csimple.sh\u201d script several times, then do squeue --me to see that it is running, pending, or completing. [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194596 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194597 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194598 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194599 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194600 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194601 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194602 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194603 [ x_birbr@tetralith3 ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 45194603 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194602 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194601 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194600 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194599 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194598 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194597 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194596 tetralith simple.s x_birbr PD 0 :00 1 ( None ) Hint Try it! Remember, \u201carrow up\u201d lets you quickly access a previous command.","title":"squeue"},{"location":"slurm/#scancel","text":"The command to cancel a job is scancel . You can either cancel a specific job: scancel <job id> or cancel all your jobs: scancel -u <username> Note As before, you get the <job id> either from when you submitted the job or from squeue --me . Note Only administrators can cancel other people\u2019s jobs!","title":"scancel"},{"location":"slurm/#scontrol__show","text":"The command scontrol show is used for getting more info on jobs and nodes.","title":"scontrol show"},{"location":"slurm/#scontrol__show__job","text":"As usual, you get the <job id> from either when you submit the job or from squeue --me . The command is: scontrol show job <job id> Example : b-an01 [ ~ ] $ scontrol show job 34815931 JobId = 34815931 JobName = compiler-run UserId = bbrydsoe ( 2897 ) GroupId = folk ( 3001 ) MCS_label = N/A Priority = 2748684 Nice = 0 Account = staff QOS = normal JobState = COMPLETED Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:07 TimeLimit = 00 :10:00 TimeMin = N/A SubmitTime = 2025 -06-24T11:36:32 EligibleTime = 2025 -06-24T11:36:32 AccrueTime = 2025 -06-24T11:36:32 StartTime = 2025 -06-24T11:36:32 EndTime = 2025 -06-24T11:36:39 Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2025 -06-24T11:36:32 Scheduler = Main Partition = cpu_zen4 AllocNode:Sid = b-an01:626814 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = b-cn [ 1703 ,1705 ] BatchHost = b-cn1703 NumNodes = 2 NumCPUs = 12 NumTasks = 12 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* ReqTRES = cpu = 12 ,mem = 30192M,node = 1 ,billing = 12 AllocTRES = cpu = 12 ,mem = 30192M,node = 2 ,billing = 12 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryCPU = 2516M MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :02:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/compile-run.sh WorkDir = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage StdErr = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out StdIn = /dev/null StdOut = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out Power = Here you get much interesting information: JobState=COMPLETED : the job was completed and was not FAILED. It could also have been PENDING or COMPLETING RunTime=00:00:07 : the job ran for 7 seconds TimeLimit=00:10:00 : It could have run for up to 10 min (what you asked for) SubmitTime=2025-06-24T11:36:32 : when your job was submitted StartTime=2025-06-24T11:36:32 : when the job started Partition=cpu_zen4 : what partition/type of node it ran on NodeList=b-cn[1703,1705] : which specific nodes it ran on BatchHost=b-cn1703 : which of the nodes (if several) that was the master NumNodes=2 NumCPUs=12 NumTasks=12 CPUs/Task=1 : number of nodes, cpus, tasks WorkDir=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage : which directory your job was submitted from/was running in StdOut=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out : which directory the output files will be placed in The command scontrol show job <job id> can be run also while the job is pending, and can be used to get an estimate of when the job will start. Actual start time depends on the jobs priority, any other (people\u2019s) jobs starting and completing and being submitted, etc. It is often useful to know which nodes a job ran on if something did not work - perhaps the node was faulty. Exercise Try it! Submit the \u201csimple.sh\u201d batch script and then do scontrol show job JOBID using the job ID you got when you submitted the batch script. Try to find the \u201cSubmitTime\u201d, \u201cStartTime\u201d, and \u201cNodeList\u201d in the output.","title":"scontrol show job"},{"location":"slurm/#scontrol__show__node","text":"This command is used to get information about a specific node. You can for instance see its features, how many cores per socket, uptime, etc. Specifics will vary and depend on the cluster you are running jobs at. Example : This is for one of the AMD Zen4 nodes at Kebnekaise, HPC2N. b-an01 [ ~ ] $ scontrol show node b-cn1703 NodeName = b-cn1703 Arch = x86_64 CoresPerSocket = 128 CPUAlloc = 253 CPUEfctv = 256 CPUTot = 256 CPULoad = 253 .38 AvailableFeatures = rack17,amd_cpu,zen4 ActiveFeatures = rack17,amd_cpu,zen4 Gres =( null ) NodeAddr = b-cn1703 NodeHostName = b-cn1703 Version = 23 .02.7 OS = Linux 5 .15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 RealMemory = 644096 AllocMem = 636548 FreeMem = 749623 Sockets = 2 Boards = 1 State = MIXED ThreadsPerCore = 1 TmpDisk = 0 Weight = 100 Owner = N/A MCS_label = N/A Partitions = cpu_zen4 BootTime = 2025 -06-24T06:32:25 SlurmdStartTime = 2025 -06-24T06:37:02 LastBusyTime = 2025 -06-24T08:29:45 ResumeAfterTime = None CfgTRES = cpu = 256 ,mem = 629G,billing = 256 AllocTRES = cpu = 253 ,mem = 636548M CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s","title":"scontrol show node"},{"location":"slurm/#sinfo","text":"The command sinfo gives you information about the partitions/queues. Example : This is for Tetralith, NSC [ x_birbr@tetralith3 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST tetralith* up 7 -00:00:00 1 plnd n1541 tetralith* up 7 -00:00:00 16 drain* n [ 237 ,245,439,532,625,646,712,759-760,809,1290,1364,1455,1638,1847,1864 ] tetralith* up 7 -00:00:00 3 drain n [ 13 ,66,454 ] tetralith* up 7 -00:00:00 20 resv n [ 1 -4,108,774,777,779-780,784-785,788,1109,1268,1281-1285,1288 ] tetralith* up 7 -00:00:00 327 mix n [ 7 ,39-40,46,50,69-70,75,78,85,91-93,97,112,119,121,124,126,128,130-134,137,139,141,149-150,156,159,164,167-168,170,174,184,187-188,190,193,196,203,206,208,212,231,241,244,257,259,262,267,280,287,293-294,310,315,323,327,329,333,340,350,352,371,377,379-381,385,405,420-422,434,441,446,465,467,501,504-505,514,524,529,549,553,558,561,564,573,575,602,610,612,615,617,622-623,626-627,631,637,651,662,671,678,691-692,699,703,709,718,720,723,726,741,745,752,754-755,768,776,781,790,792-793,803-804,808,818,853,855,859,863,867,881,883,915,925,959,966,974,981,984,999,1001-1003,1007-1011,1015,1018,1033,1044-1046,1050-1052,1056-1058,1071,1077,1102,1105-1106,1111-1115,1117,1119,1130,1132,1134-1136,1138-1140,1142-1143,1252,1254,1257,1267,1278-1279,1292,1296,1298,1309,1328,1339,1343,1345,1347,1349,1352,1354-1355,1357,1367,1375-1376,1379,1381,1386-1388,1398,1403,1410,1412,1420-1422,1428,1440,1446,1450,1459,1466,1468-1470,1474,1490-1491,1493,1498,1506,1510,1513,1520,1524,1529,1548-1549,1553,1562,1574-1575,1579,1586,1592,1595,1601,1606,1608,1612,1615,1620-1621,1631,1634,1639,1642,1647,1651-1653,1665,1688,1690,1697,1702,1706,1715-1716,1725,1728,1749,1754,1756,1767,1772,1774-1775,1778,1795-1796,1798-1799,1811,1816,1822,1826,1834,1842,1849,1857-1858,1871,1874,1879,1881,1896,1900,1902,1909,1911-1914,1945,1951,1953,1955-1956,1960,1969,1978,1983,2001,2005-2006,2008 ] tetralith* up 7 -00:00:00 1529 alloc n [ 5 -6,8-12,14-38,41-45,47-49,51-60,65,67-68,71-74,76-77,79-84,86-90,94-96,98-107,109-111,113-118,120,122-123,125,127,129,135-136,138,140,142-148,151-155,157-158,160-163,165-166,169,171-173,175-183,185-186,189,191-192,194-195,197-202,204-205,207,209-211,213-230,232-236,238-240,242-243,246-256,258,260-261,263-266,268-279,281-286,288-292,295-309,311-314,316-322,324-326,328,330-332,334-339,341-349,351,353-370,372-376,378,382-384,386-404,406-419,423-433,435-438,440,442-445,447-453,455-464,466,468-500,502-503,506-513,515-523,525-528,530-531,533-548,550-552,554-557,559-560,562-563,565-572,574,576-601,603-609,611,613-614,616,618-621,624,628-630,632-636,638-645,647-650,652-661,663-670,672-677,679-690,693-698,700-702,704-708,710-711,713-717,719,721-722,724-725,727-740,742-744,746-751,753,756-758,761-767,769-773,775,778,782-783,786-787,789,791,794-802,805-807,810-817,819-852,854,856-858,860-862,864-866,868-880,882,884-889,893,896-914,916-924,926-937,939-940,943,945,948-958,960-965,967-973,975-980,982-983,985-998,1000,1004-1006,1012-1014,1016-1017,1019-1032,1034-1043,1047-1049,1053-1055,1059-1070,1072-1076,1078-1101,1103-1104,1107-1108,1110,1116,1118,1120-1129,1131,1133,1137,1141,1144,1249-1251,1253,1255-1256,1258-1266,1269-1277,1280,1286-1287,1289,1291,1293-1295,1297,1299-1308,1310-1327,1329-1338,1340-1342,1344,1346,1348,1350-1351,1353,1356,1358-1363,1365-1366,1368-1374,1377-1378,1380,1382-1385,1389-1397,1399-1402,1404-1409,1411,1413-1419,1423-1427,1429-1439,1441-1445,1447-1449,1451-1454,1456-1458,1460-1465,1467,1471-1473,1475-1489,1492,1494-1497,1499-1505,1507-1509,1511-1512,1514-1519,1521-1523,1525-1528,1530-1540,1542-1547,1550-1552,1554-1561,1563-1573,1576-1578,1580-1585,1587-1591,1593-1594,1596-1600,1602-1605,1607,1609-1611,1613-1614,1616-1619,1622-1630,1632-1633,1635-1637,1640-1641,1643-1646,1648-1650,1654-1664,1666-1687,1689,1691-1696,1698-1701,1703-1705,1707-1714,1717-1724,1726-1727,1729-1748,1750-1753,1755,1757-1766,1768-1771,1773,1776-1777,1779-1794,1797,1800-1810,1812-1815,1817-1821,1824-1825,1827-1833,1835-1841,1843-1846,1848,1850-1856,1859-1863,1865-1870,1872-1873,1875-1878,1880,1882-1895,1897-1899,1901,1903-1908,1910,1915-1944,1946-1950,1952,1954,1957-1959,1961-1968,1970-1977,1979-1982,1984-2000,2002-2004,2007,2009-2016 ] As you can see, it shows partitions, nodes, and states. State can be drain, idle, resv, alloc, mix, plnd (and a few others), where the exact naming varies between centers. drain : node is draining after running a job resv : node is reserved/has a reservation for something alloc : node is allocated for a job mix : node is in several states, could for instance be that it is allocated, but starting to drain idle : node is free and can be allocated plnd : job planned for a higher priority job You can see the full list of states and their meaning with man sinfo . Hint Try it! Give the command sinfo and look at the output from your chosen HPC cluster.","title":"sinfo"},{"location":"slurm/#slurm__job__scripts","text":"Now we have looked at the commands to control the job, but what about the job scripts? We had a small example further up on the page, which we used to test the commands, but now we will look more at the job scripts themselves.","title":"Slurm job scripts"},{"location":"slurm/#simplest__job","text":"The simplest possible batch script would look something like this: Other Dardel This works for most of the clusters, except Dardel which also requires you to give the partition. #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 #SBATCH -n 1 echo $HOSTNAME Dardel requires you to give the partition. There are several: \u201cmain\u201d, \u201cshared\u201d, \u201clong\u201d, \u201cmemory\u201d, \u201cgpu\u201d. In most cases when you are running a regular CPU job on less than all cores on a node, you should use \u201cshared\u201d. #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 #SBATCH -n 1 #SBATCH -p shared echo $HOSTNAME Note A job submission file can either be very simple, with most of the job attributes specified on the command line, or it may consist of several Slurm directives, comments and executable statements. A Slurm directive provides a way of specifying job attributes in addition to the command line options. All Slurm directives are prefaced with #SBATCH which must be written with capital letters. Comments are added with a # in front - an extra # in front of the Slurm directive comment that out. Common Slurm arguments Some of the most commonly used arguments are: -A PROJ-ID : The project that should be accounted. It is a simple conversion from the SUPR project id. You can also find your project account with the command projinfo. The PROJ-ID argument is of the form naissXXXX-YY-ZZZ -N : number of nodes -n , --ntasks= : number of tasks. Since cores-per-task is 1 as default, this then translates to number of cores. NOTE that you cannot be sure the cores all end up on the same node. If you have a threaded job or otherwise need to have all the cores on the same node, you should instead use -c or a combination of -N and -c . -c , --cores-per-task= : This changes the number of cores each task may use. Can also be used for getting more memory, with some cores only providing memory. (example: -c 2 -n 4 allocates 4 tasks and 2 cores per task, totally 8 cores). More about this argument later. -t , --time= : walltime. How long your job is allowed to run. Given as HHH:MM:SS (example: 4 hours and 20 min is given as 4:20:00). Different clusters have different maximum walltime, but it is usually at least a week. -p : partition. Only used at some clusters. On Dardel it is required. In addition, these can be quite useful: -o , --output= : Used for naming the output differently than slurm-<job-id>.out and splitting it from errors and such. A %j in the specified filename will be replaced with the job id, which is highly recommended to prevent output from being overwritten the next time you run the job. Additional specifiers for the filename pattern , for more advanced cases, can be found in the Slurm documentation. -e , --error= : Used for naming and splitting the error from the other output. Using %j in the name to get separate files for separate runs is again highly recommended. Example: #SBATCH -o process_%j.out #SBATCH -e process_%j.err Now for the example script, \u201cSimplest job\u201d, above : The first line is called the \u201cshebang\u201d and it indicates that the script is written in the bash shell language. The second, third, and fourth lines are resource statements to the Slurm batch scheduler. Also called Slurm directives. The second line above is where you put your project ID . Depending on cluster, this is either always required or not technically required if you only have one project to your name. Regardless, we recommend that you make a habit of including it. The third line in the example above provides the walltime , the maximum amount of time that the program would be allowed to run (5 minutes in this example). If a job does not finish within the specified walltime, the resource management system terminates it and any data that were not already written to a file before time ran out are lost. The fourth line asks for compute resources, here one task (resulting in one core as cores-per-task is defaulting to one). You could ask for more cores if it was a parallel job, or for GPUs and so on. More about that later. The last line in the above sample is the code to be executed by the batch script. In this case, it just prints the name of the server on which the code ran. All of the parameters that Slurm needs to determine which resources to allocate, under whose account, and for how long, are given as a series of resource statements of the form #SBATCH -<option> <value> or #SBATCH --<key-words>=<value> (note: < and > are not typically used in real arguments; they are just used here to indicate placeholder text). Alternatively they can be given as command-line options to sbatch but it is generally useful to save them in the script. Depending on cluster, for most compute nodes, unless otherwise specified, a batch script will run on 1 core of 1 node by default. However, at several clusters it is required to always give the number of cores or nodes, so you should make it a habit to include it. Note You find the above job script in the exercises tarball. It is named \u201cfirst.sh\u201d. Remember to change the project ID to your own! Some comments on time/walltime the job will terminate when the time runs out, whether it has finished or not you will only be \u201ccharged\u201d for the consumed time asking for a lot more time than needed will generally make the job take longer to start short jobs can start quicker (backfill) if you have no idea how long your job takes, ask for \u201clong\u201d time Conclusion : It is typically a good idea to overbook your job with perhaps 30% (or more). Note There are many more resource statements and other commands that can go into the Slurm batch script. We will look at some of them in the next section, where we show some sample job scripts. You submit the job script with sbatch <jobscript.sh> as was mentioned earlier.","title":"Simplest job"},{"location":"slurm/#dependencies","text":"Sometimes your workflow has jobs that depend on a previous job (a pipeline). This can be handled through Slurm (if many, make a script): Submit your first job: sbatch my-job.sh Wait for that job to finish before starting next job: sbatch -d afterok:<prev-JOBID> my-next-job.sh Generally: after:jobid[:jobid...] begin after specified jobs have started afterany:jobid[:jobid...] begin after specified jobs have terminated afternotok:jobid[:jobid...] begin after specified jobs have failed afterok:jobid[:jobid...] begin after specified jobs have run to completion with exit code zero singleton begin execution after all previously launched jobs with the same name and user have ended Hint Try it! You can use matrix-gen.sh as the first and mmmult-v2.sh as the dependent job. You find these batch scripts in the exercises tarball, under your cluster. Remember to change the project ID of the scripts to be your project ID. Remember, you can use squeue --me to see if your jobs are running - and probably also that one of them is now marked as being dependent on another.","title":"Dependencies"},{"location":"slurm/#script__example","text":"This simple script can be run from the command line. It starts one job; gets the job ID, then tell Slurm to wait until job one has finished before starting a second job which is dependent on the first. #!/bin/bash # first job - no dependencies jid1 = $( sbatch --parsable matrix-gen.sh ) # Next job depend on first job sbatch --dependency = afterany: ${ jid1 } mmmult-v2.sh If you want to test it, the scripts for this can be found in the tarball with the exercises under the cluster name and then \u201cdependency\u201d. The first job it runs generates two matrices and then the second job does matrix-matrix multiplication, but not until the first has finished.","title":"Script example"},{"location":"slurm/#how__to__monitor__jobs","text":"Unless mentioned, these are valid at all clusters. Use the following: sacct : sacct -l -j JOBID . Lots of (wide format) info about a job with job-id JOBID jobinfo (LUNARC, C3SE): wrapper around squeue scontrol show job JOBID : info about a job, including estimated start time, assigned nodes, working directory, etc. squeue --me --start : your running and queued jobs with estimated start time job_usage (HPC2N): grafana graphics of resource use for job (> few minutes) and several more See Job monitoring and efficiency for more about job info, including several commands that are site-specific and very useful.","title":"How to monitor jobs"},{"location":"summary/","text":"Summary \u00b6 Today we have discussed: Basic architecture of a typical HPC cluster Nodes as building blocks Login nodes - don\u2019t do heavy lifting here Compute node GPU nodes Job scheduler concepts You interact with the scheduler on the front end The scheduler moves your \u201crun\u201d to the compute nodes Waiting queue Job script Description and requirements of your job Request resources, e.g.: Job time Number of nodes/cores Amount of memory Number of GPUs Includes UNIX script to describe your job to the system Program(s) to execute Location of input data Where to put the output data Disks to be utilised You can be creative here ;)","title":"Summary"},{"location":"summary/#summary","text":"Today we have discussed: Basic architecture of a typical HPC cluster Nodes as building blocks Login nodes - don\u2019t do heavy lifting here Compute node GPU nodes Job scheduler concepts You interact with the scheduler on the front end The scheduler moves your \u201crun\u201d to the compute nodes Waiting queue Job script Description and requirements of your job Request resources, e.g.: Job time Number of nodes/cores Amount of memory Number of GPUs Includes UNIX script to describe your job to the system Program(s) to execute Location of input data Where to put the output data Disks to be utilised You can be creative here ;)","title":"Summary"},{"location":"images/image_sources/","text":"Readme about the odp presentations \u00b6 The odp slide sets used to generate the images are created in google docs. If you need to modify them a typical libre office installation can not handle them properly. Re-import in to google docs and manipulate in there. The powerpoint files must not be modified in libre office, open office.","title":"Readme about the odp presentations"},{"location":"images/image_sources/#readme__about__the__odp__presentations","text":"The odp slide sets used to generate the images are created in google docs. If you need to modify them a typical libre office installation can not handle them properly. Re-import in to google docs and manipulate in there. The powerpoint files must not be modified in libre office, open office.","title":"Readme about the odp presentations"}]}