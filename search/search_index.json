{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the \u201cRunning jobs on HPC systems\u201d course materials \u00b6 Objectives In this course you will learn about: Cluster architecture sbatch options for CPU job scripts Writing submission scripts for: I/O intensive jobs OpenMP and MPI jobs Job arrays and simple task farms Jobs with more memory per task Running jobs on GPUs Monitoring jobs and their efficiency","title":"Home"},{"location":"#welcome__to__the__running__jobs__on__hpc__systems__course__materials","text":"Objectives In this course you will learn about: Cluster architecture sbatch options for CPU job scripts Writing submission scripts for: I/O intensive jobs OpenMP and MPI jobs Job arrays and simple task farms Jobs with more memory per task Running jobs on GPUs Monitoring jobs and their efficiency","title":"Welcome to the &ldquo;Running jobs on HPC systems&rdquo; course materials"},{"location":"cluster/","text":"Introduction to clusters \u00b6 This section is a beginner\u2019s guide to clusters, and provides general information about computer clusters like Tetralith, Dardel, Cosmos, Rackham, Kebnekaise and other HPC systems, but is not directly focused on any of them. What is a cluster \u00b6 A computer cluster consists of a number of computers (few or many), linked together and working closely together. In many ways, the computer cluster works as a single computer. Generally, the component-computers are connected to each other through fast local area networks (LANs). The advantage of computer clusters over single computers, are that they usually improves the performance (and availability) greatly, while still being cheaper than single computers of comparable speed and size. What are Nodes, Cores, and CPUs? \u00b6 A node is the name usually used for one unit (usually one computer) in a computer cluster. Generally, this computer will have one or two central processing units, or CPUs, each normally with (many) more than one core. Each core is a single processor able to handle a single programmed task. Memory is always shared between cores on the same CPU, but generally not between the CPUs. Computer nodes can also have GPUs (graphical processing units) in addition to the CPUs. Nodes in computer clusters are usually arranged in racks at a dedicated climate- controlled facility, and are connected via a communication network. With few exceptions, nearly all high-performance computer clusters use the Linux operating system. Normally, clusters have some sort of batch or queuing system to handle scheduling of jobs. On Linux systems, that batch or queuing system is most often Slurm (the Simple Linux Utility for Resource Management). What is a Supercomputer? Is it the same as a Cluster? \u00b6 A supercomputer is simply a computer with a processing capacity (generally calculation speed) several orders of magnitude better than a typical personal computer. For many years, supercomputers were single computers with many CPUs and usually large volumes of shared memory\u2014sometimes built specifically for a certain task. They have often been custom-built machines, like Cray, and still sometimes are. However, since desktop computers have become cheaper, most supercomputers today are made up of many \u201coff the shelf\u201d ordinary computers connected in parallel. A supercomputer is not the same as a computer cluster, though a computer cluster is often a supercomputer. How is a job run on a computer cluster? What is a batch system? \u00b6 In general, jobs are run with a batch- or queuing system. There are several variants of these, with the most common working by having the user log into a \u201clogin node\u201d and then assembling and submitting their jobs from there. A \u201cjob script\u201d (also called a \u201csubmission script\u201d) will typically be used to start a job. A job script is essentially a list of commands to the batch system telling it things like: how many nodes to use, how many CPU and/or GPU cores, how much memory, how long to run, the program name, any input data, etc. When the job has finished running, it should have produced some files, like output data, perhaps error messages, etc. The syntax of job scripts depends on the queuing system, but that system is so often Slurm that you may see the terms \u201cjob script\u201d and \u201cslurm script\u201d used interchangeably. Since jobs are queued internally and will run whenever the resources for them become available, programs requiring any kind of user interaction are usually not recommended (and often not possible) to be run via a job script. There are special programs like Desktop On-Demand and GfxLauncher that allow graphical programs to be run as scheduled jobs on some HPC clusters, but it is up to the administrators to decide which programs can be run with these tools, how they should be configured, and what options regular users will be allowed to set. Which programs can be run effectively on a computer cluster? \u00b6 Computer clusters are made up of many interconnected nodes, each with a limited number of cores and limited memory capacity. A problem must be parallelizable in order to get any speed-up. Parallelization can be done in several ways. The simplest method is what would usually be done for a parameter sweep: just run the program repeatedly for every desired combination of input parameters. Each task is the same serial program running on a single core, but many copies of the task are running at the same time. Another option is to use a more complex parallelization that requires changes to the program to split its work between multiple processors or nodes that communicate over the network. This is generally done with MPI or similar. It is also possible to parallelize over multiple cores and multiple nodes at the same time, e.g., using multi-threading within CPUs and communicating between nodes with MPI. Many serial jobs \u00b6 Tasks that can be split up into many serial jobs will run faster on a computer cluster. No special programming is needed, but you can only run on one core for each task. It is good for long-running single-threaded jobs. A job scheduler is used to control the flow of tasks. Using a small script, many instances of the same task (like a program run many times, each with slightly different parameters) can be set up. The tasks will be put in a job queue, and will run as free spaces open up in the queue. Normally, the tasks will run many at a time, since they are serial, which means each only uses one core. Example You launch 500 tasks (say, run a small program for 500 different temperatures). There are 50 cores on the machine in our example, that you can access. Fifty instances are started and then run, while the remaining 450 tasks wait. When the running programs finish, the next 50 will start, and so on. It will be is as if you ran on 50 computers instead of one, and you will finish in 1/50th of the time. Of course, this is an ideal example. In reality there may be overhead, waiting time between batches of jobs, etc. so the speed-up will not be as great, but it will certainly run faster. Parallelization \u00b6 Distributed memory \u00b6 WRITE SOMETHING HERE! What kinds of programs can be parallelized? \u00b6 For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication Shared memory \u00b6 Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. GPUs \u00b6 Many computer clusters have GPUs in several of their nodes that jobs may take advantage of. Originally, GPUs were used for computer graphics, but now they are also used extensively for general-purpose computing (GPGPU computing). Image of CPU and GPU components from NVidia. ALU = Arithmetic Logic Unit, and DRAM = Dynamic Random-Access Memory. GPU-driven parallel computing is, among other things, used for: scientific modeling machine learning graphical rendering and other parallelizable jobs. Difference between CPUs and GPUs \u00b6 CPUs (Central Processing Units) are latency-optimized general-purpose processors designed to handle a wide range of distinct tasks sequentially. GPUs (Graphics Processing Units) are throughput-optimized specialized processors designed for high-end parallel computing. Whether you should use a CPU, a GPU, or both depends on the specifics of the problem you are solving. Using GPUs \u00b6 Programs must be written especially for GPUs in order to use them. Several programming frameworks handle the graphical primitives that GPUs understand, like CUDA (Compute Unified Device Architecture), OpenCL, OpenACC, HIP, etc. In addition to the above programming frameworks, you often have the option to use software that is already prepared for use on GPUs. This includes many types of MD software, Python packages, and others.","title":"Intro to clusters"},{"location":"cluster/#introduction__to__clusters","text":"This section is a beginner\u2019s guide to clusters, and provides general information about computer clusters like Tetralith, Dardel, Cosmos, Rackham, Kebnekaise and other HPC systems, but is not directly focused on any of them.","title":"Introduction to clusters"},{"location":"cluster/#what__is__a__cluster","text":"A computer cluster consists of a number of computers (few or many), linked together and working closely together. In many ways, the computer cluster works as a single computer. Generally, the component-computers are connected to each other through fast local area networks (LANs). The advantage of computer clusters over single computers, are that they usually improves the performance (and availability) greatly, while still being cheaper than single computers of comparable speed and size.","title":"What is a cluster"},{"location":"cluster/#what__are__nodes__cores__and__cpus","text":"A node is the name usually used for one unit (usually one computer) in a computer cluster. Generally, this computer will have one or two central processing units, or CPUs, each normally with (many) more than one core. Each core is a single processor able to handle a single programmed task. Memory is always shared between cores on the same CPU, but generally not between the CPUs. Computer nodes can also have GPUs (graphical processing units) in addition to the CPUs. Nodes in computer clusters are usually arranged in racks at a dedicated climate- controlled facility, and are connected via a communication network. With few exceptions, nearly all high-performance computer clusters use the Linux operating system. Normally, clusters have some sort of batch or queuing system to handle scheduling of jobs. On Linux systems, that batch or queuing system is most often Slurm (the Simple Linux Utility for Resource Management).","title":"What are Nodes, Cores, and CPUs?"},{"location":"cluster/#what__is__a__supercomputer__is__it__the__same__as__a__cluster","text":"A supercomputer is simply a computer with a processing capacity (generally calculation speed) several orders of magnitude better than a typical personal computer. For many years, supercomputers were single computers with many CPUs and usually large volumes of shared memory\u2014sometimes built specifically for a certain task. They have often been custom-built machines, like Cray, and still sometimes are. However, since desktop computers have become cheaper, most supercomputers today are made up of many \u201coff the shelf\u201d ordinary computers connected in parallel. A supercomputer is not the same as a computer cluster, though a computer cluster is often a supercomputer.","title":"What is a Supercomputer? Is it the same as a Cluster?"},{"location":"cluster/#how__is__a__job__run__on__a__computer__cluster__what__is__a__batch__system","text":"In general, jobs are run with a batch- or queuing system. There are several variants of these, with the most common working by having the user log into a \u201clogin node\u201d and then assembling and submitting their jobs from there. A \u201cjob script\u201d (also called a \u201csubmission script\u201d) will typically be used to start a job. A job script is essentially a list of commands to the batch system telling it things like: how many nodes to use, how many CPU and/or GPU cores, how much memory, how long to run, the program name, any input data, etc. When the job has finished running, it should have produced some files, like output data, perhaps error messages, etc. The syntax of job scripts depends on the queuing system, but that system is so often Slurm that you may see the terms \u201cjob script\u201d and \u201cslurm script\u201d used interchangeably. Since jobs are queued internally and will run whenever the resources for them become available, programs requiring any kind of user interaction are usually not recommended (and often not possible) to be run via a job script. There are special programs like Desktop On-Demand and GfxLauncher that allow graphical programs to be run as scheduled jobs on some HPC clusters, but it is up to the administrators to decide which programs can be run with these tools, how they should be configured, and what options regular users will be allowed to set.","title":"How is a job run on a computer cluster? What is a batch system?"},{"location":"cluster/#which__programs__can__be__run__effectively__on__a__computer__cluster","text":"Computer clusters are made up of many interconnected nodes, each with a limited number of cores and limited memory capacity. A problem must be parallelizable in order to get any speed-up. Parallelization can be done in several ways. The simplest method is what would usually be done for a parameter sweep: just run the program repeatedly for every desired combination of input parameters. Each task is the same serial program running on a single core, but many copies of the task are running at the same time. Another option is to use a more complex parallelization that requires changes to the program to split its work between multiple processors or nodes that communicate over the network. This is generally done with MPI or similar. It is also possible to parallelize over multiple cores and multiple nodes at the same time, e.g., using multi-threading within CPUs and communicating between nodes with MPI.","title":"Which programs can be run effectively on a computer cluster?"},{"location":"cluster/#many__serial__jobs","text":"Tasks that can be split up into many serial jobs will run faster on a computer cluster. No special programming is needed, but you can only run on one core for each task. It is good for long-running single-threaded jobs. A job scheduler is used to control the flow of tasks. Using a small script, many instances of the same task (like a program run many times, each with slightly different parameters) can be set up. The tasks will be put in a job queue, and will run as free spaces open up in the queue. Normally, the tasks will run many at a time, since they are serial, which means each only uses one core. Example You launch 500 tasks (say, run a small program for 500 different temperatures). There are 50 cores on the machine in our example, that you can access. Fifty instances are started and then run, while the remaining 450 tasks wait. When the running programs finish, the next 50 will start, and so on. It will be is as if you ran on 50 computers instead of one, and you will finish in 1/50th of the time. Of course, this is an ideal example. In reality there may be overhead, waiting time between batches of jobs, etc. so the speed-up will not be as great, but it will certainly run faster.","title":"Many serial jobs"},{"location":"cluster/#parallelization","text":"","title":"Parallelization"},{"location":"cluster/#distributed__memory","text":"WRITE SOMETHING HERE!","title":"Distributed memory"},{"location":"cluster/#what__kinds__of__programs__can__be__parallelized","text":"For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"What kinds of programs can be parallelized?"},{"location":"cluster/#shared__memory","text":"Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous.","title":"Shared memory"},{"location":"cluster/#gpus","text":"Many computer clusters have GPUs in several of their nodes that jobs may take advantage of. Originally, GPUs were used for computer graphics, but now they are also used extensively for general-purpose computing (GPGPU computing). Image of CPU and GPU components from NVidia. ALU = Arithmetic Logic Unit, and DRAM = Dynamic Random-Access Memory. GPU-driven parallel computing is, among other things, used for: scientific modeling machine learning graphical rendering and other parallelizable jobs.","title":"GPUs"},{"location":"cluster/#difference__between__cpus__and__gpus","text":"CPUs (Central Processing Units) are latency-optimized general-purpose processors designed to handle a wide range of distinct tasks sequentially. GPUs (Graphics Processing Units) are throughput-optimized specialized processors designed for high-end parallel computing. Whether you should use a CPU, a GPU, or both depends on the specifics of the problem you are solving.","title":"Difference between CPUs and GPUs"},{"location":"cluster/#using__gpus","text":"Programs must be written especially for GPUs in order to use them. Several programming frameworks handle the graphical primitives that GPUs understand, like CUDA (Compute Unified Device Architecture), OpenCL, OpenACC, HIP, etc. In addition to the above programming frameworks, you often have the option to use software that is already prepared for use on GPUs. This includes many types of MD software, Python packages, and others.","title":"Using GPUs"},{"location":"concepts/","text":"Batch system concepts \u00b6 What is a batch system? \u00b6 A batch system provides a mechanism to submit programs (jobs) or groups of programs to be executed automatically.","title":"Batch system concepts"},{"location":"concepts/#batch__system__concepts","text":"","title":"Batch system concepts"},{"location":"concepts/#what__is__a__batch__system","text":"A batch system provides a mechanism to submit programs (jobs) or groups of programs to be executed automatically.","title":"What is a batch system?"},{"location":"intro/","text":"Introduction to \u201cRunning jobs on HPC systems\u201d \u00b6 Welcome page and syllabus: https://uppmax.github.io/NAISS_Slurm/index.html Link also in the House symbol at the top of the page. Learning outcomes Cluster architecture Login/compute nodes cores, nodes, GPUs memory local disk? sbatch with options for CPU job scripts sample job scripts I/O intensive jobs OpenMP and MPI jobs job arrays simple example for task farming increasing the memory per task / memory hungry jobs Running on GPUs job monitoring, job efficiency how to find optimal sbatch options Login info, project number, project directory \u00b6 Project number and project directory \u00b6 Warning This part is only relevant for people attending the course. It should be ignored if you are doing it as self-study later. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Tetralith at NSC Project ID: naiss2025-22-934 Project storage: /proj/courses-fall-2025/users Dardel at PDC Project ID: naiss2025-22-934 Project storage: /cfs/klemming/projects/supr/courses-fall-2025 Alvis at C3SE Project ID: naiss2025-22-934 Project storage: /mimer/NOBACKUP/groups/courses-fall-2025 Kebnekaise at HPC2N Project ID: hpc2n2025-151 Project storage: /proj/nobackup/fall-courses Cosmos at LUNARC Project ID: Pelle at UPPMAX Project ID: Project storage: Login info \u00b6 You will not need a graphical user interface for this course. Even so, if you do not have a preferred SSH client, we recomment using ThinLinc Connection info Login to the system you are using (Tetralith/Dardel, other Swedish HPC system) Connection info for some Swedish HPC systems - use the one you have access to: Tetralith Dardel Alvis Kebnekaise Pelle Cosmos SSH: ssh <user>@tetralith.nsc.liu.se ThinLinc: Server: tetralith.nsc.liu.se Username: <your-nsc-username> Password: <your-nsc-password> Note that you need to setup TFA to use NSC! SSH: ssh <user>@dardel.pdc.kth.se ThinLinc: Server: dardel-vnc.pdc.kth.se Username: <your-pdc-username> Password: <your-pdc-password> Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se ThinLinc: Server: alvis1.c3se.chalmers.se or alvis2.c3se.chalmers.se Username: <your-c3se-username> Password: <your-c3se-username> OpenOndemand portal: Put https://alvis.c3se.chalmers.se in browser address bar Put <your-c3se-username> and <your-c3se-password> in the login box Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. SSH: ssh <user>@kebnekaise.hpc2n.umu.se ThinLinc: Server: kebnekaise-tl.hpc2n.umu.se Username: <your-hpc2n-username> Password: <yout-hpc2n-password> ThinLinc Webaccess: Put https://kebnekaise-tl.hpc2n.umu.se:300/ in browser address bar Put <your-hpc2n-username> and <your-hpc2n-password> in th e login box that opens and click Login Open OnDemand: https://portal.hpc2n.umu.se SSH: ssh <user>@rackham.uppmax.uu.se ThinLinc: Server: rackham-gui.uppmax.uu.se Username: <your-uppmax-username> Password: <your-uppmax-password> ThinLinc Webaccess: Put https://rackham-gui.uppmax.uu.se in browser address bar Put <your-uppmax-username> and <your-uppmax-password> in the login box that opens and click Login Note that you may have to setup TFA for Uppmax when using either of the ThinLinc connections. SSH: ssh <user>@cosmos.lunarc.lu.se ThinLinc: Server: cosmos-dt.lunarc.lu.se Username: <your-lunarc-username> Password: <your-lunarc-password> Note that you need to setup TFA (PocketPass) to use LUNARC\u2019s systems! Schedule \u00b6 Time Topic Activity Teacher 9:00 - 9:05 Intro to course Lecture 9:05 - 9.25 Intro to clusters Lecture 9:25 - 9:40 Batch system concepts / job scheduling Lecture 9:40 - Parallelism Lecture+type along Intro to Slurm (sbatch, squeue, scontrol, \u2026) Lecture+type along BREAK Additional sample scripts, including job arrays, task farms??? Job monitoring and efficiency Summary Prepare the exercise environment \u00b6 It is now time to login and download the exercises. Login to your cluster. You find login info for several Swedish HPC clusters here . Create a directory to work in: mkdir cluster-intro Fetch the exercises tarball: wget Unpack the tarball: Change to the directory of your cluster. If it is not listed, pick \u201cother\u201d.","title":"Introduction"},{"location":"intro/#introduction__to__running__jobs__on__hpc__systems","text":"Welcome page and syllabus: https://uppmax.github.io/NAISS_Slurm/index.html Link also in the House symbol at the top of the page. Learning outcomes Cluster architecture Login/compute nodes cores, nodes, GPUs memory local disk? sbatch with options for CPU job scripts sample job scripts I/O intensive jobs OpenMP and MPI jobs job arrays simple example for task farming increasing the memory per task / memory hungry jobs Running on GPUs job monitoring, job efficiency how to find optimal sbatch options","title":"Introduction to &ldquo;Running jobs on HPC systems&rdquo;"},{"location":"intro/#login__info__project__number__project__directory","text":"","title":"Login info, project number, project directory"},{"location":"intro/#project__number__and__project__directory","text":"Warning This part is only relevant for people attending the course. It should be ignored if you are doing it as self-study later. Tetralith Dardel Alvis Kebnekaise Cosmos Pelle Tetralith at NSC Project ID: naiss2025-22-934 Project storage: /proj/courses-fall-2025/users Dardel at PDC Project ID: naiss2025-22-934 Project storage: /cfs/klemming/projects/supr/courses-fall-2025 Alvis at C3SE Project ID: naiss2025-22-934 Project storage: /mimer/NOBACKUP/groups/courses-fall-2025 Kebnekaise at HPC2N Project ID: hpc2n2025-151 Project storage: /proj/nobackup/fall-courses Cosmos at LUNARC Project ID: Pelle at UPPMAX Project ID: Project storage:","title":"Project number and project directory"},{"location":"intro/#login__info","text":"You will not need a graphical user interface for this course. Even so, if you do not have a preferred SSH client, we recomment using ThinLinc Connection info Login to the system you are using (Tetralith/Dardel, other Swedish HPC system) Connection info for some Swedish HPC systems - use the one you have access to: Tetralith Dardel Alvis Kebnekaise Pelle Cosmos SSH: ssh <user>@tetralith.nsc.liu.se ThinLinc: Server: tetralith.nsc.liu.se Username: <your-nsc-username> Password: <your-nsc-password> Note that you need to setup TFA to use NSC! SSH: ssh <user>@dardel.pdc.kth.se ThinLinc: Server: dardel-vnc.pdc.kth.se Username: <your-pdc-username> Password: <your-pdc-password> Note that you need to setup SSH keys or kerberos in order to login to PDC! SSH: ssh <user>@alvis1.c3se.chalmers.se or ssh <user>@alvis2.c3se.chalmers.se ThinLinc: Server: alvis1.c3se.chalmers.se or alvis2.c3se.chalmers.se Username: <your-c3se-username> Password: <your-c3se-username> OpenOndemand portal: Put https://alvis.c3se.chalmers.se in browser address bar Put <your-c3se-username> and <your-c3se-password> in the login box Note that Alvis is accessible via SUNET networks (i.e. most Swedish university networks). If you are not on one of those networks you need to use a VPN - preferrably your own Swedish university VPN. If this is not possible, contact support@chalmers.se and ask to be added to the Chalmers\u2019s eduVPN. SSH: ssh <user>@kebnekaise.hpc2n.umu.se ThinLinc: Server: kebnekaise-tl.hpc2n.umu.se Username: <your-hpc2n-username> Password: <yout-hpc2n-password> ThinLinc Webaccess: Put https://kebnekaise-tl.hpc2n.umu.se:300/ in browser address bar Put <your-hpc2n-username> and <your-hpc2n-password> in th e login box that opens and click Login Open OnDemand: https://portal.hpc2n.umu.se SSH: ssh <user>@rackham.uppmax.uu.se ThinLinc: Server: rackham-gui.uppmax.uu.se Username: <your-uppmax-username> Password: <your-uppmax-password> ThinLinc Webaccess: Put https://rackham-gui.uppmax.uu.se in browser address bar Put <your-uppmax-username> and <your-uppmax-password> in the login box that opens and click Login Note that you may have to setup TFA for Uppmax when using either of the ThinLinc connections. SSH: ssh <user>@cosmos.lunarc.lu.se ThinLinc: Server: cosmos-dt.lunarc.lu.se Username: <your-lunarc-username> Password: <your-lunarc-password> Note that you need to setup TFA (PocketPass) to use LUNARC\u2019s systems!","title":"Login info"},{"location":"intro/#schedule","text":"Time Topic Activity Teacher 9:00 - 9:05 Intro to course Lecture 9:05 - 9.25 Intro to clusters Lecture 9:25 - 9:40 Batch system concepts / job scheduling Lecture 9:40 - Parallelism Lecture+type along Intro to Slurm (sbatch, squeue, scontrol, \u2026) Lecture+type along BREAK Additional sample scripts, including job arrays, task farms??? Job monitoring and efficiency Summary","title":"Schedule"},{"location":"intro/#prepare__the__exercise__environment","text":"It is now time to login and download the exercises. Login to your cluster. You find login info for several Swedish HPC clusters here . Create a directory to work in: mkdir cluster-intro Fetch the exercises tarball: wget Unpack the tarball: Change to the directory of your cluster. If it is not listed, pick \u201cother\u201d.","title":"Prepare the exercise environment"},{"location":"jobscripts/","text":"Sample job scripts \u00b6 Basic Serial Job \u00b6 Let\u2019s say you have a simple Python script called mmmult.py that creates 2 random-valued matrices, multiplies them together, and prints the shape of the result and the computation time. Let\u2019s also say that you want to run this code in your current working directory. Here is how you might run that program once on 1 core and 1 node: #!/bin/bash #SBATCH -A <project ID> ### replace with your project ID #SBATCH -t 00:10:00 ### walltime in hh:mm:ss format #SBATCH -J mmmult ### sample job name; customize as desired or omit #SBATCH -o process_%j.out ### filename for stderr - customise, include %j #SBATCH -e process_%j.err ### filename for stderr - customise, include %j #SBATCH -n 1 ### number of cores to use; same as --ntasks-per-node # write this script to stdout-file - useful for scripting errors cat $0 # Purge any loaded modules # Some centres recommend this, while at other centres (PDC in particular) this # should not be done. ml purge > /dev/null 2 > & 1 # Load required modules; customize as needed - this is for LUNARC # Can omit module version number if prerequisites only allow one version ml foss/2023b Python/3.11.5 SciPy-bundle #run the script python3 mmmult.py Examples by centre \u00b6 Let us look at the above batch script as it might be written for some other centres. Tetralith Dardel HPC2N LUNARC UPPMAX mmmult.py #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here GCC 11.3.0 and Python 3.10.4 module load buildtool-easybuild/4.8.0-hpce082752a2 GCC/11.3.0 OpenMPI/4.1.4 Python/3.10.4 SciPy-bundle/2022.05 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for cray-python/3.11.7. module load cray-python/3.11.7 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.3 and compatible SciPy-bundle module load GCC/12.3.0 Python/3.11.3 SciPy-bundle/2023.07 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A luXXXX-Y-ZZ # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.5 and compatible SciPy-bundle module load GCC/13.2.0 Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZ # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here Python 3.11.8. module load python/3.11.8 # Run your Python script python mmmult.py import timeit import numpy as np starttime = timeit . default_timer () np . random . seed ( 1701 ) A = np . random . randint ( - 1000 , 1000 , size = ( 8 , 4 )) B = np . random . randint ( - 1000 , 1000 , size = ( 4 , 4 )) print ( \"This is matrix A: \\n \" , A ) print ( \"The shape of matrix A is \" , A . shape ) print () print ( \"This is matrix B: \\n \" , B ) print ( \"The shape of matrix B is \" , B . shape ) print () print ( \"Doing matrix-matrix multiplication...\" ) print () C = np . matmul ( A , B ) print ( \"The product of matrices A and B is: \\n \" , C ) print ( \"The shape of the resulting matrix is \" , C . shape ) print () print ( \"Time elapsed for generating matrices and multiplying them is \" , timeit . default_timer () - starttime ) There is no example for Alvis since you should only use that for running GPU jobs. OpenMP \u00b6 #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -c <cores-per-task> module load <modules> # Set OMP_NUM_THREADS to the same value as -c with a fallback in case it isn't set. # SLURM_CPUS_PER_TASK is set to the value of -c, but only if -c is explicitly set if [ -n \" $SLURM_CPUS_PER_TASK \" ] ; then omp_threads = $SLURM_CPUS_PER_TASK else omp_threads = 1 fi export OMP_NUM_THREADS = $omp_threads ./myopenmpprogram -c is used to set cores per task and should be the same as OMP_NUM_THREADS Remember, Alvis is only for GPU jobs MPI \u00b6 #!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n <tasks> module load <modules> srun ./mympiprogram Asking for whole nodes ( - N ) and possibly --tasks-per-node srun and mpirun should be interchangeable at many centres. Tetralith uses mpprun and Dardel uses srun Remember, you need to load modules with MPI At some centres mpirun --bind-to-core or srun --cpu-bind=cores is recommended for MPI jobs NOTE: Alvis is only used for GPU jobs Memory-intensive jobs \u00b6 I/O intensive jobs \u00b6 Job arrays \u00b6 Job arrays: a mechanism for submitting and managing collections of similar jobs. All jobs must have the same initial options (e.g. size, time limit, etc.) the execution times can vary depending on input data You create multiple jobs from one script, using the -- array directive. This requires very little BASH scripting abilities max number of jobs is restricted by max number of jobs/user - centre specific More information here on the official Slurm documentation pages. Example This shows how to run a small Python script hello-world-array.py as an array. # import sys library (we need this for the command line args) import sys # print task number print ( 'Hello world! from task number: ' , sys . argv [ 1 ]) You could then make a batch script like this, hello-world-array.sh : #!/bin/bash # A very simple example of how to run a Python script with a job array #SBATCH -A <account> #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --array=1-10 # how many tasks in the array #SBATCH -c 1 # Asking for 1 core # one core per task # Create specific output files for each task with the environment variable %j # which contains the job id and %a for each step #SBATCH -o hello-world-%j-%a.out # Load any modules you need module load <module> <python-module> # Run your Python script srun python hello-world-array.py $SLURM_ARRAY_TASK_ID Some array comments \u00b6 Default step of 1 Example: #SBATCH --array=4-80 Give an index (here steps of 4) Example: #SBATCH --array=1-100:4 Give a list instead of a range Example: #SBATCH --array=5,8,33,38 Throttle jobs, so only a smaller number of jobs run at a time Example: #SBATCH --array1-400%4 Name output/error files so each job ( %j or %A ) and step ( %a ) gets own file #SBATCH -o process_%j_%a.out #SBATCH -e process_%j_%a.err There is an environment variable $SLURM_ARRAY_TASK_ID which can be used to check/query with GPU jobs \u00b6 There are some differences between the centres in Sweden what type of GPUs they have. Resource cores/node RAM/node GPUs, type (per node) Tetralith 32 96-384 GB Nvidia T4 GPUs (1) Dardel 128 256-2048 GB 4 AMD Instinct\u2122 MI250X (2) Alvis 16 (skylake 2xV100), 32 (skylake 4xV100, 8xT4), 64 (icelake 4xA40, 4xA100) 256-1024 GB Nvidia v100 (2), v100 (4), T4 (8), A40 (4), A100 (4) Kebnekaise 28 (skylake), 72 (largemem), 128/256 (Zen3/Zen4) 128-3072 GB NVidia v100 (2), NVidia a100 (2), NVidia a6000 (2), NVidia l40s (2 or 6), NVidia H100 (4), NVidia A40 (8), AMD MI100 (2) Alvis also has a small number of nodes without GPUs, for heavy-duty pre- and post-processing that does not require a GPU. To use, specify the constraint -C NOGPU in your Slurm script. Allocating a GPU \u00b6 This is the most different of the Slurm settings, between centers. Resource batch settings Comments Tetralith #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu Alvis #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x - no node-sharing on multi-node jobs ( --exclusive is automatic) - Requesting -N 1 does not mean 1 full node Cosmos #SBATCH -p gpua100 #SBATCH --gres=gpu:1 Kebnekaise #SBATCH --gpus=x #SBATCH -C <type> Pelle Example GPU scripts \u00b6 This shows a simple GPU script, asking for 1 or 2 cards on a single node. NSC","title":"Sample job scripts"},{"location":"jobscripts/#sample__job__scripts","text":"","title":"Sample job scripts"},{"location":"jobscripts/#basic__serial__job","text":"Let\u2019s say you have a simple Python script called mmmult.py that creates 2 random-valued matrices, multiplies them together, and prints the shape of the result and the computation time. Let\u2019s also say that you want to run this code in your current working directory. Here is how you might run that program once on 1 core and 1 node: #!/bin/bash #SBATCH -A <project ID> ### replace with your project ID #SBATCH -t 00:10:00 ### walltime in hh:mm:ss format #SBATCH -J mmmult ### sample job name; customize as desired or omit #SBATCH -o process_%j.out ### filename for stderr - customise, include %j #SBATCH -e process_%j.err ### filename for stderr - customise, include %j #SBATCH -n 1 ### number of cores to use; same as --ntasks-per-node # write this script to stdout-file - useful for scripting errors cat $0 # Purge any loaded modules # Some centres recommend this, while at other centres (PDC in particular) this # should not be done. ml purge > /dev/null 2 > & 1 # Load required modules; customize as needed - this is for LUNARC # Can omit module version number if prerequisites only allow one version ml foss/2023b Python/3.11.5 SciPy-bundle #run the script python3 mmmult.py","title":"Basic Serial Job"},{"location":"jobscripts/#examples__by__centre","text":"Let us look at the above batch script as it might be written for some other centres. Tetralith Dardel HPC2N LUNARC UPPMAX mmmult.py #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here GCC 11.3.0 and Python 3.10.4 module load buildtool-easybuild/4.8.0-hpce082752a2 GCC/11.3.0 OpenMPI/4.1.4 Python/3.10.4 SciPy-bundle/2022.05 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A naiss2025-22-934 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for cray-python/3.11.7. module load cray-python/3.11.7 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A hpc2n2025-151 # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.3 and compatible SciPy-bundle module load GCC/12.3.0 Python/3.11.3 SciPy-bundle/2023.07 # Run your Python script python mmmult.py #!/bin/bash #SBATCH -A luXXXX-Y-ZZ # Change to your own #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here for Python/3.11.5 and compatible SciPy-bundle module load GCC/13.2.0 Python/3.11.5 SciPy-bundle/2023.11 # Run your Python script python mmmult.py #!/bin/bash -l #SBATCH -A uppmaxXXXX-Y-ZZZ # Change to your own after the course #SBATCH --time=00:10:00 # Asking for 10 minutes #SBATCH -n 1 # Asking for 1 core # Load any modules you need, here Python 3.11.8. module load python/3.11.8 # Run your Python script python mmmult.py import timeit import numpy as np starttime = timeit . default_timer () np . random . seed ( 1701 ) A = np . random . randint ( - 1000 , 1000 , size = ( 8 , 4 )) B = np . random . randint ( - 1000 , 1000 , size = ( 4 , 4 )) print ( \"This is matrix A: \\n \" , A ) print ( \"The shape of matrix A is \" , A . shape ) print () print ( \"This is matrix B: \\n \" , B ) print ( \"The shape of matrix B is \" , B . shape ) print () print ( \"Doing matrix-matrix multiplication...\" ) print () C = np . matmul ( A , B ) print ( \"The product of matrices A and B is: \\n \" , C ) print ( \"The shape of the resulting matrix is \" , C . shape ) print () print ( \"Time elapsed for generating matrices and multiplying them is \" , timeit . default_timer () - starttime ) There is no example for Alvis since you should only use that for running GPU jobs.","title":"Examples by centre"},{"location":"jobscripts/#openmp","text":"#!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -c <cores-per-task> module load <modules> # Set OMP_NUM_THREADS to the same value as -c with a fallback in case it isn't set. # SLURM_CPUS_PER_TASK is set to the value of -c, but only if -c is explicitly set if [ -n \" $SLURM_CPUS_PER_TASK \" ] ; then omp_threads = $SLURM_CPUS_PER_TASK else omp_threads = 1 fi export OMP_NUM_THREADS = $omp_threads ./myopenmpprogram -c is used to set cores per task and should be the same as OMP_NUM_THREADS Remember, Alvis is only for GPU jobs","title":"OpenMP"},{"location":"jobscripts/#mpi","text":"#!/bin/bash #SBATCH -A <account> #SBATCH -t HHH:MM:SS #SBATCH -n <tasks> module load <modules> srun ./mympiprogram Asking for whole nodes ( - N ) and possibly --tasks-per-node srun and mpirun should be interchangeable at many centres. Tetralith uses mpprun and Dardel uses srun Remember, you need to load modules with MPI At some centres mpirun --bind-to-core or srun --cpu-bind=cores is recommended for MPI jobs NOTE: Alvis is only used for GPU jobs","title":"MPI"},{"location":"jobscripts/#memory-intensive__jobs","text":"","title":"Memory-intensive jobs"},{"location":"jobscripts/#io__intensive__jobs","text":"","title":"I/O intensive jobs"},{"location":"jobscripts/#job__arrays","text":"Job arrays: a mechanism for submitting and managing collections of similar jobs. All jobs must have the same initial options (e.g. size, time limit, etc.) the execution times can vary depending on input data You create multiple jobs from one script, using the -- array directive. This requires very little BASH scripting abilities max number of jobs is restricted by max number of jobs/user - centre specific More information here on the official Slurm documentation pages. Example This shows how to run a small Python script hello-world-array.py as an array. # import sys library (we need this for the command line args) import sys # print task number print ( 'Hello world! from task number: ' , sys . argv [ 1 ]) You could then make a batch script like this, hello-world-array.sh : #!/bin/bash # A very simple example of how to run a Python script with a job array #SBATCH -A <account> #SBATCH --time=00:05:00 # Asking for 5 minutes #SBATCH --array=1-10 # how many tasks in the array #SBATCH -c 1 # Asking for 1 core # one core per task # Create specific output files for each task with the environment variable %j # which contains the job id and %a for each step #SBATCH -o hello-world-%j-%a.out # Load any modules you need module load <module> <python-module> # Run your Python script srun python hello-world-array.py $SLURM_ARRAY_TASK_ID","title":"Job arrays"},{"location":"jobscripts/#some__array__comments","text":"Default step of 1 Example: #SBATCH --array=4-80 Give an index (here steps of 4) Example: #SBATCH --array=1-100:4 Give a list instead of a range Example: #SBATCH --array=5,8,33,38 Throttle jobs, so only a smaller number of jobs run at a time Example: #SBATCH --array1-400%4 Name output/error files so each job ( %j or %A ) and step ( %a ) gets own file #SBATCH -o process_%j_%a.out #SBATCH -e process_%j_%a.err There is an environment variable $SLURM_ARRAY_TASK_ID which can be used to check/query with","title":"Some array comments"},{"location":"jobscripts/#gpu__jobs","text":"There are some differences between the centres in Sweden what type of GPUs they have. Resource cores/node RAM/node GPUs, type (per node) Tetralith 32 96-384 GB Nvidia T4 GPUs (1) Dardel 128 256-2048 GB 4 AMD Instinct\u2122 MI250X (2) Alvis 16 (skylake 2xV100), 32 (skylake 4xV100, 8xT4), 64 (icelake 4xA40, 4xA100) 256-1024 GB Nvidia v100 (2), v100 (4), T4 (8), A40 (4), A100 (4) Kebnekaise 28 (skylake), 72 (largemem), 128/256 (Zen3/Zen4) 128-3072 GB NVidia v100 (2), NVidia a100 (2), NVidia a6000 (2), NVidia l40s (2 or 6), NVidia H100 (4), NVidia A40 (8), AMD MI100 (2) Alvis also has a small number of nodes without GPUs, for heavy-duty pre- and post-processing that does not require a GPU. To use, specify the constraint -C NOGPU in your Slurm script.","title":"GPU jobs"},{"location":"jobscripts/#allocating__a__gpu","text":"This is the most different of the Slurm settings, between centers. Resource batch settings Comments Tetralith #SBATCH -n 1 #SBATCH -c 32 #SBATCH --gpus-per-task=1 Dardel #SBATCH -N 1 #SBATCH --ntasks-per-node=1 #SBATCH -p gpu Alvis #SBATCH -p alvis #SBATCH -N <nodes> #SBATCH --gpus-per-node=<type>:x - no node-sharing on multi-node jobs ( --exclusive is automatic) - Requesting -N 1 does not mean 1 full node Cosmos #SBATCH -p gpua100 #SBATCH --gres=gpu:1 Kebnekaise #SBATCH --gpus=x #SBATCH -C <type> Pelle","title":"Allocating a GPU"},{"location":"jobscripts/#example__gpu__scripts","text":"This shows a simple GPU script, asking for 1 or 2 cards on a single node. NSC","title":"Example GPU scripts"},{"location":"monitoring/","text":"Job monitoring and efficiency \u00b6 Discuss squeue, scancel, sinfo, sacct, \u2026 Why is a job ineffective? \u00b6 more threads than allocated cores not using all the cores you have allocated (unless on purpose/for memory) inefficient use of the file system (many small files, open/close many files) running job that could run on GPU on CPU instead Job monitoring \u00b6 Commands valid at all centres \u00b6 Command What scontrol show job JOBID info about a job, including estimated start time squeue --me --start your running and queued jobs with estimated start time sacct -l -j JOBID info about job, pipe to less -S for scrolling side-ways (it is a wide output) projinfo usage of your project, adding -vd lists member usage sshare -l -A <proj-account> gives priority/fairshare (LevelIFS) Most up-to-date project usage on a project\u2019s SUPR page, linked from here: https://supr.naiss.se/project/ Site-specific commands \u00b6 Command What Centre jobinfo wrapper around squeue UPPMAX, LUNARC, C3SE jobstats -p JOBID CPU and memory use of finished job (> 5 min) in a plot UPPMAX job_stats.py link to Grafana dashboard with overview of your running jobs. Add JOBID for real-time usage of a job C3SE job-usage JOBID grafana graphics of resource use for job (> few minutes) HPC2N jobload JOBID show cpu and memory usage in a job NSC jobsh NODE login to node, run \u201ctop\u201d NSC seff JOBID displays memory and CPU usage from job run NSC, PDC lastjobs lists 10 most recent job in recent 30 days NSC https://pdc-web.eecs.kth.se/cluster_usage/ Information about project usage PDC https://grafana.c3se.chalmers.se/d/user-jobs/user-jobs Grafana dashboard for user jobs C3SE https://www.nsc.liu.se/support/batch-jobs/tetralith/monitoring/ Job monitoring NSC https://docs.uppmax.uu.se/software/jobstats/ Job efficiency UPPMAX","title":"Job monitoring and efficiency"},{"location":"monitoring/#job__monitoring__and__efficiency","text":"Discuss squeue, scancel, sinfo, sacct, \u2026","title":"Job monitoring and efficiency"},{"location":"monitoring/#why__is__a__job__ineffective","text":"more threads than allocated cores not using all the cores you have allocated (unless on purpose/for memory) inefficient use of the file system (many small files, open/close many files) running job that could run on GPU on CPU instead","title":"Why is a job ineffective?"},{"location":"monitoring/#job__monitoring","text":"","title":"Job monitoring"},{"location":"monitoring/#commands__valid__at__all__centres","text":"Command What scontrol show job JOBID info about a job, including estimated start time squeue --me --start your running and queued jobs with estimated start time sacct -l -j JOBID info about job, pipe to less -S for scrolling side-ways (it is a wide output) projinfo usage of your project, adding -vd lists member usage sshare -l -A <proj-account> gives priority/fairshare (LevelIFS) Most up-to-date project usage on a project\u2019s SUPR page, linked from here: https://supr.naiss.se/project/","title":"Commands valid at all centres"},{"location":"monitoring/#site-specific__commands","text":"Command What Centre jobinfo wrapper around squeue UPPMAX, LUNARC, C3SE jobstats -p JOBID CPU and memory use of finished job (> 5 min) in a plot UPPMAX job_stats.py link to Grafana dashboard with overview of your running jobs. Add JOBID for real-time usage of a job C3SE job-usage JOBID grafana graphics of resource use for job (> few minutes) HPC2N jobload JOBID show cpu and memory usage in a job NSC jobsh NODE login to node, run \u201ctop\u201d NSC seff JOBID displays memory and CPU usage from job run NSC, PDC lastjobs lists 10 most recent job in recent 30 days NSC https://pdc-web.eecs.kth.se/cluster_usage/ Information about project usage PDC https://grafana.c3se.chalmers.se/d/user-jobs/user-jobs Grafana dashboard for user jobs C3SE https://www.nsc.liu.se/support/batch-jobs/tetralith/monitoring/ Job monitoring NSC https://docs.uppmax.uu.se/software/jobstats/ Job efficiency UPPMAX","title":"Site-specific commands"},{"location":"mpi/","text":"MPI (Message Passing Interface \u00b6 In order to take advantage of more than one core per job and get speed-up this way, you need to do some extra programming. In return, the job will be able to run across several processors that communicate over the local network - distributed parallelism. MPI is a way of doing distributed parallelism. MPI is a language-independent communications protocol used to program parallel computers. There are several MPI implementations, but most of them consists of a set of routines that can be called from Fortran, C, C++, and Python, as well as any language that can interface with their libraries. MPI is very portable and generally optimized for the hardware it runs on, so it will be reasonably fast. Programs that are parallelizable should be reasonably easy to convert to MPI programs by adding MPI routines to it. What kinds of programs can be parallelized? \u00b6 For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"A bit about MPI"},{"location":"mpi/#mpi__message__passing__interface","text":"In order to take advantage of more than one core per job and get speed-up this way, you need to do some extra programming. In return, the job will be able to run across several processors that communicate over the local network - distributed parallelism. MPI is a way of doing distributed parallelism. MPI is a language-independent communications protocol used to program parallel computers. There are several MPI implementations, but most of them consists of a set of routines that can be called from Fortran, C, C++, and Python, as well as any language that can interface with their libraries. MPI is very portable and generally optimized for the hardware it runs on, so it will be reasonably fast. Programs that are parallelizable should be reasonably easy to convert to MPI programs by adding MPI routines to it.","title":"MPI (Message Passing Interface"},{"location":"mpi/#what__kinds__of__programs__can__be__parallelized","text":"For a problem to be parallelizable, it must be possible to split it into smaller sections that can be solved independently of each other and then combined. What happens in a parallel program is generally the following: A \u201cmaster\u201d process is created to control the distribution of data and tasks. The \u201cmaster\u201d sends data and instructions to one or more \u201cworker\u201d processes that do the calculations. The \u201cworker\u201d processes then send the results back to the \u201cmaster\u201d. The \u201cmaster\u201d combines the results and/or may send out further subsections of the problem to be solved. Examples of parallel problems: Sorting Rendering computer graphics Computer simulations comparing many independent scenarios, like climate models Matrix Multiplication","title":"What kinds of programs can be parallelized?"},{"location":"openmp/","text":"Shared memory \u00b6 Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. \u00a7","title":"Short about OpenMP"},{"location":"openmp/#shared__memory","text":"Shared memory is memory that can be accessed by several programs at the same time, enabling them to communicate quickly and avoid redundant copies. Shared memory generally refers to a block of RAM accessible by several cores in a multi-core system. Computers with large amounts of shared memory and many cores per node are well suited for threaded programs, using OpenMP or similar. Computer clusters built up of many off-the-shelf computers usually have smaller amounts of shared memory and fewer cores per node than custom-built single supercomputers. This means they are more suited for programs using MPI than OpenMP. However, the number of cores per node is going up and many-core chips are now common. This means that OpenMP programs as well as programs combining MPI and OpenMP are often advantageous. \u00a7","title":"Shared memory"},{"location":"slurm/","text":"Introduction to Slurm \u00b6 The batch system used at UPPMAX, HPC2N, LUNARC, NSC, PDC, C3SE (and most other HPC centres in Sweden) is called Slurm. Guides and documentation HPC2N: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ UPPMAX: https://docs.uppmax.uu.se/cluster_guides/slurm/ LUNARC: https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ NSC: https://www.nsc.liu.se/support/batch-jobs/introduction/ PDC: https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ C3SE: https://www.c3se.chalmers.se/documentation/submitting_jobs/ Slurm is an Open Source job scheduler, which provides three key functions: Keeps track of available system resources - it allocates to users, exclusive or non-exclusive access to resources for some period of time Enforces local system resource usage and job scheduling policies - provides a framework for starting, executing, and monitoring work Manages a job queue, distributing work across resources according to policies Slurm is designed to handle thousands of nodes in a single cluster, and can sustain throughput of 120,000 jobs per hour. You can run programs either by giving all the commands on the command line or by submitting a job script. Using a job script is often recommended: If you ask for the resources on the command line, you will wait for the program to run before you can use the window again (unless you can send it to the background with & ). If you use a job script you have an easy record of the commands you used, to reuse or edit for later use. In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script). Slurm commands \u00b6 There are many more commands than the ones we have chosen to look at here, but these are the most commonly used ones. You can find more information on the Slurm homepage: Slurm documentation . salloc : requesting an interactive allocation interactive : another way of requesting an interactive allocation sbatch : submitting jobs to the batch system squeue : viewing the state of the batch queue scancel : cancel a job scontrol show : getting more info on jobs, nodes sinfo : information about the partitions/queues Let us look at these one at a time. salloc and interactive \u00b6 This is for requesting an interactive allocation. This is done differently depending on the centre. Cluster interactive salloc srun GfxLauncher or OpenOnDemand HPC2N Works Recommended N/A Recommended (OOD) UPPMAX Recommended Works N/A N/A LUNARC Works N/A N/A Recommended (GfxLauncher) NSC Recommended N/A N/A N/A PDC N/A Recommended N/A Possible C3SE N/A N/A Works Recommended (OOD) Examples \u00b6 interactive This is recommended at UPPMAX and NSC, and works at HPC2N and LUNARC. Usage: interactive -A [project_name] If you want longer walltime, more CPUs/GPUs, etc. you need to ask for that as well. This is the default which gives 1 CPU for 1 hour. UPPMAX NSC [ bbrydsoe@rackham3 ~ ] $ interactive -A uppmax2025-2-296 You receive the high interactive priority. You may run for at most one hour. Your job has been put into the devcore partition and is expected to start at once. ( Please remember, you may not simultaneously have more than one devel/devcore job, running or queued, in the batch system. ) Please, use no more than 6 .4 GB of RAM. salloc: Pending job allocation 55388069 salloc: job 55388069 queued and waiting for resources salloc: job 55388069 has been allocated resources salloc: Granted job allocation 55388069 salloc: Waiting for resource configuration salloc: Nodes r483 are ready for job _ _ ____ ____ __ __ _ __ __ | | | | _ \\| _ \\| \\/ | / \\ \\ \\/ / | System: r483 | | | | | _ ) | | _ ) | | \\/ | | / _ \\ \\ / | User: bbrydsoe | | _ | | __/ | __/ | | | | / ___ \\ / \\ | \\_ __/ | _ | | _ | | _ | | _/_/ \\_\\/ _/ \\_\\ | ############################################################################### User Guides: https://docs.uppmax.uu.se/ Write to support@uppmax.uu.se, if you have questions or comments. [ bbrydsoe@r483 ~ ] $ [ x_birbr@tetralith3 ~ ] $ interactive -A naiss2025-22-403 salloc: Pending job allocation 44252533 salloc: job 44252533 queued and waiting for resources salloc: job 44252533 has been allocated resources salloc: Granted job allocation 44252533 salloc: Waiting for resource configuration salloc: Nodes n340 are ready for job [ x_birbr@n340 ~ ] $ salloc This is recommended at HPC2N and PDC, and works at UPPMAX. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. At PDC, you also have to give the partition: main, shared, gpu HPC2N PDC b-an01 [ ~ ] $ salloc -A hpc2n2025-076 -t 00 :10:00 salloc: Pending job allocation 34624444 salloc: job 34624444 queued and waiting for resources salloc: job 34624444 has been allocated resources salloc: Granted job allocation 34624444 salloc: Nodes b-cn1403 are ready for job b-an01 [ ~ ] $ WARNING! This is not true interactivity! Note that we are still on the login node! In order to run anything in the allocation, you need to preface with srun like this: b-an01 [ ~ ] $ srun /bin/hostname b-cn1403.hpc2n.umu.se b-an01 [ ~ ] $ Otherwise anything will run on the login node! Also, interactive sessions (for instance a program that asks for input) will not work correctly as that dialogoue happens on the compute node which you do not have real access to! bbrydsoe@login1:~> salloc --time = 00 :10:00 -A naiss2025-22-403 -p main salloc: Pending job allocation 9722449 salloc: job 9722449 queued and waiting for resources salloc: job 9722449 has been allocated resources salloc: Granted job allocation 9722449 salloc: Waiting for resource configuration salloc: Nodes nid001134 are ready for job bbrydsoe@login1:~> Again, you are on the login node, and anything you want to run in the allocation must be preface with srun . However, at PDC you have another option; you can ssh to the allocated compute node and then it will be true interactivity: bbrydsoe@login1:~> ssh nid001134 bbrydsoe@nid001134:~ srun This works at C3SE, but is not recommended as when the login node is restarted the interactive job is also terminated. C3SE [ brydso@alvis2 ~ ] $ srun --account = NAISS2025-22-395 --gpus-per-node = T4:1 --time = 01 :00:00 --pty = /bin/bash [ brydso@alvis2-12 ~ ] $ GfxLauncher and OpenOnDemand This is recommended at HPC2N, LUNARC, and C3SE, and is possible at PDC. HPC2N LUNARC C3SE Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/ Go to https://alvis.c3se.chalmers.se/ There is some documentation here: https://uppmax.github.io/HPC-python/common/interactive_ondemand.html#start-an-interactive-session-from-ondemand sbatch \u00b6 The command sbatch is used to submit jobs to the batch system. This is done from the command line in the same way at all the HPC centres in Sweden: sbatch <batchscript.sh> For any named whatever you want to. It is a convention to use the suffix .sbatch or .sh , but it is not a requirement. You can use any or no suffix. It is merely to make it easier to find the script among the other files. Note At centres that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At centres that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB. We will talk much more about batch scripts in a short while, but for now we can use this small batch script for testing the Slurm commands: #!/bin/bash # Project id - change to your own! #SBATCH -A PROJ-ID # Asking for 1 core #SBATCH -n 1 # Asking for a walltime of 1 min #SBATCH --time=00:01:00 echo \"What is the hostname? It is this: \" /bin/hostname Example : Submitting the above batch script on Tetralith (NSC) [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194426 As you can see, you get the job id when submitting the batch script. When it has run, you can see with ls that you got a file called slurm-JOBID.out in your directory. squeue \u00b6 The command squeue is for viewing the state of the batch queue. If you just give the command, you will get a long list of all jobs in the queue, so it is usually best to constrain it to your own jobs. This can be done in two ways: squeue -u <username> squeue --me Example : b-an01 [ ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 34815904 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1404 34815905 cpu_sky mpi_hell bbrydsoe R 0 :00 2 b-cn [ 1404 ,1511 ] 34815906 cpu_sky mpi_hi.s bbrydsoe R 0 :00 2 b-cn [ 1511 -1512 ] 34815907 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1512 34815908 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1415 ,1512 ] 34815909 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1415 34815910 cpu_sky mpi_hell bbrydsoe R 0 :00 3 b-cn [ 1415 ,1421-1422 ] 34815911 cpu_sky mpi_hi.s bbrydsoe R 0 :00 1 b-cn1422 34815912 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1422 34815913 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1422 ,1427 ] 34815902 cpu_zen4 simple.s bbrydsoe CG 0 :03 1 b-cn1707 34815903 cpu_zen4 compiler bbrydsoe R 0 :00 1 b-cn1708 34815898 cpu_zen4 compiler bbrydsoe R 0 :03 2 b-cn [ 1703 ,1705 ] 34815899 cpu_zen4 mpi_gree bbrydsoe R 0 :03 2 b-cn [ 1705 ,1707 ] 34815900 cpu_zen4 mpi_hell bbrydsoe R 0 :03 1 b-cn1707 34815901 cpu_zen4 mpi_hi.s bbrydsoe R 0 :03 1 b-cn1707 34815922 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815921 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815920 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815919 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Priority ) 34815918 cpu_zen4, compiler bbrydsoe PD 0 :00 1 ( Priority ) 34815917 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815916 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815915 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815914 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Resources ) Here you also see some of the \u201cstates\u201d a job can be in. Some of the more common ones are: CA : CANCELLED. Job was explicitly cancelled by the user or system administrator. CF : CONFIGURING. Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG : COMPLETING. Job is in the process of completing. Some processes on some nodes may still be active. PD : PENDING. Job is awaiting resource allocation. R : RUNNING. Job currently has an allocation. S : SUSPENDED. Job has an allocation, but execution has been suspended and resources have been released for other jobs. List above from Slurm workload manager page about squeue . Example : Submit the \u201csimple.sh\u201d script several times, then do squeue --me to see that it is running, pending, or completing. [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194596 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194597 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194598 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194599 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194600 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194601 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194602 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194603 [ x_birbr@tetralith3 ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 45194603 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194602 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194601 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194600 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194599 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194598 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194597 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194596 tetralith simple.s x_birbr PD 0 :00 1 ( None ) scancel \u00b6 The command to cancel a job is scancel . You can either cancel a specific job: scancel <job id> or cancel all your jobs: scancel -u <username> Note As before, you get the <job id> either from when you submitted the job or from squeue --me . Note Only administrators can cancel other people\u2019s jobs! scontrol show \u00b6 The command scontrol show is used for getting more info on jobs and nodes. scontrol show job \u00b6 As usual, you get the <job id> from either when you submit the job or from squeue --me . The command is: scontrol show job <job id> Example : b-an01 [ ~ ] $ scontrol show job 34815931 JobId = 34815931 JobName = compiler-run UserId = bbrydsoe ( 2897 ) GroupId = folk ( 3001 ) MCS_label = N/A Priority = 2748684 Nice = 0 Account = staff QOS = normal JobState = COMPLETED Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:07 TimeLimit = 00 :10:00 TimeMin = N/A SubmitTime = 2025 -06-24T11:36:32 EligibleTime = 2025 -06-24T11:36:32 AccrueTime = 2025 -06-24T11:36:32 StartTime = 2025 -06-24T11:36:32 EndTime = 2025 -06-24T11:36:39 Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2025 -06-24T11:36:32 Scheduler = Main Partition = cpu_zen4 AllocNode:Sid = b-an01:626814 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = b-cn [ 1703 ,1705 ] BatchHost = b-cn1703 NumNodes = 2 NumCPUs = 12 NumTasks = 12 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* ReqTRES = cpu = 12 ,mem = 30192M,node = 1 ,billing = 12 AllocTRES = cpu = 12 ,mem = 30192M,node = 2 ,billing = 12 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryCPU = 2516M MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :02:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/compile-run.sh WorkDir = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage StdErr = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out StdIn = /dev/null StdOut = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out Power = Here you get much interesting information: JobState=COMPLETED : the job was completed and was not FAILED. It could also have been PENDING or COMPLETING RunTime=00:00:07 : the job ran for 7 seconds TimeLimit=00:10:00 : It could have run for up to 10 min (what you asked for) SubmitTime=2025-06-24T11:36:32 : when your job was submitted StartTime=2025-06-24T11:36:32 : when the job started Partition=cpu_zen4 : what partition/type of node it ran on NodeList=b-cn[1703,1705] : which specific nodes it ran on BatchHost=b-cn1703 : which of the nodes (if several) that was the master NumNodes=2 NumCPUs=12 NumTasks=12 CPUs/Task=1 : number of nodes, cpus, tasks WorkDir=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage : which directory your job was submitted from/was running in StdOut=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out : which directory the output files will be placed in The command scontrol show job <job id> can be run also while the job is pending, and can be used to get an estimate of when the job will start. Actual start time depends on the jobs priority, any other (people\u2019s) jobs starting and completing and being submitted, etc. It is often useful to know which nodes a job ran on if something did not work - perhaps the node was faulty. scontrol show node \u00b6 This command is used to get information about a specific node. You can for instance see its features, how many cores per socket, uptime, etc. Specifics will vary and depend on the centre you are running jobs at. Example : This if for one of the AMD Zen4 nodes at Kebnekaise, HPC2N. b-an01 [ ~ ] $ scontrol show node b-cn1703 NodeName = b-cn1703 Arch = x86_64 CoresPerSocket = 128 CPUAlloc = 253 CPUEfctv = 256 CPUTot = 256 CPULoad = 253 .38 AvailableFeatures = rack17,amd_cpu,zen4 ActiveFeatures = rack17,amd_cpu,zen4 Gres =( null ) NodeAddr = b-cn1703 NodeHostName = b-cn1703 Version = 23 .02.7 OS = Linux 5 .15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 RealMemory = 644096 AllocMem = 636548 FreeMem = 749623 Sockets = 2 Boards = 1 State = MIXED ThreadsPerCore = 1 TmpDisk = 0 Weight = 100 Owner = N/A MCS_label = N/A Partitions = cpu_zen4 BootTime = 2025 -06-24T06:32:25 SlurmdStartTime = 2025 -06-24T06:37:02 LastBusyTime = 2025 -06-24T08:29:45 ResumeAfterTime = None CfgTRES = cpu = 256 ,mem = 629G,billing = 256 AllocTRES = cpu = 253 ,mem = 636548M CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s sinfo \u00b6 The command sinfo gives you information about the partitions/queues. Example : This is for Tetralith, NSC [ x_birbr@tetralith3 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST tetralith* up 7 -00:00:00 1 plnd n1541 tetralith* up 7 -00:00:00 16 drain* n [ 237 ,245,439,532,625,646,712,759-760,809,1290,1364,1455,1638,1847,1864 ] tetralith* up 7 -00:00:00 3 drain n [ 13 ,66,454 ] tetralith* up 7 -00:00:00 20 resv n [ 1 -4,108,774,777,779-780,784-785,788,1109,1268,1281-1285,1288 ] tetralith* up 7 -00:00:00 327 mix n [ 7 ,39-40,46,50,69-70,75,78,85,91-93,97,112,119,121,124,126,128,130-134,137,139,141,149-150,156,159,164,167-168,170,174,184,187-188,190,193,196,203,206,208,212,231,241,244,257,259,262,267,280,287,293-294,310,315,323,327,329,333,340,350,352,371,377,379-381,385,405,420-422,434,441,446,465,467,501,504-505,514,524,529,549,553,558,561,564,573,575,602,610,612,615,617,622-623,626-627,631,637,651,662,671,678,691-692,699,703,709,718,720,723,726,741,745,752,754-755,768,776,781,790,792-793,803-804,808,818,853,855,859,863,867,881,883,915,925,959,966,974,981,984,999,1001-1003,1007-1011,1015,1018,1033,1044-1046,1050-1052,1056-1058,1071,1077,1102,1105-1106,1111-1115,1117,1119,1130,1132,1134-1136,1138-1140,1142-1143,1252,1254,1257,1267,1278-1279,1292,1296,1298,1309,1328,1339,1343,1345,1347,1349,1352,1354-1355,1357,1367,1375-1376,1379,1381,1386-1388,1398,1403,1410,1412,1420-1422,1428,1440,1446,1450,1459,1466,1468-1470,1474,1490-1491,1493,1498,1506,1510,1513,1520,1524,1529,1548-1549,1553,1562,1574-1575,1579,1586,1592,1595,1601,1606,1608,1612,1615,1620-1621,1631,1634,1639,1642,1647,1651-1653,1665,1688,1690,1697,1702,1706,1715-1716,1725,1728,1749,1754,1756,1767,1772,1774-1775,1778,1795-1796,1798-1799,1811,1816,1822,1826,1834,1842,1849,1857-1858,1871,1874,1879,1881,1896,1900,1902,1909,1911-1914,1945,1951,1953,1955-1956,1960,1969,1978,1983,2001,2005-2006,2008 ] tetralith* up 7 -00:00:00 1529 alloc n [ 5 -6,8-12,14-38,41-45,47-49,51-60,65,67-68,71-74,76-77,79-84,86-90,94-96,98-107,109-111,113-118,120,122-123,125,127,129,135-136,138,140,142-148,151-155,157-158,160-163,165-166,169,171-173,175-183,185-186,189,191-192,194-195,197-202,204-205,207,209-211,213-230,232-236,238-240,242-243,246-256,258,260-261,263-266,268-279,281-286,288-292,295-309,311-314,316-322,324-326,328,330-332,334-339,341-349,351,353-370,372-376,378,382-384,386-404,406-419,423-433,435-438,440,442-445,447-453,455-464,466,468-500,502-503,506-513,515-523,525-528,530-531,533-548,550-552,554-557,559-560,562-563,565-572,574,576-601,603-609,611,613-614,616,618-621,624,628-630,632-636,638-645,647-650,652-661,663-670,672-677,679-690,693-698,700-702,704-708,710-711,713-717,719,721-722,724-725,727-740,742-744,746-751,753,756-758,761-767,769-773,775,778,782-783,786-787,789,791,794-802,805-807,810-817,819-852,854,856-858,860-862,864-866,868-880,882,884-889,893,896-914,916-924,926-937,939-940,943,945,948-958,960-965,967-973,975-980,982-983,985-998,1000,1004-1006,1012-1014,1016-1017,1019-1032,1034-1043,1047-1049,1053-1055,1059-1070,1072-1076,1078-1101,1103-1104,1107-1108,1110,1116,1118,1120-1129,1131,1133,1137,1141,1144,1249-1251,1253,1255-1256,1258-1266,1269-1277,1280,1286-1287,1289,1291,1293-1295,1297,1299-1308,1310-1327,1329-1338,1340-1342,1344,1346,1348,1350-1351,1353,1356,1358-1363,1365-1366,1368-1374,1377-1378,1380,1382-1385,1389-1397,1399-1402,1404-1409,1411,1413-1419,1423-1427,1429-1439,1441-1445,1447-1449,1451-1454,1456-1458,1460-1465,1467,1471-1473,1475-1489,1492,1494-1497,1499-1505,1507-1509,1511-1512,1514-1519,1521-1523,1525-1528,1530-1540,1542-1547,1550-1552,1554-1561,1563-1573,1576-1578,1580-1585,1587-1591,1593-1594,1596-1600,1602-1605,1607,1609-1611,1613-1614,1616-1619,1622-1630,1632-1633,1635-1637,1640-1641,1643-1646,1648-1650,1654-1664,1666-1687,1689,1691-1696,1698-1701,1703-1705,1707-1714,1717-1724,1726-1727,1729-1748,1750-1753,1755,1757-1766,1768-1771,1773,1776-1777,1779-1794,1797,1800-1810,1812-1815,1817-1821,1824-1825,1827-1833,1835-1841,1843-1846,1848,1850-1856,1859-1863,1865-1870,1872-1873,1875-1878,1880,1882-1895,1897-1899,1901,1903-1908,1910,1915-1944,1946-1950,1952,1954,1957-1959,1961-1968,1970-1977,1979-1982,1984-2000,2002-2004,2007,2009-2016 ] As you can see, it shows partitions, nodes, and states. State can be drain, idle, resv, alloc, mix, plnd (and a few others), where the exact naming varies between centers. drain : node is draining after running a job resv : node is reserved/has a reservation for something alloc : node is allocated for a job mix : node is in several states, could for instance be that it is allocated, but starting to drain idle : node is free and can be allocated plnd : job planned for a higher priority job You can see the full list of states and their meaning with man sinfo . Slurm job scripts \u00b6 Now we have looked at the commands to control the job, but what about the job scripts? We had a small example further up on the page, which we used to test the commands, but now we will look more at the job scripts themselves. Simplest job \u00b6 The simplest possible batch script would look something like this: #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 echo $HOSTNAME The first line is called the \u201cshebang\u201d and it indicates that the script is written in the bash shell language. The second and third lines are resource statements to the Slurm batch scheduler. The second line above is where you put your project ID. Depending on centre, this is either always required or not technically required if you only have one project to your name. Regardless, we recommend that you make a habit of including it. The third line in the example above provides the walltime, the maximum amount of time that the program would be allowed to run (5 minutes in this example). If a job does not finish within the specified walltime, the resource management system terminates it and any data that were not already written to a file before time ran out are lost. The last line in the above sample is the code to be executed by the batch script. In this case, it just prints the name of the server on which the code ran. All of the parameters that Slurm needs to determine which resources to allocate, under whose account, and for how long, must be given as a series of resource statements of the form #SBATCH -<option> <value> or #SBATCH --<key-words>=<value> (note: < and > are not typically used in real arguments; they are just used here to indicate placeholder text). Depending on centre, for most compute nodes, unless otherwise specified, a batch script will run on 1 core of 1 node by default. However, at some centres it is required to always give the number of cores or nodes, so you should make it a habit to include it. Time/walltime the job will terminate when the time runs out, whether it has finished or not you will only be \u201ccharged\u201d for the consumed time asking for more time than needed will generally make the job take longer to start short jobs can start quicker (backfill) if you have no idea how long your job takes, ask for \u201clong\u201d time Conclusion: Ask for \u201ca bit\u201d more time than needed, but not too much Note There are many more resource statements and other commands that can go into the Slurm batch script. We will look at some of them in the next section, where we show some sample job scripts. You submit the job script with sbatch <jobscript.sh> as was mentioned earlier. Information about jobs \u00b6 Use the following: sacct job-info scontrol show job JOBID squeue \u2013me job_usage (HPC2N) and several more See Job monitoring and efficiency for more about job info.","title":"Introduction to Slurm"},{"location":"slurm/#introduction__to__slurm","text":"The batch system used at UPPMAX, HPC2N, LUNARC, NSC, PDC, C3SE (and most other HPC centres in Sweden) is called Slurm. Guides and documentation HPC2N: https://docs.hpc2n.umu.se/documentation/batchsystem/intro/ UPPMAX: https://docs.uppmax.uu.se/cluster_guides/slurm/ LUNARC: https://lunarc-documentation.readthedocs.io/en/latest/manual/manual_intro/ NSC: https://www.nsc.liu.se/support/batch-jobs/introduction/ PDC: https://support.pdc.kth.se/doc/run_jobs/job_scheduling/ C3SE: https://www.c3se.chalmers.se/documentation/submitting_jobs/ Slurm is an Open Source job scheduler, which provides three key functions: Keeps track of available system resources - it allocates to users, exclusive or non-exclusive access to resources for some period of time Enforces local system resource usage and job scheduling policies - provides a framework for starting, executing, and monitoring work Manages a job queue, distributing work across resources according to policies Slurm is designed to handle thousands of nodes in a single cluster, and can sustain throughput of 120,000 jobs per hour. You can run programs either by giving all the commands on the command line or by submitting a job script. Using a job script is often recommended: If you ask for the resources on the command line, you will wait for the program to run before you can use the window again (unless you can send it to the background with & ). If you use a job script you have an easy record of the commands you used, to reuse or edit for later use. In order to run a batch job, you need to create and submit a SLURM submit file (also called a batch submit file, a batch script, or a job script).","title":"Introduction to Slurm"},{"location":"slurm/#slurm__commands","text":"There are many more commands than the ones we have chosen to look at here, but these are the most commonly used ones. You can find more information on the Slurm homepage: Slurm documentation . salloc : requesting an interactive allocation interactive : another way of requesting an interactive allocation sbatch : submitting jobs to the batch system squeue : viewing the state of the batch queue scancel : cancel a job scontrol show : getting more info on jobs, nodes sinfo : information about the partitions/queues Let us look at these one at a time.","title":"Slurm commands"},{"location":"slurm/#salloc__and__interactive","text":"This is for requesting an interactive allocation. This is done differently depending on the centre. Cluster interactive salloc srun GfxLauncher or OpenOnDemand HPC2N Works Recommended N/A Recommended (OOD) UPPMAX Recommended Works N/A N/A LUNARC Works N/A N/A Recommended (GfxLauncher) NSC Recommended N/A N/A N/A PDC N/A Recommended N/A Possible C3SE N/A N/A Works Recommended (OOD)","title":"salloc and interactive"},{"location":"slurm/#examples","text":"interactive This is recommended at UPPMAX and NSC, and works at HPC2N and LUNARC. Usage: interactive -A [project_name] If you want longer walltime, more CPUs/GPUs, etc. you need to ask for that as well. This is the default which gives 1 CPU for 1 hour. UPPMAX NSC [ bbrydsoe@rackham3 ~ ] $ interactive -A uppmax2025-2-296 You receive the high interactive priority. You may run for at most one hour. Your job has been put into the devcore partition and is expected to start at once. ( Please remember, you may not simultaneously have more than one devel/devcore job, running or queued, in the batch system. ) Please, use no more than 6 .4 GB of RAM. salloc: Pending job allocation 55388069 salloc: job 55388069 queued and waiting for resources salloc: job 55388069 has been allocated resources salloc: Granted job allocation 55388069 salloc: Waiting for resource configuration salloc: Nodes r483 are ready for job _ _ ____ ____ __ __ _ __ __ | | | | _ \\| _ \\| \\/ | / \\ \\ \\/ / | System: r483 | | | | | _ ) | | _ ) | | \\/ | | / _ \\ \\ / | User: bbrydsoe | | _ | | __/ | __/ | | | | / ___ \\ / \\ | \\_ __/ | _ | | _ | | _ | | _/_/ \\_\\/ _/ \\_\\ | ############################################################################### User Guides: https://docs.uppmax.uu.se/ Write to support@uppmax.uu.se, if you have questions or comments. [ bbrydsoe@r483 ~ ] $ [ x_birbr@tetralith3 ~ ] $ interactive -A naiss2025-22-403 salloc: Pending job allocation 44252533 salloc: job 44252533 queued and waiting for resources salloc: job 44252533 has been allocated resources salloc: Granted job allocation 44252533 salloc: Waiting for resource configuration salloc: Nodes n340 are ready for job [ x_birbr@n340 ~ ] $ salloc This is recommended at HPC2N and PDC, and works at UPPMAX. Usage: salloc -A [project_name] -t HHH:MM:SS You have to give project ID and walltime. If you need more CPUs (1 is default) or GPUs, you have to ask for that as well. At PDC, you also have to give the partition: main, shared, gpu HPC2N PDC b-an01 [ ~ ] $ salloc -A hpc2n2025-076 -t 00 :10:00 salloc: Pending job allocation 34624444 salloc: job 34624444 queued and waiting for resources salloc: job 34624444 has been allocated resources salloc: Granted job allocation 34624444 salloc: Nodes b-cn1403 are ready for job b-an01 [ ~ ] $ WARNING! This is not true interactivity! Note that we are still on the login node! In order to run anything in the allocation, you need to preface with srun like this: b-an01 [ ~ ] $ srun /bin/hostname b-cn1403.hpc2n.umu.se b-an01 [ ~ ] $ Otherwise anything will run on the login node! Also, interactive sessions (for instance a program that asks for input) will not work correctly as that dialogoue happens on the compute node which you do not have real access to! bbrydsoe@login1:~> salloc --time = 00 :10:00 -A naiss2025-22-403 -p main salloc: Pending job allocation 9722449 salloc: job 9722449 queued and waiting for resources salloc: job 9722449 has been allocated resources salloc: Granted job allocation 9722449 salloc: Waiting for resource configuration salloc: Nodes nid001134 are ready for job bbrydsoe@login1:~> Again, you are on the login node, and anything you want to run in the allocation must be preface with srun . However, at PDC you have another option; you can ssh to the allocated compute node and then it will be true interactivity: bbrydsoe@login1:~> ssh nid001134 bbrydsoe@nid001134:~ srun This works at C3SE, but is not recommended as when the login node is restarted the interactive job is also terminated. C3SE [ brydso@alvis2 ~ ] $ srun --account = NAISS2025-22-395 --gpus-per-node = T4:1 --time = 01 :00:00 --pty = /bin/bash [ brydso@alvis2-12 ~ ] $ GfxLauncher and OpenOnDemand This is recommended at HPC2N, LUNARC, and C3SE, and is possible at PDC. HPC2N LUNARC C3SE Go to https://portal.hpc2n.umu.se/ and login. Documentation here: https://docs.hpc2n.umu.se/tutorials/connections/#open__ondemand Login with ThinLinc: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/using_hpc_desktop/ Follow the documentation for starting the GfxLauncher for OpenOnDemand: https://lunarc-documentation.readthedocs.io/en/latest/getting_started/gfxlauncher/ Go to https://alvis.c3se.chalmers.se/ There is some documentation here: https://uppmax.github.io/HPC-python/common/interactive_ondemand.html#start-an-interactive-session-from-ondemand","title":"Examples"},{"location":"slurm/#sbatch","text":"The command sbatch is used to submit jobs to the batch system. This is done from the command line in the same way at all the HPC centres in Sweden: sbatch <batchscript.sh> For any named whatever you want to. It is a convention to use the suffix .sbatch or .sh , but it is not a requirement. You can use any or no suffix. It is merely to make it easier to find the script among the other files. Note At centres that have OpenOnDemand installed, you do not have to submit a batch job, but can run directly on the already allocated resources (see interactive jobs). OpenOnDemand is a good option for interactive tasks, graphical applications/visualization, and simpler job submittions. It can also be more user-friendly. Regardless, there are many situations where submitting a batch job is the best option instead, including when you want to run jobs that need many resources (time, memory, multiple cores, multiple GPUs) or when you run multiple jobs concurrently or in a specified succession, without need for manual intervention. Batch jobs are often also preferred for automation (scripts) and reproducibility. Many types of application software fall into this category. At centres that have ThinLinc you can usually submit MATLAB jobs to compute resources from within MATLAB. We will talk much more about batch scripts in a short while, but for now we can use this small batch script for testing the Slurm commands: #!/bin/bash # Project id - change to your own! #SBATCH -A PROJ-ID # Asking for 1 core #SBATCH -n 1 # Asking for a walltime of 1 min #SBATCH --time=00:01:00 echo \"What is the hostname? It is this: \" /bin/hostname Example : Submitting the above batch script on Tetralith (NSC) [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194426 As you can see, you get the job id when submitting the batch script. When it has run, you can see with ls that you got a file called slurm-JOBID.out in your directory.","title":"sbatch"},{"location":"slurm/#squeue","text":"The command squeue is for viewing the state of the batch queue. If you just give the command, you will get a long list of all jobs in the queue, so it is usually best to constrain it to your own jobs. This can be done in two ways: squeue -u <username> squeue --me Example : b-an01 [ ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 34815904 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1404 34815905 cpu_sky mpi_hell bbrydsoe R 0 :00 2 b-cn [ 1404 ,1511 ] 34815906 cpu_sky mpi_hi.s bbrydsoe R 0 :00 2 b-cn [ 1511 -1512 ] 34815907 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1512 34815908 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1415 ,1512 ] 34815909 cpu_sky mpi_gree bbrydsoe R 0 :00 1 b-cn1415 34815910 cpu_sky mpi_hell bbrydsoe R 0 :00 3 b-cn [ 1415 ,1421-1422 ] 34815911 cpu_sky mpi_hi.s bbrydsoe R 0 :00 1 b-cn1422 34815912 cpu_sky simple.s bbrydsoe R 0 :00 1 b-cn1422 34815913 cpu_sky compiler bbrydsoe R 0 :00 2 b-cn [ 1422 ,1427 ] 34815902 cpu_zen4 simple.s bbrydsoe CG 0 :03 1 b-cn1707 34815903 cpu_zen4 compiler bbrydsoe R 0 :00 1 b-cn1708 34815898 cpu_zen4 compiler bbrydsoe R 0 :03 2 b-cn [ 1703 ,1705 ] 34815899 cpu_zen4 mpi_gree bbrydsoe R 0 :03 2 b-cn [ 1705 ,1707 ] 34815900 cpu_zen4 mpi_hell bbrydsoe R 0 :03 1 b-cn1707 34815901 cpu_zen4 mpi_hi.s bbrydsoe R 0 :03 1 b-cn1707 34815922 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815921 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815920 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815919 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Priority ) 34815918 cpu_zen4, compiler bbrydsoe PD 0 :00 1 ( Priority ) 34815917 cpu_zen4, simple.s bbrydsoe PD 0 :00 1 ( Priority ) 34815916 cpu_zen4, mpi_hi.s bbrydsoe PD 0 :00 1 ( Priority ) 34815915 cpu_zen4, mpi_hell bbrydsoe PD 0 :00 1 ( Priority ) 34815914 cpu_zen4, mpi_gree bbrydsoe PD 0 :00 1 ( Resources ) Here you also see some of the \u201cstates\u201d a job can be in. Some of the more common ones are: CA : CANCELLED. Job was explicitly cancelled by the user or system administrator. CF : CONFIGURING. Job has been allocated resources, but are waiting for them to become ready for use (e.g. booting). CG : COMPLETING. Job is in the process of completing. Some processes on some nodes may still be active. PD : PENDING. Job is awaiting resource allocation. R : RUNNING. Job currently has an allocation. S : SUSPENDED. Job has an allocation, but execution has been suspended and resources have been released for other jobs. List above from Slurm workload manager page about squeue . Example : Submit the \u201csimple.sh\u201d script several times, then do squeue --me to see that it is running, pending, or completing. [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194596 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194597 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194598 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194599 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194600 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194601 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194602 [ x_birbr@tetralith3 ~ ] $ sbatch simple.sh Submitted batch job 45194603 [ x_birbr@tetralith3 ~ ] $ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 45194603 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194602 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194601 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194600 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194599 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194598 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194597 tetralith simple.s x_birbr PD 0 :00 1 ( None ) 45194596 tetralith simple.s x_birbr PD 0 :00 1 ( None )","title":"squeue"},{"location":"slurm/#scancel","text":"The command to cancel a job is scancel . You can either cancel a specific job: scancel <job id> or cancel all your jobs: scancel -u <username> Note As before, you get the <job id> either from when you submitted the job or from squeue --me . Note Only administrators can cancel other people\u2019s jobs!","title":"scancel"},{"location":"slurm/#scontrol__show","text":"The command scontrol show is used for getting more info on jobs and nodes.","title":"scontrol show"},{"location":"slurm/#scontrol__show__job","text":"As usual, you get the <job id> from either when you submit the job or from squeue --me . The command is: scontrol show job <job id> Example : b-an01 [ ~ ] $ scontrol show job 34815931 JobId = 34815931 JobName = compiler-run UserId = bbrydsoe ( 2897 ) GroupId = folk ( 3001 ) MCS_label = N/A Priority = 2748684 Nice = 0 Account = staff QOS = normal JobState = COMPLETED Reason = None Dependency =( null ) Requeue = 0 Restarts = 0 BatchFlag = 1 Reboot = 0 ExitCode = 0 :0 RunTime = 00 :00:07 TimeLimit = 00 :10:00 TimeMin = N/A SubmitTime = 2025 -06-24T11:36:32 EligibleTime = 2025 -06-24T11:36:32 AccrueTime = 2025 -06-24T11:36:32 StartTime = 2025 -06-24T11:36:32 EndTime = 2025 -06-24T11:36:39 Deadline = N/A SuspendTime = None SecsPreSuspend = 0 LastSchedEval = 2025 -06-24T11:36:32 Scheduler = Main Partition = cpu_zen4 AllocNode:Sid = b-an01:626814 ReqNodeList =( null ) ExcNodeList =( null ) NodeList = b-cn [ 1703 ,1705 ] BatchHost = b-cn1703 NumNodes = 2 NumCPUs = 12 NumTasks = 12 CPUs/Task = 1 ReqB:S:C:T = 0 :0:*:* ReqTRES = cpu = 12 ,mem = 30192M,node = 1 ,billing = 12 AllocTRES = cpu = 12 ,mem = 30192M,node = 2 ,billing = 12 Socks/Node = * NtasksPerN:B:S:C = 0 :0:*:* CoreSpec = * MinCPUsNode = 1 MinMemoryCPU = 2516M MinTmpDiskNode = 0 Features =( null ) DelayBoot = 00 :02:00 OverSubscribe = OK Contiguous = 0 Licenses =( null ) Network =( null ) Command = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/compile-run.sh WorkDir = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage StdErr = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out StdIn = /dev/null StdOut = /pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out Power = Here you get much interesting information: JobState=COMPLETED : the job was completed and was not FAILED. It could also have been PENDING or COMPLETING RunTime=00:00:07 : the job ran for 7 seconds TimeLimit=00:10:00 : It could have run for up to 10 min (what you asked for) SubmitTime=2025-06-24T11:36:32 : when your job was submitted StartTime=2025-06-24T11:36:32 : when the job started Partition=cpu_zen4 : what partition/type of node it ran on NodeList=b-cn[1703,1705] : which specific nodes it ran on BatchHost=b-cn1703 : which of the nodes (if several) that was the master NumNodes=2 NumCPUs=12 NumTasks=12 CPUs/Task=1 : number of nodes, cpus, tasks WorkDir=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage : which directory your job was submitted from/was running in StdOut=/pfs/proj/nobackup/fs/projnb10/support-hpc2n/bbrydsoe/intro-course/hands-ons/3.usage/slurm-34815931.out : which directory the output files will be placed in The command scontrol show job <job id> can be run also while the job is pending, and can be used to get an estimate of when the job will start. Actual start time depends on the jobs priority, any other (people\u2019s) jobs starting and completing and being submitted, etc. It is often useful to know which nodes a job ran on if something did not work - perhaps the node was faulty.","title":"scontrol show job"},{"location":"slurm/#scontrol__show__node","text":"This command is used to get information about a specific node. You can for instance see its features, how many cores per socket, uptime, etc. Specifics will vary and depend on the centre you are running jobs at. Example : This if for one of the AMD Zen4 nodes at Kebnekaise, HPC2N. b-an01 [ ~ ] $ scontrol show node b-cn1703 NodeName = b-cn1703 Arch = x86_64 CoresPerSocket = 128 CPUAlloc = 253 CPUEfctv = 256 CPUTot = 256 CPULoad = 253 .38 AvailableFeatures = rack17,amd_cpu,zen4 ActiveFeatures = rack17,amd_cpu,zen4 Gres =( null ) NodeAddr = b-cn1703 NodeHostName = b-cn1703 Version = 23 .02.7 OS = Linux 5 .15.0-142-generic #152-Ubuntu SMP Mon May 19 10:54:31 UTC 2025 RealMemory = 644096 AllocMem = 636548 FreeMem = 749623 Sockets = 2 Boards = 1 State = MIXED ThreadsPerCore = 1 TmpDisk = 0 Weight = 100 Owner = N/A MCS_label = N/A Partitions = cpu_zen4 BootTime = 2025 -06-24T06:32:25 SlurmdStartTime = 2025 -06-24T06:37:02 LastBusyTime = 2025 -06-24T08:29:45 ResumeAfterTime = None CfgTRES = cpu = 256 ,mem = 629G,billing = 256 AllocTRES = cpu = 253 ,mem = 636548M CapWatts = n/a CurrentWatts = 0 AveWatts = 0 ExtSensorsJoules = n/s ExtSensorsWatts = 0 ExtSensorsTemp = n/s","title":"scontrol show node"},{"location":"slurm/#sinfo","text":"The command sinfo gives you information about the partitions/queues. Example : This is for Tetralith, NSC [ x_birbr@tetralith3 ~ ] $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST tetralith* up 7 -00:00:00 1 plnd n1541 tetralith* up 7 -00:00:00 16 drain* n [ 237 ,245,439,532,625,646,712,759-760,809,1290,1364,1455,1638,1847,1864 ] tetralith* up 7 -00:00:00 3 drain n [ 13 ,66,454 ] tetralith* up 7 -00:00:00 20 resv n [ 1 -4,108,774,777,779-780,784-785,788,1109,1268,1281-1285,1288 ] tetralith* up 7 -00:00:00 327 mix n [ 7 ,39-40,46,50,69-70,75,78,85,91-93,97,112,119,121,124,126,128,130-134,137,139,141,149-150,156,159,164,167-168,170,174,184,187-188,190,193,196,203,206,208,212,231,241,244,257,259,262,267,280,287,293-294,310,315,323,327,329,333,340,350,352,371,377,379-381,385,405,420-422,434,441,446,465,467,501,504-505,514,524,529,549,553,558,561,564,573,575,602,610,612,615,617,622-623,626-627,631,637,651,662,671,678,691-692,699,703,709,718,720,723,726,741,745,752,754-755,768,776,781,790,792-793,803-804,808,818,853,855,859,863,867,881,883,915,925,959,966,974,981,984,999,1001-1003,1007-1011,1015,1018,1033,1044-1046,1050-1052,1056-1058,1071,1077,1102,1105-1106,1111-1115,1117,1119,1130,1132,1134-1136,1138-1140,1142-1143,1252,1254,1257,1267,1278-1279,1292,1296,1298,1309,1328,1339,1343,1345,1347,1349,1352,1354-1355,1357,1367,1375-1376,1379,1381,1386-1388,1398,1403,1410,1412,1420-1422,1428,1440,1446,1450,1459,1466,1468-1470,1474,1490-1491,1493,1498,1506,1510,1513,1520,1524,1529,1548-1549,1553,1562,1574-1575,1579,1586,1592,1595,1601,1606,1608,1612,1615,1620-1621,1631,1634,1639,1642,1647,1651-1653,1665,1688,1690,1697,1702,1706,1715-1716,1725,1728,1749,1754,1756,1767,1772,1774-1775,1778,1795-1796,1798-1799,1811,1816,1822,1826,1834,1842,1849,1857-1858,1871,1874,1879,1881,1896,1900,1902,1909,1911-1914,1945,1951,1953,1955-1956,1960,1969,1978,1983,2001,2005-2006,2008 ] tetralith* up 7 -00:00:00 1529 alloc n [ 5 -6,8-12,14-38,41-45,47-49,51-60,65,67-68,71-74,76-77,79-84,86-90,94-96,98-107,109-111,113-118,120,122-123,125,127,129,135-136,138,140,142-148,151-155,157-158,160-163,165-166,169,171-173,175-183,185-186,189,191-192,194-195,197-202,204-205,207,209-211,213-230,232-236,238-240,242-243,246-256,258,260-261,263-266,268-279,281-286,288-292,295-309,311-314,316-322,324-326,328,330-332,334-339,341-349,351,353-370,372-376,378,382-384,386-404,406-419,423-433,435-438,440,442-445,447-453,455-464,466,468-500,502-503,506-513,515-523,525-528,530-531,533-548,550-552,554-557,559-560,562-563,565-572,574,576-601,603-609,611,613-614,616,618-621,624,628-630,632-636,638-645,647-650,652-661,663-670,672-677,679-690,693-698,700-702,704-708,710-711,713-717,719,721-722,724-725,727-740,742-744,746-751,753,756-758,761-767,769-773,775,778,782-783,786-787,789,791,794-802,805-807,810-817,819-852,854,856-858,860-862,864-866,868-880,882,884-889,893,896-914,916-924,926-937,939-940,943,945,948-958,960-965,967-973,975-980,982-983,985-998,1000,1004-1006,1012-1014,1016-1017,1019-1032,1034-1043,1047-1049,1053-1055,1059-1070,1072-1076,1078-1101,1103-1104,1107-1108,1110,1116,1118,1120-1129,1131,1133,1137,1141,1144,1249-1251,1253,1255-1256,1258-1266,1269-1277,1280,1286-1287,1289,1291,1293-1295,1297,1299-1308,1310-1327,1329-1338,1340-1342,1344,1346,1348,1350-1351,1353,1356,1358-1363,1365-1366,1368-1374,1377-1378,1380,1382-1385,1389-1397,1399-1402,1404-1409,1411,1413-1419,1423-1427,1429-1439,1441-1445,1447-1449,1451-1454,1456-1458,1460-1465,1467,1471-1473,1475-1489,1492,1494-1497,1499-1505,1507-1509,1511-1512,1514-1519,1521-1523,1525-1528,1530-1540,1542-1547,1550-1552,1554-1561,1563-1573,1576-1578,1580-1585,1587-1591,1593-1594,1596-1600,1602-1605,1607,1609-1611,1613-1614,1616-1619,1622-1630,1632-1633,1635-1637,1640-1641,1643-1646,1648-1650,1654-1664,1666-1687,1689,1691-1696,1698-1701,1703-1705,1707-1714,1717-1724,1726-1727,1729-1748,1750-1753,1755,1757-1766,1768-1771,1773,1776-1777,1779-1794,1797,1800-1810,1812-1815,1817-1821,1824-1825,1827-1833,1835-1841,1843-1846,1848,1850-1856,1859-1863,1865-1870,1872-1873,1875-1878,1880,1882-1895,1897-1899,1901,1903-1908,1910,1915-1944,1946-1950,1952,1954,1957-1959,1961-1968,1970-1977,1979-1982,1984-2000,2002-2004,2007,2009-2016 ] As you can see, it shows partitions, nodes, and states. State can be drain, idle, resv, alloc, mix, plnd (and a few others), where the exact naming varies between centers. drain : node is draining after running a job resv : node is reserved/has a reservation for something alloc : node is allocated for a job mix : node is in several states, could for instance be that it is allocated, but starting to drain idle : node is free and can be allocated plnd : job planned for a higher priority job You can see the full list of states and their meaning with man sinfo .","title":"sinfo"},{"location":"slurm/#slurm__job__scripts","text":"Now we have looked at the commands to control the job, but what about the job scripts? We had a small example further up on the page, which we used to test the commands, but now we will look more at the job scripts themselves.","title":"Slurm job scripts"},{"location":"slurm/#simplest__job","text":"The simplest possible batch script would look something like this: #!/bin/bash #SBATCH -A <proj-id> ###replace with your project ID #SBATCH -t 00:05:00 echo $HOSTNAME The first line is called the \u201cshebang\u201d and it indicates that the script is written in the bash shell language. The second and third lines are resource statements to the Slurm batch scheduler. The second line above is where you put your project ID. Depending on centre, this is either always required or not technically required if you only have one project to your name. Regardless, we recommend that you make a habit of including it. The third line in the example above provides the walltime, the maximum amount of time that the program would be allowed to run (5 minutes in this example). If a job does not finish within the specified walltime, the resource management system terminates it and any data that were not already written to a file before time ran out are lost. The last line in the above sample is the code to be executed by the batch script. In this case, it just prints the name of the server on which the code ran. All of the parameters that Slurm needs to determine which resources to allocate, under whose account, and for how long, must be given as a series of resource statements of the form #SBATCH -<option> <value> or #SBATCH --<key-words>=<value> (note: < and > are not typically used in real arguments; they are just used here to indicate placeholder text). Depending on centre, for most compute nodes, unless otherwise specified, a batch script will run on 1 core of 1 node by default. However, at some centres it is required to always give the number of cores or nodes, so you should make it a habit to include it. Time/walltime the job will terminate when the time runs out, whether it has finished or not you will only be \u201ccharged\u201d for the consumed time asking for more time than needed will generally make the job take longer to start short jobs can start quicker (backfill) if you have no idea how long your job takes, ask for \u201clong\u201d time Conclusion: Ask for \u201ca bit\u201d more time than needed, but not too much Note There are many more resource statements and other commands that can go into the Slurm batch script. We will look at some of them in the next section, where we show some sample job scripts. You submit the job script with sbatch <jobscript.sh> as was mentioned earlier.","title":"Simplest job"},{"location":"slurm/#information__about__jobs","text":"Use the following: sacct job-info scontrol show job JOBID squeue \u2013me job_usage (HPC2N) and several more See Job monitoring and efficiency for more about job info.","title":"Information about jobs"},{"location":"summary/","text":"Summary \u00b6 Today we have discussed\u2026","title":"Summary"},{"location":"summary/#summary","text":"Today we have discussed\u2026","title":"Summary"},{"location":"images/image_sources/","text":"Readme about the odp presentations \u00b6 The odp slide sets used to generate the images are created in google docs. If you need to modify them a typical libre office installation can not handle them properly. Re-import in to google docs and manipulate in there.","title":"Readme about the odp presentations"},{"location":"images/image_sources/#readme__about__the__odp__presentations","text":"The odp slide sets used to generate the images are created in google docs. If you need to modify them a typical libre office installation can not handle them properly. Re-import in to google docs and manipulate in there.","title":"Readme about the odp presentations"}]}